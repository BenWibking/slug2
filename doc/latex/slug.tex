% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{\nobreakspace}
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}

\addto\captionsenglish{\renewcommand{\figurename}{Fig. }}
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\floatname{literal-block}{Listing }



\title{slug Documentation}
\date{July 19, 2015}
\release{2.0}
\author{Mark Krumholz, Michele Fumagalli, et al.}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@mb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\renewcommand\PYGZsq{\textquotesingle}

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}


Contents:


\chapter{License}
\label{license::doc}\label{license:welcome-to-slug-s-documentation}\label{license:license}
SLUG is distributed under the terms of the \href{http://www.gnu.org/copyleft/gpl.html}{GNU General Public License version 3.0}. The text of the license is included in the main directory of the repository as \code{GPL-3.0.txt}.


\chapter{Getting SLUG}
\label{getting:getting-slug}\label{getting::doc}
SLUG is available at \href{https://bitbucket.org/krumholz/slug2}{https://bitbucket.org/krumholz/slug2}. The easiest way to download a copy is via \href{http://git-scm.com/}{git}. If you have git, you can download SLUG by doing:

\begin{Verbatim}[commandchars=\\\{\}]
git clone https://krumholz@bitbucket.org/krumholz/slug2.git
\end{Verbatim}

In addition to the core SLUG code, the Bayesian inference tools cluster\_slug and sfr\_slug require large libraries of simulations on which to operate. These are not included in the git repository due to their sizes. You can download these from \href{http://www.slugsps.com/data}{http://www.slugsps.com/data}.


\chapter{Introduction to SLUG}
\label{intro:introduction-to-slug}\label{intro::doc}
This is a guide for users of the SLUG software package. SLUG is distributed under the terms of the \href{http://www.gnu.org/licenses/gpl.html}{GNU General Public License v. 3.0}. A copy of the license notification is included in the main SLUG directory. If you use SLUG in any published work, please cite the SLUG method papers, \href{http://adsabs.harvard.edu/abs/2012ApJ...745..145D}{da Silva, R. L., Fumagalli, M., \& Krumholz, M. R., 2012, The Astrophysical Journal, 745, 145} and \href{http://adsabs.harvard.edu/abs/2015arXiv150205408K}{Krumholz, M. R., Fumagalli, M., da Silva, R. L., Rendahl, T., \& Parra, J. 2015, submitted to Monthly Notices of the Royal Astronomical Society, arXiv:1502.05408}.


\section{What Does SLUG Do?}
\label{intro:what-does-slug-do}
SLUG is a stellar population synthesis (SPS) code, meaning that, for a specified stellar initial mass function (IMF), star formation history (SFH), cluster mass function (CMF), cluster lifetime function (CLF), and (optionally) distribution of extinctions (A\_V), it predicts the spectra and photometry of both individual star clusters and the galaxies (or sub-regions of galaxies) that contain them. In this regard, SLUG operates much like any other SPS code. The main difference is that SLUG regards the functions describing the stellar population as probability distributions, and the resulting stellar population as being the result of a draw from them. SLUG performs a Monte Carlo simulation to determine the PDF of the light produced by the stellar populations that are drawn from these distributions. The remainder of this section briefly describes the major conceptual pieces of a SLUG simulation. For a more detailed description, readers are referred to \href{http://adsabs.harvard.edu/abs/2012ApJ...745..145D}{da Silva, Fumagalli, \& Krumholz (2012)}.


\section{Cluster Simulations and Galaxy Simulations}
\label{intro:cluster-simulations-and-galaxy-simulations}
SLUG can simulate either a simple stellar population (i.e., a group of stars all born at one time) or a composite stellar population, consisting of stars born at a distribution of times. We refer to the former case as a ``cluster'' simulation, and the latter as a ``galaxy'' simulation, since one can be thought of as approximating the behavior of a single star cluster, and the other as approximating a whole galaxy.


\section{Probability Distribution Functions: the IMF, SFH, CMF, CLF, A\_V distribution}
\label{intro:probability-distribution-functions-the-imf-sfh-cmf-clf-a-v-distribution}\label{intro:ssec-slugpdfs}
As mentioned above, SLUG regards the IMF, SFH, CMF, CLF, and extinction A\_V as probability distribution functions. These PDFs can be described by a very wide range of possible functional forms; see {\hyperref[pdfs:sec-pdfs]{\emph{\DUspan{}{Probability Distribution Functions}}}} for details on the exact functional forms allowed, and on how they can be specified in the code. When SLUG runs a cluster simulation, it draws stars from the specified IMF in an attempt to produce a cluster of a user-specified total mass. There are a number of possible methods for performing such mass-limited sampling, and SLUG gives the user a wide menu of options; see {\hyperref[pdfs:sec-pdfs]{\emph{\DUspan{}{Probability Distribution Functions}}}}. SLUG will also, upon user request, randomly draw a visual extinction A\_V to be applied to the light.

For a galaxy simulation, the procedure involves one extra step. In this case, SLUG assumes that some fraction \(f_c\) of the stars in the galaxy are born in star clusters, which, for the purposes of SLUG, means that they all share the same birth time. The remaining fraction \(1-f_c\) of stars are field stars. When a galaxy simulation is run, SLUG determines the total mass of stars \(M_*\) that should have formed since the start of the simulation (or since the last output, if more than one output is requested) from the star formation history, and then draws field stars and star clusters in an attempt to produce masses \((1-f_c)M_*\) and \(f_c M_*\). For the field stars, the stellar masses are drawn from the IMF, in a process completely analogous to the cluster case, and each star is given its own randomly-generated extinction. For star clusters, the masses of the clusters are drawn from the CMF, and each cluster is then populated from the IMF as in the cluster case. Again, each cluster gets its own extinction. For both the field stars and the star clusters, the time of their birth is drawn from the PDF describing the SFH.

Finally, star clusters can be disrupted independent of the fate of their parent stars. When each cluster is formed, it is assigned a lifetime drawn from the CLF. Once that time has passed, the cluster ceases to be entered in the lists of individual cluster spectra and photometry (see next section), although the individual stars continue to contribute to the integrated light of the galaxy.


\section{Spectra and Photometry}
\label{intro:ssec-spec-phot}\label{intro:spectra-and-photometry}
Once SLUG has drawn a population of stars, its final step is to compute the light they produce. SLUG does this in several steps. First, it computes the physical properties of all the stars present user-specified times using a set of stellar evolutionary tracks. Second, it uses these physical properties to compute the composite spectra produced by the stars, using a user-specified set of stellar atmosphere models. Formally, the quantity computed is the specific luminosity per unit wavelength \(L_\lambda\). Third, if nebular emission is enabled, the code calculates the spectrum \(L_{\lambda,\mathrm{neb}}\) that emerges after the starlight passes through the HII region aruond the star -- see {\hyperref[intro:ssec-nebula]{\emph{\DUspan{}{Nebular Processing}}}}. Fourth, if extinction is enabled, SLUG computes the extincted stellar and nebula-processed spectra \(L_{\lambda,\mathrm{ex}}\) and \(L_{\lambda,\mathrm{neb,ex}}\) -- see {\hyperref[intro:ssec-extinction]{\emph{\DUspan{}{Extinction}}}}. Fifth and finally, SLUG computes photometry for the stellar population by integrating all computed spectra over a set of specified photometric filters. Depending on the options specified by the user and the filter under consideration, the photometric value output will be one of the following:
\begin{itemize}
\item {} 
The frequency-averaged luminosity across the filter, defined as

\end{itemize}
\begin{gather}
\begin{split}\langle L_\nu\rangle_R = \frac{\int L_\nu \, d\ln\nu}{\int R_\nu (\nu/\nu_c)^\beta \, d\ln\nu},\end{split}\notag
\end{gather}
where \(L_\nu\) is the specific luminosity per unit frequency, \(R_\nu\) is the filter response function per photon at frequency \(\nu\), \(\nu_c\) is the central wavelength of the filter, and \(\beta\) is a constant that is defined by convention for each filter, and is either 0, 1, or 2; usually it is 0 for optical and UV filters.
\begin{itemize}
\item {} 
The wavelength-averaged luminosity across the filter, defined as

\end{itemize}
\begin{gather}
\begin{split}\langle L_\lambda\rangle_R = \frac{\int L_\lambda \, d\ln\lambda}{\int R_\lambda (\lambda/\lambda_c)^{-\beta} \, d\ln\lambda},\end{split}\notag
\end{gather}
where \(L_\lambda\) is the specific luminosity per unit wavelength, \(R_\lambda\) is the filter response function per photon at wavelength \(\lambda\), and \(\lambda_c\) is the central wavelength of the filter.
\begin{itemize}
\item {} 
The AB magnitude, defined by

\end{itemize}
\begin{gather}
\begin{split}M_{\rm AB} = -2.5 \log_{10} \left[\frac{\langle L_\nu\rangle_R}{4\pi\left(10\,\mathrm{pc}\right)^2}\right] - 48.6,\end{split}\notag
\end{gather}
where \(\langle L_\nu\rangle_R\) is in units of \(\mathrm{erg\,s}^{-1}\,\mathrm{Hz}^{-1}\).
\begin{itemize}
\item {} 
The ST magnitude, defined by

\end{itemize}
\begin{gather}
\begin{split}M_{\rm ST} = -2.5 \log_{10} \left[\frac{\langle L_\lambda\rangle_R}{4\pi\left(10\,\mathrm{pc}\right)^2}\right] - 21.1,\end{split}\notag
\end{gather}
where \(\langle L_\lambda\rangle_R\) is in units of \(\mathrm{erg\, s}^{-1}\,\mathrm{Angstrom}^{-1}\).
\begin{itemize}
\item {} 
The Vega magnitude, defined by

\end{itemize}
\begin{gather}
\begin{split}M_{\rm Vega} = M_{\rm AB} - M_{\rm AB}(\mbox{Vega}),\end{split}\notag
\end{gather}
where \(M_{\rm AB}(\mbox{Vega})\) is the AB magnitude of Vega. The latter quantity is computed on the fly, using a stored Kurucz model spectrum for Vega.
\begin{itemize}
\item {} 
The photon flux above some threshold \(\nu_0\), defined as

\end{itemize}
\begin{gather}
\begin{split}Q(\nu_0) = \int_{\nu_0}^\infty \frac{L_\nu}{h\nu} \, d\nu.\end{split}\notag
\end{gather}\begin{itemize}
\item {} 
The bolometric luminosity,

\end{itemize}
\begin{gather}
\begin{split}L_{\rm bol} = \int_0^\infty L_\nu \, d\nu.\end{split}\notag
\end{gather}
If nebular processing and/or extinction are enabled, photometric quantities are computed separately for each available version of the spectrum, \(L_\lambda\), \(L_{\lambda,\mathrm{neb}}\), \(L_{\lambda,\mathrm{ex}}\), and \(L_{\lambda,\mathrm{neb,ex}}\).

For a cluster simulation, this procedure is applied to the star cluster being simulated at a user-specified set of output times. For a galaxy simulation, the procedure is much the same, but it can be done both for all the stars in the galaxy taken as a whole, and individually for each star cluster that is still present (i.e., that has not been disrupted).


\section{Monte Carlo Simulation}
\label{intro:monte-carlo-simulation}
The steps described in the previous two section are those required for a single realization of the stellar population. However, the entire point of SLUG is to repeat this procedure many times in order to build up the statistics of the population light output. Thus the entire procedure can be repeated as many times as the user desires.


\section{Nebular Processing}
\label{intro:ssec-nebula}\label{intro:nebular-processing}
SLUG includes methods for post-processing the output starlight to compute the light that will emerge from the HII region around star clusters, and to further apply extinction to that light.

Nebular emission is computed by assuming that, for stars / star clusters younger than 10 Myr, all the ionizing photons are absorbed in a uniform-density, uniform-temperature HII region around each star cluster / star, and then computing the resulting emission at non-ionizing energies. The calculation assumes that the HII region is in photoionization equilibrium, and consists of hydrogen that is fully ionized and helium that is singly ionized. Under these assumptions the volume \(V\), electron density \(n_e\), and hydrogen density \(n_{\mathrm{H}}\) are related to the hydrogen ionizing luminosity \(Q(\mathrm{H}^0)\) via
\begin{gather}
\begin{split}\phi Q(\mathrm{H}^0) = \alpha_{\mathrm{B}}(T) n_e n_{\mathrm{H}} V\end{split}\notag
\end{gather}
Here \(\phi\) is the fraction of ionizing photons that are absorbed by hydrogen rather than dust grains, and \(\alpha_{\mathrm{B}}(T)\) is the temperature-dependent case B recombination rate coefficient. SLUG approximates \(\alpha_{\mathrm{B}}(T)\) using the analytic approximation given by equation 14.6 of \href{http://adsabs.harvard.edu/abs/2011piim.book.....D}{Draine (2011, Physics of the Interstellar and Intergalactic Medium, Princeton University Press)}, while \(\phi\) is a user-chosen parameter. The temperature can either be set by the user directly, or can be looked up automatically based on the age of the stellar population.

The relation above determines \(n_e n_{\mathrm{H}} V\), and from this SLUG computes the nebular emission including the following processes:
\begin{itemize}
\item {} 
\(\mathrm{H}^+\) and \(\mathrm{He}^+\) free-free emission

\item {} 
\(\mathrm{H}\) and \(\mathrm{He}\) bound-free emission

\item {} 
Hydrogen 2-photon emission

\item {} 
Hydrogen recombination lines from all lines with upper levels \(n_u \leq 25\)

\item {} 
Non-hydrogen line emission based on a tabulation (see below)

\end{itemize}

Formally, the luminosity per unit wavelength is computed as
\begin{gather}
\begin{split}L_{\lambda,\mathrm{neb}} = \left[\gamma_{\mathrm{ff}}^{(\mathrm{H})} + \gamma_{\mathrm{bf}}^{(\mathrm{H})} + \gamma_{\mathrm{2p}}^{(\mathrm{H})} + \sum_{n,n' \leq 25, n<n'} \alpha_{nn'}^{\mathrm{eff,B,(H)}} E_{nn'}^{(\mathrm{H})} +  x_{\mathrm{He}} \gamma_{\mathrm{ff}}^{(\mathrm{He})} +  x_{\mathrm{He}} \gamma_{\mathrm{bf}}^{(\mathrm{He})} + \sum_i \gamma_{i,\mathrm{line}}^{(\mathrm{M})}\right] n_e n_{\mathrm{H}}{V}\end{split}\notag
\end{gather}
Here \(n_e n_{\mathrm{H}} V = \phi_{\mathrm{dust}} Q(\mathrm{H}^0)/ \alpha_{\mathrm{B}}(T)\) from photoionization equilibrium, \(E_{nn'}\) is the energy difference between hydrogen levels \(n\) and \(n'\), and the remaining terms and their sources appearing in this equation are:
\begin{itemize}
\item {} 
\(\gamma_{\mathrm{ff}}^{(\mathrm{H})}\) and \(\gamma_{\mathrm{ff}}^{(\mathrm{He})}\): HII and HeII free-free emission coefficients; these are computed from eqution 10.1 of \href{http://adsabs.harvard.edu/abs/2011piim.book.....D}{Draine (2011)}, using the analytic approximation to the Gaunt factor given by equation 10.8 of the same source

\item {} 
\(\gamma_{\mathrm{bf}}^{(\mathrm{H})}\) and \(\gamma_{\mathrm{bf}}^{(\mathrm{He})}\): HI and HeI bound-free emission coefficients; these are computed using the tabulation and interpolation method given in \href{http://adsabs.harvard.edu/abs/2006MNRAS.372.1875E}{Ercolano \& Storey (2006, MNRAS, 372, 1875)}

\item {} 
\(\alpha_{nn'}^{\mathrm{eff,B,(H)}}\) is the effective emission rate coefficient for the \(n\) to \(n'\) H recombination line, taken from the tabulation of \href{http://adsabs.harvard.edu/abs/1995MNRAS.272...41S}{Storey \& Hummer (1995, MNRAS, 272, 41)}

\item {} 
\(\gamma_{i,\mathrm{line}}^{(\mathrm{M})}\) is the emissivity for the brightest non-hydrogen lines, computed using a set of pre-tabulated values, following the procedure described in the \href{http://adsabs.harvard.edu/abs/2015arXiv150205408K}{SLUG 2 method paper}

\item {} 
\(\gamma_{\mathrm{2p}}^{(\mathrm{H})}\): hydrogen two-photon emissivity, computed as

\end{itemize}
\begin{gather}
\begin{split}\gamma_{\mathrm{2p}}^{(\mathrm{H})} = \frac{hc}{\lambda^3} I(\mathrm{H}^0) \alpha_{2s}^{\mathrm{eff,(H)}} \frac{1}{1 + \frac{n_{\mathrm{H}} q_{2s-2p,p} + (1+x_{\mathrm{He}}) n_{\mathrm{H}} q_{2s-2p,e}}{A_{2s-1s}}} P_\nu\end{split}\notag
\end{gather}
Here
\begin{itemize}
\item {} 
\(I(\mathrm{H}^0)\) is the hydrogen ionization potential

\item {} 
\(\alpha_{2s}^{\mathrm{eff,(H)}}\) is the effective recombination rate to the 2s state, taken from the tabulation of \href{http://adsabs.harvard.edu/abs/1995MNRAS.272...41S}{Storey \& Hummer (1995, MNRAS, 272, 41)}

\item {} 
\(q_{2s-2p,p}\) and \(q_{2s-2p,e}\) are the collisional rate coefficients for transitions from the 2s to the 2p state induced by collisions with protons and electrons, respectively, taken from \href{http://adsabs.harvard.edu/abs/1989agna.book.....O}{Osterbrock (1989, University Science Books, table 4.10)}

\item {} 
\(A_{2s-1s}\) is the Einstein coefficient for the hydrogen 2s-1s two-photon emission process, taken from \href{http://adsabs.harvard.edu/abs/2011piim.book.....D}{Draine (2011, section 14.2.4)}

\item {} 
\(P_\nu\) is the frequency distribution for two-photon emission, computed from the analytic approximation of \href{http://adsabs.harvard.edu/abs/1984A\%26A...138..495N}{Nussbaumer \& Schmutz (1984, A\&A, 138, 495)}

\end{itemize}


\section{Extinction}
\label{intro:extinction}\label{intro:ssec-extinction}
If extinction is enabled, SLUG applies extinction to the stellar spectra and, if nebular processing is enabled as well, to the spectrum that emerges from the nebula. Note that the nebular plus extincted spectrum computation is not fully self-consistent, in that the dust absorption factor \(\phi_{\mathrm{dust}}\) used in the nebular emission calculation (see {\hyperref[intro:ssec-nebula]{\emph{\DUspan{}{Nebular Processing}}}}) is not affected by the value of \(A_V\) used in the calculation.

SLUG computes the extincted spectrum as
\begin{gather}
\begin{split}L_{\lambda,\mathrm{ex}} = L_{\lambda} e^{-\tau_\lambda}\end{split}\notag
\end{gather}
where the optical depth \(\tau_\lambda = (\kappa_\lambda / \kappa_V) (A_V/1.086)\), \(A_V\) is the visual extinction in mag, the factor 1.086 is the conversion between magnitudes and the true dimensionless optical depth, \(\kappa_\lambda\) is a user-specified input extinction at wavelength \(\lambda\), and the V-band mean opacity is defined by
\begin{gather}
\begin{split}\kappa_V = \frac{\int \kappa_\nu R_\nu(V) \, d\nu}{\int R_\nu(V) \, d\nu}\end{split}\notag
\end{gather}
where \(R_\nu(V)\) is the filter response function as frequency \(\nu\) for the Johnson V filter. The extinction curve \(\kappa_\lambda\) can be specified via a user-provided file, or the user may select from a set of pre-defined extinction curves; see {\hyperref[parameters:ssec-ism-keywords]{\emph{\DUspan{}{Interstellar Medium Model Keywords}}}} for details.

The computation for \(L_{\lambda,\mathrm{neb,ex}}\) is analogous.


\chapter{Compiling and Installing SLUG}
\label{compiling::doc}\label{compiling:compiling-and-installing-slug}

\section{Dependencies}
\label{compiling:dependencies}
The core SLUG program requires
\begin{itemize}
\item {} 
The \href{http://www.boost.org/}{Boost C++ libraries}

\item {} 
The \href{http://www.gnu.org/software/gsl/}{GNU scientific library}

\item {} 
The \href{http://heasarc.gsfc.nasa.gov/fitsio/fitsio.html}{cfitsio library} (optional, only required for FITS capabilities)

\end{itemize}

Compilation will be easiest if you install these libraries such that the header files are included in your \code{CXX\_INCLUDE\_PATH} and the compiled object files are in your \code{LD\_LIBRARY\_PATH}. Alternately, you can manually specify the locations of these files by editing the Makefiles -- see below. The cfitsio library is optional, and is only required if you want the ability to write FITS output. To compile without it, use the flag \code{FITS=DISABLE\_FITS} when calling \code{make} (see below). Note that SLUG uses some Boost libraries that must be built separately (see the Boost documentation on how to build and install Boost libraries).

In addition to the core dependencies, slugpy, the python helper library requires:
\begin{itemize}
\item {} 
\href{http://www.numpy.org/}{numpy}

\item {} 
\href{http://www.scipy.org/}{scipy}

\item {} 
\href{http://www.astropy.org/}{astropy} (optional, only required for FITS capabilities)

\end{itemize}

Finally, the cloudy coupling capability requires:
\begin{itemize}
\item {} 
\href{http://nublado.org}{cloudy}

\end{itemize}

This is only required performing cloudy runs, and is not required for any other part of SLUG.


\section{Compiling}
\label{compiling:compiling}
If you have boost, GSL, and (if you're using it) cfitsio included in your \code{CXX\_INCLUDE\_PATH} and \code{LD\_LIBRARY\_PATH} environment variables, and your system is running either MacOSX or Linux, you should be able to compile simply by doing:

\begin{Verbatim}[commandchars=\\\{\}]
make
\end{Verbatim}

from the main \code{slug} directory. To compile in debug mode, do:

\begin{Verbatim}[commandchars=\\\{\}]
make debug
\end{Verbatim}

instead. To compile without cfitsio, do:

\begin{Verbatim}[commandchars=\\\{\}]
make FITS=DISABLE\PYGZus{}FITS
\end{Verbatim}

Alternately, you can manually specify the compiler flags to be used by creating a file named \code{Make.mach.MACHINE\_NAME} in the \code{src} directory, and then doing:

\begin{Verbatim}[commandchars=\\\{\}]
make MACHINE=MACHINE\PYGZus{}NAME
\end{Verbatim}

An example machine-specific file, \code{src/Make.mach.ucsc-hyades} is included in the repository. You can also override or reset any compilation flag you want by editing the file \code{src/Make.config.override}.

Finally, note that SLUG is written in C++11, and requires some C++11 features, so it may not work with older C++ compilers. The following compiler versions are known to work: gcc \textgreater{}= 4.8 (4.7 works on most but not all platforms), clang/llvm \textgreater{}= 3.3, icc \textgreater{}= 14.0. Earlier versions may work as well, but no guarantees.


\chapter{Running a SLUG simulation}
\label{running::doc}\label{running:running-a-slug-simulation}
Once SLUG is compiled, running a simulation is extremely simple. The first step, which is not required but makes life a lot simpler, is to set the environment variable \code{SLUG\_DIR} to the directory where you have installed SLUG. If you are using a \code{bash}-like shell, the syntax for this is:

\begin{Verbatim}[commandchars=\\\{\}]
export SLUG\PYGZus{}DIR = /path/to/slug
\end{Verbatim}

while for a \code{csh}-like shell, it is:

\begin{Verbatim}[commandchars=\\\{\}]
setenv SLUG\PYGZus{}DIR /path/to/slug
\end{Verbatim}

This is helpful because SLUG needs a lot of input data, and if you don't set this variable, you will have to manually specify where to find it.

Next, to run on a single processor, just do:

\begin{Verbatim}[commandchars=\\\{\}]
./bin/slug param/filename.param
\end{Verbatim}

where \code{filename.param} is the name of a parameter file, formatted as specified in {\hyperref[parameters:sec-parameters]{\emph{\DUspan{}{Parameter Specification}}}}. The code will write a series of output files as described in {\hyperref[output:sec-output]{\emph{\DUspan{}{Output Files and Format}}}}.

If you have more than one core at your disposal, you can also run SLUG in parallel, using the command line:

\begin{Verbatim}[commandchars=\\\{\}]
python ./bin/slug.py param/filename.param
\end{Verbatim}

This called a python script that automatically divides up the Monte Carlo trials you have requested between the available processors, then consolidates the output so that it looks the same as if you had run a single-processor job. The python script allows fairly fine-grained control of the parallelism. It accepts the following command line arguments:
\begin{itemize}
\item {} 
\code{-n NPROC, -{-}nproc NPROC}: this parameter specifies the number of simultaneous SLUG processes to run. It defaults to the number of cores present on the machine where the code is running

\item {} 
\code{-b BATCHSIZE, -{-}batchsize BATCHSIZE}: this specifies how to many trials to do per SLUG process. It defaults to the total number of trials requested divided by the total number of processes, rounded up, so that only one SLUG process is run per processor. \emph{Rationale}: The default behavior is optimal from the standpoint of minimizing the overhead associated with reading data from disk, etc. However, if you are doing a very large number of runs that are going to require hours, days, or weeks to complete, and you probably want the code to checkpoint along the way. In that case it is probably wise to set this to a value smaller than the default in order to force output to be dumped periodically.

\item {} 
\code{-nc, -{-}noconsolidate}: by default the \code{slug.py} script will take all the outputs produced by the parallel runs and consolidate them into single output files, matching what would have been produced had the code been run in serial mode. If set, this flag suppresses that behavior, and instead leaves the output as a series of files whose root names match the model name given in the parameter file, plus the extension \code{\_pPPPPP\_nNNNNN}, where the digits \code{PPPPP} give the number of the processor that produces that file, and the digits \code{NNNNN} give the run number on that processor. \emph{Rationale}: normally consolidation is convenient. However, if the output is very large, this may produce undesirably bulky files. Furthermore, if one is doing a very large number of simulations over an extended period, and the \code{slug.py} script is going to be run multiple times (e.g.due to wall clock limits on a cluster), it may be preferable to leave the files unconsolidated until all runs have been completed.

\end{itemize}


\chapter{Parameter Specification}
\label{parameters:sec-parameters}\label{parameters::doc}\label{parameters:parameter-specification}

\section{Automated Parameter File Generation}
\label{parameters:automated-parameter-file-generation}
The remainder of this section contains information on how parameter files are formatted, and exactly how parameter choices specify code behavior. However, as a convenience SLUG comes with a python script that provides a simple menu-driven interface to write parameter files automatically. The script can be started by doing:

\begin{Verbatim}[commandchars=\\\{\}]
python bin/write\PYGZus{}param.py
\end{Verbatim}

Once started, the script provides a series of menus that allow the user to set all the keywords specified below. The script can then write a validly-formatted parameter file based on the options chosen.


\section{File Format}
\label{parameters:file-format}
An example parameter file is included as \code{param/example.param} in the source tree. Parameter files for SLUG are generically formatted as a series of entries of the form:

\begin{Verbatim}[commandchars=\\\{\}]
keyword    value
\end{Verbatim}

Any line starting with \code{\#} is considered to be a comment and is ignored, and anything on a line after a \code{\#} is similarly treated as a comment and ignored. Some general rules on keywords are:
\begin{itemize}
\item {} 
Keywords may appear in any order.

\item {} 
Some keywords have default values, indicated in parenthesis in the list below. These keywords are optional and need not appear in the parameter file. All others are required.

\item {} 
Keywords and values are case-insensitive.

\item {} 
Unless explicitly stated otherwise, units for mass are always \(M_\odot\), units for time are always yr.

\item {} 
Any time a file or directory is specified, if it is given as a relative rather than absolute path, it is assumed to be relative to the environment variable \code{\$SLUG\_DIR}. If this environment variable is not set, it is assumed to be relative to the current working directory.

\end{itemize}

The keywords recognized by SLUG can be categorized as described in the remainder of this section.


\section{Basic Keywords}
\label{parameters:basic-keywords}\label{parameters:ssec-basic-keywords}
These specify basic data for the run.
\begin{itemize}
\item {} 
\code{model\_name} (default: \code{SLUG\_DEF}): name of the model. This will become the base filename for the output files.

\item {} 
\code{out\_dir} (default: \code{output}): name of the directory into which output should be written.

\item {} 
\code{verbosity} (default: \code{1}): level of verbosity when running, with 0 indicating no output, 1 indicating some output, and 2 indicating a great deal of output.

\end{itemize}


\section{Simulation Control Keywords}
\label{parameters:simulation-control-keywords}
These control the operation of the simulation.
\begin{itemize}
\item {} 
\code{sim\_type} (default: \code{galaxy}): set to \code{galaxy} to run a galaxy simulation (a composite stellar population), or to \code{cluster} to run a cluster simulation (a simple stellar population)

\item {} 
\code{n\_trials} (default: \code{1}): number of trials to run

\item {} 
\code{log\_time} (default: \code{0}): set to 1 for logarithmic time step, 0 for linear time steps

\item {} 
\code{time\_step}: size of the time step. If \code{log\_time} is set to 0, this is in yr. If \code{log\_time} is set to 1, this is in dex (i.e., a value of 0.2 indicates that every 5 time steps correspond to a factor of 10 increase in time). Alternately, if \code{time\_step} is set to any value that cannot be converted to a real number, then this is interpreted as giving the name of a PDF file, which must be formatted as described in {\hyperref[pdfs:sec-pdfs]{\emph{\DUspan{}{Probability Distribution Functions}}}}. In this case one output time will be selected randomly for each trial from the specified PDF. This option is useful, for example, for generating a library of simulations that are randomly sampled in stellar population age. For the PDF option, the options \code{log\_time}, \code{start\_time} and \code{end\_time} will all be ignored, as the relevant parameters will be taken from the specified PDF file.

\item {} 
\code{start\_time}: first output time. This may be omitted if \code{log\_time} is set to 0, in which case it defaults to a value equal to \code{time\_step}.

\item {} 
\code{end\_time}: last output time, in yr. Note that not all the tracks include entries going out to times \textgreater{}1 Gyr, and the results will become inaccurate if the final time is larger than the tracks allow.

\item {} 
\code{sfr}: star formation rate. Only used if \code{sim\_type} is \code{galaxy}; for \code{cluster}, it will be ignored, and can be omitted. If, instead of specifying a numerical value for this parameter, you specify the string \code{sfh}, the code will interpret this as a flag that a star formation history should be read from the file specified by the \code{sfh} keyword.

\item {} 
\code{sfh}: name of star formation history file. This file is a PDF file, formatted as described in {\hyperref[pdfs:sec-pdfs]{\emph{\DUspan{}{Probability Distribution Functions}}}}. This is ignored, and can be omitted, if \code{sim\_type} is \code{cluster}, or if \code{sfr} is not set to \code{sfh}.

\item {} 
\code{cluster\_mass}: mass of the star cluster for simulations with \code{sim\_type} set to \code{cluster}. This can be omitted, and will be ignored, if \code{sim\_type} is \code{galaxy}. This parameter can be set to either a positive number or to the string \code{cmf}. If it is set to a numerical value, that value will be used as the cluster mass, in \(M_\odot\) for each trial. If it is set to \code{cmf}, then a new cluster mass will be drawn from the CMF for each trial.

\item {} 
\code{redshift} (default: \code{0}): place the system at the specified redshift. The computed spectra and photometry will then be computed in the observed rather than the rest frame of the system.

\end{itemize}


\section{Output Control Keywords}
\label{parameters:output-control-keywords}
These control what quantities are computed and written to disk. Full a full description of the output files and how they are formatted, see {\hyperref[output:sec-output]{\emph{\DUspan{}{Output Files and Format}}}}.
\begin{itemize}
\item {} 
\code{out\_cluster} (default: \code{1}): write out the physical properties of star clusters? Set to 1 for yes, 0 for no.

\item {} 
\code{out\_cluster\_phot} (default: \code{1}): write out the photometry of star clusters? Set to 1 for yes, 0 for no.

\item {} 
\code{out\_cluster\_spec} (default: \code{1}): write out the spectra of star clusters? Set to 1 for yes, 0 for no.

\item {} 
\code{out\_integrated} (default: \code{1}): write out the integrated physical properties of the whole galaxy? Set to 1 for yes, 0 for no. This keyword is ignored if \code{sim\_type} is \code{cluster}.

\item {} 
\code{out\_integrated\_phot} (default: \code{1}): write out the integrated photometry of the entire galaxy? Set to 1 for yes, 0 for no. This keyword is ignored if \code{sim\_type} is \code{cluster}.

\item {} 
\code{out\_integrated\_spec} (default: \code{1}): write out the integrated spectra of the entire galaxy? Set to 1 for yes, 0 for no. This keyword is ignored if \code{sim\_type} is \code{cluster}.

\item {} 
\code{output\_mode} (default: \code{ascii}): set to \code{ascii}, \code{binary}, or \code{fits}. Selecting \code{ascii} causes the output to be written in ASCII text, which is human-readable, but produces much larger files. Selecting \code{binary} causes the output to be written in raw binary. Selecting \code{fits} causes the output to be written FITS format. This will be somewhat larger than raw binary output, but the resulting files will be portable between machines, which the raw binary files are not guaranteed to be. All three output modes can be read by the python library, though with varying speed -- ASCII output is slowest, FITS is intermediate, and binary is fastest.

\end{itemize}


\section{Stellar Model Keywords}
\label{parameters:stellar-model-keywords}\label{parameters:ssec-stellar-keywords}
These specify the physical models to be used for stellar evolution, atmospheres, the IMF, extinction, etc.
\begin{itemize}
\item {} \begin{description}
\item[{\code{imf} (default: \code{lib/imf/chabrier.imf}): name of the IMF descriptor file; this is a PDF file, formatted as described in {\hyperref[pdfs:sec-pdfs]{\emph{\DUspan{}{Probability Distribution Functions}}}}. Note that SLUG ships with the following IMF files pre-defined (in the directory \code{lib/imf})}] \leavevmode\begin{itemize}
\item {} 
\code{chabrier.imf} (single-star IMF from \href{http://adsabs.harvard.edu/abs/2005ASSL..327...41C}{Chabrier, 2005, in ``The Initial Mass Function 50 Years Later'', eds. E. Corbelli, F. Palla, \& H. Zinnecker, Springer: Dordrecht, p. 41})

\item {} 
\code{chabrier03.imf} (single-star IMF from \href{http://adsabs.harvard.edu/abs/2003PASP..115..763C}{Chabrier, 2003, PASP, 115, 763-795})

\item {} 
\code{kroupa.imf} (IMF from \href{http://adsabs.harvard.edu/abs/2002Sci...295...82K}{Kroupa, 2002, Science, 295, 82-91})

\item {} 
\code{kroupa\_sb99.imf} (simplified version of the Kroupa, 2002 IMF used by default by \href{http://www.stsci.edu/science/starburst99/docs/default.htm}{starburst99})

\item {} 
\code{salpeter.imf} (single-component power law IMF from \href{http://adsabs.harvard.edu/abs/1955ApJ...121..161S}{Salpeter, 1955, ApJ, 121, 161})

\end{itemize}

\end{description}

\item {} 
\code{cmf} (default: \code{lib/cmf/slug\_default.cmf}): name of the CMF descriptor file; this is a PDF file, formatted as described in {\hyperref[pdfs:sec-pdfs]{\emph{\DUspan{}{Probability Distribution Functions}}}}. The default selection is a power law \(dN/dM \propto M^{-2}\) from \(M = 10^2 - 10^7\;M_\odot\). This is ignored, and may be omitted, if \code{sim\_type} is set to \code{cluster} and \code{cluster\_mass} is set to a numerical value.

\item {} 
\code{clf} (default: \code{lib/clf/slug\_default.clf}): name of the CLF descriptor file; this is a PDF file, formatted as described in {\hyperref[pdfs:sec-pdfs]{\emph{\DUspan{}{Probability Distribution Functions}}}}. The default gives a power law distribution of lifetimes \(t\) with \(dN/dt\propto t^{-1.9}\) from 1 Myr to 1 Gyr. Note that this corresponds to a cluster age distribution of slope -0.9. The SLUG source also ships with an alternative CLF file, \code{lib/clf/nodisrupt.clf}, which disables cluster disruption entirely (by setting the lifetime distribution to a \(\delta\) function at \(10^{300}\) yr).

\item {} \begin{description}
\item[{\code{tracks} (default: \code{lib/tracks/Z0140v00.txt}): stellar evolution tracks to use. The following tracks ship with SLUG (all in the directory \code{lib/tracks}):}] \leavevmode\begin{itemize}
\item {} 
\code{ZXXXXvYY.txt}: Geneva (2013) tracks; metallicities are Solar (\code{XXXX = 0140}) and 1/7 Solar (\code{XXXX = 0020}), and rotation rates are 0 (\code{YY = 00}) and 40\% of breakup (\code{YY = 40}).

\item {} 
\code{modcXXX.dat}: Geneva tracks with standard mass loss, for metallicities of \(2\times\) Solar (\code{040}), Solar (\code{020}), \(0.4\times\) Solar (\code{008}), \(0.2\times\) Solar (\code{004}), and \(0.05\times\) Solar (\code{001}).

\item {} 
\code{modeXXX.dat}: same as \code{modcXXX.dat}, but with higher mass loss rates.

\item {} 
\code{modpXXX.dat}: Padova tracks with thermally pulsing AGB stars; metallicities use the same scale as \code{modcXXX.dat} files (i.e., \code{020} is Solar).

\item {} 
\code{modsXXX.dat}: same as \code{modpXXX.dat}, but without thermally pulsing AGB stars

\end{itemize}

\end{description}

\item {} 
\code{atmospheres} (default: \code{lib/atmospheres}): directory where the stellar atmosphere library is located. Note that file names are hard-coded, so if you want to use different atmosphere models with a different format, you will have to write new source code to do so.

\item {} \begin{description}
\item[{\code{specsyn\_mode} (default: \code{sb99}): spectral synthesis mode. Allowed values are:}] \leavevmode\begin{itemize}
\item {} 
\code{planck}: treat all stars as black bodies

\item {} 
\code{Kurucz}: use Kurucz atmospheres, as compiled by \href{http://adsabs.harvard.edu/abs/1997A\%26AS..125..229L}{Lejeune et al. (1997, A\&AS, 125, 229)}, for all stars

\item {} 
\code{Kurucz+Hillier}: use Kurucz atmospheres for all stars except Wolf-Rayet stars; WR stars use Hillier model atmospheres (\href{http://adsabs.harvard.edu/abs/1998ApJ...496..407H}{Hillier \& Miller, 1998, ApJ, 496, 407})

\item {} 
\code{Kurucz+Pauldrach}: use Kurucz atmospheres for all stars except OB stars; OB stars use Pauldrach model atmospheres (\href{http://adsabs.harvard.edu/abs/2001A\%26A...375..161P}{Pauldrach et al., 2001, A\&A, 375, 161})

\item {} 
\code{SB99}: emulate the behavior of \code{starburst99}: use Pauldrach for OB stars, Hillier for WR stars, and Kurucz for all other stars

\end{itemize}

\end{description}

\item {} 
\code{clust\_frac} (default: \code{1.0}): fraction of stars formed in clusters

\item {} 
\code{min\_stoch\_mass} (default: \code{0.0}): minimum stellar mass to be treated stochastically. All stars with masses below this value are assumed to be sampled continuously from the IMF.

\item {} 
\code{metallicity}: metallicity of the stellar population, relative to solar. This may be omitted if \code{tracks} is set to one of the default sets of tracks that ships with SLUG, as the metallicities for these tracks are hardwired in. This keyword is provided to allow users to supply their own tracks.

\item {} 
\code{WR\_mass}: minimum starting mass that stars must have in order to pass through a Wolf-Rayet phase. This can be omitted if \code{tracks} is set to one of the default sets of tracks that ships with SLUG, as the WR cutoff masses for these tracks are hardwired in. This keyword is provided to allow users to supply their own tracks.

\end{itemize}


\section{Interstellar Medium Model Keywords}
\label{parameters:ssec-ism-keywords}\label{parameters:interstellar-medium-model-keywords}\begin{itemize}
\item {} 
\code{A\_V} (default: no extinction): extinction distribution. This parameter has three possible behaviors. If the parameter \code{A\_V} is omitted entirely, then the code will not compute extinction-corrected spectra or photometry at all; only unextincted values will be reported. If this parameter is specified as a real number, it will be interepreted as specifying a uniform extinction value \(A_V\), in mag, and this extinction will be applied to all predicted light output. Finally, if this parameter is a string that cannot be converted to a real number, it will be interpreted as the name of a PDF file, formatted as described in {\hyperref[pdfs:sec-pdfs]{\emph{\DUspan{}{Probability Distribution Functions}}}}, specifying the probability distribution of \(A_V\) values, in mag.

\item {} \begin{description}
\item[{\code{extinction\_curve} (default: \code{lib/extinct/SB\_ATT\_SLUG.dat}) file specifying the extinction curve; the file format is two columns of numbers in ASCII, the first giving the wavelength in Angstrom and the second giving the exintction \(\kappa_\nu\) at that wavelength / frequency in \(\mathrm{cm}^2\). Note that the absolute normalization of the exitnction curve is unimportant; only the wavelength-dependence matters (see {\hyperref[intro:ssec-spec-phot]{\emph{\DUspan{}{Spectra and Photometry}}}}). SLUG ships with the following extinction curves (all in \code{lib/extinct}):}] \leavevmode\begin{itemize}
\item {} 
\code{LMC\_EXT\_SLUG.dat} : LMC extinction curve; optical-UV from \href{http://adsabs.harvard.edu/abs/1999PASP..111...63F}{Fitzpatrick, E. L., 1999, PASP, 111, 63}, IR from \href{http://adsabs.harvard.edu/abs/1984A\%26A...134..284L}{Landini, M., et al., 1984, A\&A, 134, 284}; parts combined by D. Calzetti

\item {} 
\code{MW\_EXT\_SLUG.dat} : MW extinction curve; optical-UV from \href{http://adsabs.harvard.edu/abs/1999PASP..111...63F}{Fitzpatrick, E. L., 1999 PASP, 111, 63}, IR from \href{http://adsabs.harvard.edu/abs/1984A\%26A...134..284L}{Landini, M., et al., 1984, A\&A, 134, 284}; parts combined by D. Calzetti

\item {} 
\code{SB\_ATT\_SLUG.dat} : ``starburst'' extinction curve from \href{http://adsabs.harvard.edu/abs/2000ApJ...533..682C}{Calzetti, D., et al., 2000, ApJ, 533, 682}

\item {} 
\code{SMC\_EXT\_SLUG.dat} : SMC extinction curve from \href{http://adsabs.harvard.edu/abs/1985A\%26A...149..330B}{Bouchet, P., et al., 1985, A\&A, 149, 330}

\end{itemize}

\end{description}

\item {} 
\code{compute\_nebular} (default: \code{1}): compute the spectrum that results after starlight is processed through the nebula surrounding each cluster or star? Set to 1 for yes, 0 for no.

\item {} 
\code{atomic\_dir} (default: \code{lib/atomic/}): directory where the atomic data used for nebular emission calculations is located

\item {} 
\code{nebular\_no\_metals} (default: 0): if set to 1, metal lines are not used when computing nebular emission

\item {} 
\code{nebular\_den} (default: \code{1e2}): hydrogen number density in \(\mathrm{cm}^{-3}\) to use in nebular emission computations

\item {} 
\code{nebular\_temp} (default: \code{-1}): gas kinetic temperature in K to use in nebular emission computations; if set to non-positive value, the temperature will be determined via the lookup table of cloudy runs for fully sampled IMFs

\item {} 
\code{nebular\_logU} (default: \code{-3}): log of dimensionless volume-weighted ionization parameter to assume when computing metal line emission and HII region temperatures from the tabulated cloudy data. At present the allowed values are -3, -2.5, and -2.

\item {} 
\code{nebular\_phi} (default: \code{0.73}): fraction of ionizing photons absorbed by H atoms rather than being absorbed by dust grains or rescaping; the default value of \code{0.73}, taken from \href{http://adsabs.harvard.edu/abs/1997ApJ...476..144M}{McKee \& Williams (1997, ApJ, 476, 144)} means that 73\% of ionizing photons are absorbed by H

\end{itemize}


\section{Photometric Filter Keywords}
\label{parameters:ssec-phot-keywords}\label{parameters:photometric-filter-keywords}
These describe the photometry to be computed. Note that none of these keywords have any effect unless \code{out\_integrated\_phot} or \code{out\_cluster\_phot} is set to 1.
\begin{itemize}
\item {} \begin{description}
\item[{\code{phot\_bands}: photometric bands for which photometry is to be computed. The values listed here can be comma- or whitespace-separated. For a list of available photometric filters, see the file \code{lib/filters/FILTER\_LIST}. In addition to these filters, SLUG always allows four special ``bands'':}] \leavevmode\begin{itemize}
\item {} 
\code{QH0}: the \(\mathrm{H}^0\) ionizing luminosity, in photons/sec

\item {} 
\code{QHe0}: the \(\mathrm{He}^0\) ionizing luminosity, in photons/sec

\item {} 
\code{QHe1}: the \(\mathrm{He}^+\) ionizing luminosity, in photons/sec

\item {} 
\code{Lbol}: the bolometric luminosity, in \(L_\odot\)

\end{itemize}

\end{description}

\item {} 
\code{filters} (default: \code{lib/filters}): directory containing photometric filter data

\item {} \begin{description}
\item[{\code{phot\_mode} (default: \code{L\_nu}): photometric system to be used when writing photometric outputs. Full definitions of the quantities computed for each of the choices listed below are given in {\hyperref[intro:ssec-spec-phot]{\emph{\DUspan{}{Spectra and Photometry}}}}. Note that these values are ignored for the four special bands \code{QH0}, \code{QHe0}, \code{QHe1}, and \code{Lbol}. These four bands are always written out in the units specified above. Allowed values are:}] \leavevmode\begin{itemize}
\item {} 
\code{L\_nu}: report frequency-averaged luminosity in the band, in units of erg/s/Hz

\item {} 
\code{L\_lambda}: report wavelength-averaged luminosity in the band, in units of erg/s/Angstrom

\item {} 
\code{AB}: report AB magnitude

\item {} 
\code{STMAG}: report ST magnitude

\item {} 
\code{VEGA}: report Vega magnitude

\end{itemize}

\end{description}

\end{itemize}


\chapter{Probability Distribution Functions}
\label{pdfs:probability-distribution-functions}\label{pdfs::doc}\label{pdfs:sec-pdfs}
The SLUG code regards the IMF, the CMF, the CLF, the SFH, and the extinction \(A_V\) as probability distribution functions -- see {\hyperref[intro:ssec-slugpdfs]{\emph{\DUspan{}{Probability Distribution Functions: the IMF, SFH, CMF, CLF, A\_V distribution}}}}. The code provides a generic file format through which PDFs can be specified. Examples can be found in the \code{lib/imf}, \code{lib/cmf}, \code{lib/clf}, and \code{lib/sfh} directories of the SLUG distribution.

PDFs in SLUG are generically written as functions
\begin{gather}
\begin{split}\frac{dp}{dx} = n_1 f_1(x; x_{1,a}, x_{1,b}) + n_2 f_2(x; x_{2,a}, x_{2,b}) + n_3 f_3(x; x_{3,a}, x_{3,b}) + \cdots,\end{split}\notag
\end{gather}
where \(f_i(x; x_{i,a}, x_{i,b})\) is non-zero only for \(x \in [x_{i,a}, x_{i,b}]\). The functions \(f_i\) are simple continuous functional forms, which we refer to as \emph{segments}. Functions in this form can be specified in SLUG in two ways.


\section{Basic Mode}
\label{pdfs:basic-mode}
The most common way of specifying a PDF is in basic mode. Basic mode describes a PDF that has the properties that
\begin{enumerate}
\item {} 
the segments are contiguous with one another, i.e., \(x_{i,b} = x_{i+1,a}\)

\item {} 
\(n_i f_i(x_{i,b}; x_{i,a}, x_{i,b}) = n_{i+1} f_{i+1}(x_{i+1,a}; x_{i+1,a}, x_{i+1,b})\)

\item {} 
the overall PDF is normalized such that \(\int (dp/dx)\, dx = 1\)

\end{enumerate}

Given these constraints, the PDF can be specified fully simply by giving the \(x\) values that define the edges of the segments and the functional forms \(f\) of each segment; the normalizations can be computed from the constraint equations. Note that SFH PDFs cannot be described using basic mode, because they are not normalized to unity. Specifying a non-constant SFH requires advanced mode.

An example of a basic mode PDF file is as follows:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}
\PYGZsh{} This is an IMF definition file for SLUG v2.
\PYG{g+gh}{\PYGZsh{} This file defines the Chabrier (2005) IMF}
\PYG{g+gh}{\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}}

\PYGZsh{} Breakpoints: mass values where the functional form changes
\PYGZsh{} The first and last breakpoint will define the minimum and
\PYGZsh{} maximum mass
breakpoints 0.08 1 120

\PYGZsh{} Definitions of segments between the breakpoints

\PYGZsh{} This segment is a lognormal with a mean of log\PYGZus{}10 (0.2 Msun)
\PYGZsh{} and dispersion 0.55; the dispersion is in log base 10, not
\PYGZsh{} log base e
segment
type lognormal
mean 0.2
disp 0.55

\PYGZsh{} This segment is a powerlaw of slope \PYGZhy{}2.35
segment
type powerlaw
slope \PYGZhy{}2.35
\end{Verbatim}

This example represents a \href{http://adsabs.harvard.edu/abs/2005ASSL..327...41C}{Chabrier (2005)} IMF from \(0.08 - 120\) \(M_\odot\), which is of the functional form
\begin{gather}
\begin{split}\frac{dp}{dm} \propto \left\{\begin{array}{ll} \exp[-\log(m/m_0)^2/(2\sigma^2)] (m/m_b)^{-1} , & m < m_b \\ \exp[-\log(m_b/m_0)^2/(2\sigma^2)] (m/m_b)^{-2.35}, & m \geq m_b \end{array} \right.,\end{split}\notag
\end{gather}
where \(m_0 = 0.2\) \(M_\odot\), \(\sigma = 0.55\), and \(m_b = 1\) \(M_\odot\).

Formally, the format of a basic mode file is as follows. Any line beginning with \code{\#} is a comment and is ignored. The first non-empty, non-comment line in a basic mode PDF file must be of the form:

\begin{Verbatim}[commandchars=\\\{\}]
breakpoints x1 x2 x3 ...
\end{Verbatim}

where \code{x1}, \code{x2}, \code{x3}, \code{...} are a non-decreasing series of real numbers. These represent the breakpoints that define the edges of the segment, in units of \(M_\odot\). In the example given above, the breakpoints are are \(0.08\), \(1\), and \(120\), indicating that the first segment goes from \(0.08 - 1\) \(M_\odot\), and the second from \(1 - 120\) \(M_\odot\).

After the \code{breakpoints} line, there must be a series of entries of the form:

\begin{Verbatim}[commandchars=\\\{\}]
segment
type TYPE
key1 VAL1
\PYG{g+gh}{key2 VAL2}
\PYG{g+gh}{...}
\end{Verbatim}

where \code{TYPE} specifies what functional form describes the segment, and \code{key1 VAL1}, \code{key2 VAL2}, etc. are a series of (key, value) pairs the define the free parameters for that segment. In the example above, the first segment is described as having a \code{lognormal} functional form, and the keywords \code{mean} and \code{disp} specify that the lognormal has a mean of 0.2 \(M_\odot\) and a dispersion of 0.55 in \(\log_{10}\). The second segment is of type \code{powerlaw}, and it has a slope of \(-2.35\). The full list of allowed segment types and the keywords that must be specified with them are listed in the {\hyperref[pdfs:tab-segtypes]{\emph{\DUspan{}{Segment Types}}}} Table. Keywords and segment types are case-insensitive. Where more than one keyword is required, the order is arbitrary.

The total number of segments must be equal to one less than the number of breakpoints, so that each segment is described. Note that it is not necessary to specify a normalization for each segment, as the segments will be normalized relative to one another automatically so as to guarantee that the overall function is continuous.


\begin{threeparttable}
\capstart\caption{Segment Types}
\label{pdfs:tab-segtypes}\label{pdfs:id1}
\begin{tabulary}{\linewidth}{|L|L|L|L|L|L|}
\hline
\textsf{\relax 
Name
} & \textsf{\relax 
Functional form
} & \textsf{\relax 
Keyword
} & \textsf{\relax 
Meaning
} & \textsf{\relax 
Keyword
} & \textsf{\relax 
Meaning
}\\
\hline
\code{delta}
 & 
\(\delta(x-x_a)\)
 &  &  &  & \\
\hline
\code{exponential}
 & 
\(\exp(-x/x_*)\)
 & 
\code{scale}
 & 
Scale length, \(x_*\)
 &  & \\
\hline
\code{lognormal}
 & 
\(x^{-1} \exp\{-[\log_{10}(x/x_0)]^2/2\sigma^2\}\)
 & 
\code{mean}
 & 
Mean, \(x_0\)
 & 
\code{disp}
 & 
Dispersion in \(\log_{10}\), \(\sigma\)
\\
\hline
\code{normal}
 & 
\(\exp[-(x-x_0)^2/2\sigma^2]\)
 & 
\code{mean}
 & 
Mean, \(x_0\)
 & 
\code{disp}
 & 
Dispersion, \(\sigma\)
\\
\hline
\code{powerlaw}
 & 
\(x^p\)
 & 
\code{slope}
 & 
Slope, \(p\)
 &  & \\
\hline
\code{schechter}
 & 
\(x^p \exp(-x/x_*)\)
 & 
\code{slope}
 & 
Slope, \(p\)
 & 
\code{xstar}
 & 
Cutoff, \(x_*\)
\\
\hline\end{tabulary}

\end{threeparttable}



\section{Advanced Mode}
\label{pdfs:advanced-mode}
In advanced mode, one has complete freedom to set all the parameters describing the PDF: the endpoints of each segment \(x_{i,a}\) and \(x_{i,b}\), the normalization of each segment \(n_i\), and the functional forms of each segment \(f_i\). This can be used to defined PDFs that are non-continuous, or that are overlapping; the latter option can be used to construct segments with nearly arbitrary functional forms, by constructing a Taylor series approximation to the desired functional form and then using a series of overlapping \code{powerlaw} segments to implement that series.

An example of an advanced mode PDF file is as follows:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}
\PYGZsh{} This is a SFH definition file for SLUG v2.
\PYGZsh{} This defines a SF history consisting of a series of
\PYGZsh{} exponentially\PYGZhy{}decaying bursts with a period of 100 Myr and
\PYGZsh{} a decay timescale of 10 Myr, with an amplitude chosen to
\PYG{g+gh}{\PYGZsh{} give a mean SFR of 10\PYGZca{}\PYGZhy{}3 Msun/yr.}
\PYG{g+gh}{\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}}

\PYGZsh{} Declare that this is an advanced mode file
advanced

\PYGZsh{} First exponential burst
segment
type exponential
min      0.0
max      1.0e8         \PYGZsh{} Go to 100 Myr
weight   1.0e5         \PYGZsh{} Form 10\PYGZca{}5 Msun of stars over 100 Myr
scale    1.0e7         \PYGZsh{} Decay time 10 Myr

\PYGZsh{} Next 4 bursts
segment
type exponential
min      1.0e8
max      2.0e8
weight   1.0e5
scale    1.0e7

segment
type exponential
min      2.0e8
max      3.0e8
weight   1.0e5
scale    1.0e7

segment
type exponential
min      3.0e8
max      4.0e8
weight   1.0e5
scale    1.0e7

segment
type exponential
min      4.0e8
max      5.0e8
weight   1.0e5
scale    1.0e7
\end{Verbatim}

This represents a star formation history that is a series of exponential bursts, separated by 100 Myr, with decay times of 10 Myr. Formally, this SFH follows the functional form
\begin{gather}
\begin{split}\dot{M}_* = n e^{-(t\,\mathrm{mod}\, P)/t_{\rm dec}},\end{split}\notag
\end{gather}
where \(P = 100\) Myr is the period and \(t_{\rm dec} = 10\) Myr is the decay time, from times \(0-500\) Myr. The normalization constant \(n\) is set by the condition that \((1/P) \int_0^P \dot{M}_* \,dt = 0.001\) \(M_\odot\;\mathrm{yr}^{-1}\), i.e., that the mean SFR averaged over a single burst period is 0.001 \(M_\odot\;\mathrm{yr}^{-1}\).

Formally, the format of an advanced mode file is as follows. First, all advanced mode files must start with the line:

\begin{Verbatim}[commandchars=\\\{\}]
advanced
\end{Verbatim}

to declare that the file is in advanced mode. After that, there must be a series of entries of the form:

\begin{Verbatim}[commandchars=\\\{\}]
segment
type TYPE
min MIN
max MAX
weight WEIGHT
key1 VAL1
\PYG{g+gh}{key2 VAL2}
\PYG{g+gh}{...}
\end{Verbatim}

The \code{type} keyword is exactly the same as in basic mode, as are the segment-specific parameter keywords \code{key1}, \code{key2}, \(\ldots\). The same functional forms, listed in the {\hyperref[pdfs:tab-segtypes]{\emph{\DUspan{}{Segment Types}}}} Table, are available as in basic mode. The additional keywords that must be supplied in advanced mode are \code{min}, \code{max}, and \code{weight}. The \code{min} and \code{max} keywords give the upper and lower limits \(x_{i,a}\) and \(x_{i,b}\) for the segment; the probability is zero outside these limits. The keyword \code{weight} specifies the integral under the segment, i.e., the weight \(w_i\) given for segment \(i\) is used to set the normalization \(n_i\) via the equation
\begin{gather}
\begin{split}w_i = n_i \int_{x_{i,a}}^{x_{i,b}} f_i(x) \, dx.\end{split}\notag
\end{gather}
In the case of a star formation history, as in the example above, the weight \(w_i\) of a segment is simply the total mass of stars formed in that segment. In the example given above, the first segment declaration sets up a PDF that with a minimum at 0 Myr, a maximum at 100 Myr, following an exponential functional form with a decay time of \(10^7\) yr. During this time, a total mass of \(10^5\) \(M_\odot\) of stars is formed.

Note that, for the IMF, CMF, and CLF, the absolute values of the weights to not matter, only their relative values. On the other hand, for the SFH, the absolute weight does matter.


\section{Sampling Methods}
\label{pdfs:sampling-methods}\label{pdfs:sampling-metod-label}
A final option allowed in both basic and advanced mode is a specification of the sampling method. The sampling method is a description of how to draw a population of objects from the PDF, when the population is specified as having a total sum \(M_{\rm target}\) (usually but not necessarily a total mass) rather than a total number of members \(N\); there are a number of ways to do this, which do not necessarily yield identical distributions, even for the same underlying PDF. To specify a sampling method, simply add the line:

\begin{Verbatim}[commandchars=\\\{\}]
method METHOD
\end{Verbatim}

to the PDF file. This line can appear anywhere except inside a \code{segment} specification, or before the \code{breakpoints} or \code{advanced} line that begins the file. The following values are allowed for \code{METHOD} (case-insensitive, as always):
\begin{itemize}
\item {} 
\code{stop\_nearest}: this is the default option: draw until the total mass of the population exceeds \(M_{\rm target}\). Either keep or exclude the final star drawn depending on which choice brings the total mass closer to the target value.

\item {} 
\code{stop\_before}: same as \code{stop\_nearest}, but the final object drawn is always excluded.

\item {} 
\code{stop\_after}: same as \code{stop\_nearest}, but the final object drawn is always kept.

\item {} 
\code{stop\_50}: same as \code{stop\_nearest}, but keep or exclude the final object with 50\% probability regardless of which choice gets closer to the target.

\item {} 
\code{number}: draw exactly \(N = M_{\rm target}/\langle M\rangle\) object, where \(\langle M\rangle\) is the expectation value for a single draw.

\item {} 
\code{poisson}: draw exactly \(N\) objects, where the value of \(N\) is chosen from a Poisson distribution with expectation value \(\langle N \rangle = M_{\rm target}/\langle M\rangle\)

\item {} 
\code{sorted\_sampling}: this method was introduced by \href{http://adsabs.harvard.edu/abs/2006MNRAS.365.1333W}{Weidner \& Kroupa (2006, MNRAS. 365, 1333)}, and proceeds in steps. One first draws exactly \(N= M_{\rm target}/\langle M\rangle\) as in the \code{number} method. If the resulting total mass \(M_{\rm pop}\) is less than \(M_{\rm target}\), the procedure is repeated recursively using a target mass \(M_{\rm target} - M_{\rm pop}\) until \(M_{\rm pop} > M_{\rm target}\). Finally, one sorts the resulting stellar list from least to most massive, and then keeps or removes the final, most massive star using a \code{stop\_nearest} policy.

\end{itemize}

See the file \code{lib/imf/wk06.imf} for an example of a PDF file with a \code{method} specification.


\chapter{Output Files and Format}
\label{output:output-files-and-format}\label{output::doc}\label{output:sec-output}
SLUG can produce 7 output files, though the actual number produced depends on the setting for the \code{out\_*} keywords in the parameter file. (Additional output files can be produced by {\hyperref[cloudy:sec-cloudy-slug]{\emph{\DUspan{}{cloudy\_slug: An Automated Interface to cloudy}}}}, and are documented in that section rather than here.)

The only file that is always produced is the summary file, which is named \code{MODEL\_NAME\_summary.txt}, where \code{MODEL\_NAME} is the value given by the \code{model\_name} keyword in the parameter file. This file contains some basic summary information for the run, and is always formatted as ASCII text regardless of the output format requested.

The other six output files all have names of the form \code{MODEL\_NAME\_xxx.ext}, where the extension \code{.ext} is one of \code{.txt}, \code{.bin}, or \code{.fits} depending on the \code{output\_mode} specified in the parameter file, and \code{xxx} is \code{integrated\_prop}, \code{integrated\_spec}, \code{integrated\_phot}, \code{cluster\_prop}, \code{cluster\_spec}, or \code{cluster\_phot}. The production of these output files is controlled by the parameters \code{out\_integrated}, \code{out\_integrated\_spec}, \code{out\_integrated\_phot}, \code{out\_cluster}, \code{out\_cluster\_spec}, and \code{out\_cluster\_phot} in the parameter file.

The easiest way to read these output files is with {\hyperref[slugpy:sec-slugpy]{\emph{\DUspan{}{slugpy -- The Python Helper Library}}}}, which can parse them and store the information in python structures. However, for users who wish to write their own parsers or examine the data directly, the format is documented below. The following conventions are used throughout, unless noted otherwise:
\begin{itemize}
\item {} 
Masses are in \(M_\odot\)

\item {} 
Times in year

\item {} 
Wavelengths are in Angstrom

\item {} 
Specific luminosities are in erg/s/Angstrom

\item {} 
For \code{binary} outputs, variable types refer to C++ types

\end{itemize}


\section{The \texttt{integrated\_prop} File}
\label{output:the-integrated-prop-file}
This file contains data on the bulk physical properties of the galaxy as a whole. It consists of a series of entries containing the following fields:
\begin{itemize}
\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{TargetMass}: target mass of stars in the galaxy up that time, if the IMF and SFH were perfectly sampled

\item {} 
\code{ActualMass}: actual mass of stars produced in the galaxy up to that time; generally not exactly equal to \code{TargetMass} due to finite sampling of the IMF and SFH

\item {} 
\code{LiveMass}: actual mass of stars produced in the galaxy up to that time, and which have not yet reached the end of their lives (as marked by the final entry in the stellar evolution tracks)

\item {} 
\code{ClusterMass}: actual mass of stars produced in the galaxy up to that time that are still members of non-disrupted clusters

\item {} 
\code{NumClusters}: number of non-disrupted clusters present in the galaxy at this time

\item {} 
\code{NumDisClust}: number of disrupted clusters present in the galaxy at this time

\item {} 
\code{NumFldStars}: number of field stars present in the galaxy at this time; this count only includes those stars being treated stochastically (see the parameter \code{min\_stoch\_mass} in {\hyperref[parameters:ssec-stellar-keywords]{\emph{\DUspan{}{Stellar Model Keywords}}}})

\end{itemize}

If \code{output\_mode} is \code{ascii}, these data are output in a series of columns, with different trials separated by lines of dashes. If \code{output\_mode} is \code{fits}, the data are stored as a FITS binary table extension, with one column for each of the variables above, plus an additional column giving the trial number for that entry. Both the ASCII- and FITS-formatted output should be fairly self-documenting.

For \code{binary} output, the file consists of a series of records containing the following variables
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{TargetMass} (\code{double})

\item {} 
\code{ActualMass} (\code{double})

\item {} 
\code{LiveMass} (\code{double})

\item {} 
\code{ClusterMass} (\code{double})

\item {} 
\code{NumClusters} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long})

\item {} 
\code{NumDisClust} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long})

\item {} 
\code{NumFldStars} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long})

\end{itemize}

There is one record of this form for each output time, with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial.


\section{The \texttt{integrated\_spec} File}
\label{output:the-integrated-spec-file}\label{output:ssec-int-spec-file}
This file contains data on the spectra of the entire galaxy, and consists of a series of entries containing the following fields:
\begin{itemize}
\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{Wavelength}: observed frame wavelength at which the stellar spectrum is evaluated

\item {} 
\code{L\_lambda}: specific luminosity at the specified wavelength, before extinction or nebular effects are applied

\item {} 
\code{Wavelength\_neb}: observed frame wavelength at which the stellar plus nebular spectrum is evaluated (present only if SLUG was run with nebular emission enabled)

\item {} 
\code{L\_lambda\_neb}: specific luminosity at the specified wavelength, after the light has been processed through the nebula (only present if SLUG was run with nebular emission enabled)

\item {} 
\code{Wavelength\_ex}: observed frame wavelength at which the extincted stellar spectrum is evaluated (present only if SLUG was run with extinction enabled)

\item {} 
\code{L\_lambda\_ex}: specific luminosity at the specified wavelength after extinction is applied, but without the effects of the nebula (only present if SLUG was run with extinction enabled)

\item {} 
\code{Wavelength\_neb\_ex}: observed frame wavelength at which the extincted stellar plus nebular spectrum is evaluated (present only if SLUG was run with nebular processing and  extinction enabled)

\item {} 
\code{L\_lambda\_neb\_ex}: specific luminosity at the specified wavelength, after the light is first processed by the nebular and then subjected to dust extinction (only present if SLUG was run with both extinction and nebular emission enabled)

\end{itemize}

If \code{output\_mode} is \code{ascii}, these data are output in a series of columns, with different trials separated by lines of dashes. In \code{ascii} mode, only a single \code{Wavelength} column is present, and for those wavelengths that are not included in one of the grids, some entries may be blank.

If \code{output\_mode} is \code{fits}, the output FITS file has two binary table extensions. The first table contains a field \code{Wavelength} listing the wavelengths at which the stellar spectra are given; if nebular emission was enabled in the SLUG calculation, there is also a field \code{Wavelength\_neb} giving the nebular wavelength grid, and if extinction was enabled the table has a field \code{Wavelength\_ex} listing the wavelengths at which the extincted spectrum is computed. If both nebular emission and extinction were included, the field \code{Wavelength\_neb\_ex} exists as well, giving the wavelength grid for that spectrum. The second table has three fields, \code{Trial}, \code{Time}, and \code{L\_lambda} giving the trial number, time, and stellar spectrum. It may also contain fields \code{L\_lambda\_neb}, \code{L\_lambda\_ex}, and \code{L\_lambda\_neb\_ex} giving the stellar plus nebular spectrum, extincted stellar spectrum, and extincted stellar plus nebular spectrum. Both the ASCII- and FITS-formatted output should be fairly self-documenting.

For binary output, the file is formatted as follows. The file starts with
\begin{itemize}
\item {} 
\code{Nebular} (\code{byte}): a single byte, with a value of 0 indicating that nebular processing was not enabled for this run, and a value of 1 indicating that it was enabled

\item {} 
\code{Extinct} (\code{byte}): a single byte, with a value of 0 indicating that extinction was not enabled for this run, and a value of 1 indicating that it was enabled

\item {} 
\code{NWavelength} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the stellar spectra

\item {} 
\code{Wavelength} (\code{NWavelength} entries of type \code{double})

\item {} 
\code{NWavelength\_neb} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the stellar plus nebular spectra; only present if \code{Nebular} is 1

\item {} 
\code{Wavelength\_neb} (\code{NWavelength\_neb} entries of type \code{double})

\item {} 
\code{NWavelength\_ex} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the extincted spectra; only present if \code{Extinct} is 1

\item {} 
\code{Wavelength\_ex} (\code{NWavelength\_ex} entries of type \code{double}); only present if \code{Extinct} is 1

\item {} 
\code{NWavelength\_neb\_ex} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the extincted nebular plus stellar spectra; only present if \code{Nebular} and \code{Extinct} are both 1

\item {} 
\code{Wavelength\_ex} (\code{NWavelength\_neb\_ex} entries of type \code{double}); only present if \code{Nebular} and \code{Extinct} are both 1

\end{itemize}

and then contains a series of records in the format
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{L\_lambda} (\code{NWavelength} entries of type \code{double})

\item {} 
\code{L\_lambda\_neb} (\code{NWavelength\_neb} entries of type \code{double}); only present if \code{Nebular} is 1

\item {} 
\code{L\_lambda\_ex} (\code{NWavelength\_ex} entries of type \code{double}); only present if \code{Extinct} is 1

\item {} 
\code{L\_lambda\_neb\_ex} (\code{NWavelength\_neb\_ex} entries of type \code{double}); only present if \code{Nebular} and \code{Extinct} are both 1

\end{itemize}

There is one such record for each output time, with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial.


\section{The \texttt{integrated\_phot} File}
\label{output:the-integrated-phot-file}\label{output:ssec-int-phot-file}
This file contains data on the photometric properties of the entire galaxy, and consists of a series of entries containing the following fields:
\begin{itemize}
\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{PhotFilter1}: photometric value through filter 1, where filters follow the order in which they are specified by the \code{phot\_bands} keyword; units depend on the value of \code{phot\_mode} (see {\hyperref[parameters:ssec-phot-keywords]{\emph{\DUspan{}{Photometric Filter Keywords}}}})

\item {} 
\code{PhotFilter2}

\item {} 
\code{PhotFilter3}

\item {} 
\code{...}

\item {} 
\code{PhotFilter1\_neb}: photometric value through filter 1 for the spectrum after nebular processing, in the same units as \code{PhotFilter1}; only present if SLUG was run with nebular processing enabled

\item {} 
\code{PhotFilter2\_neb}

\item {} 
\code{PhotFilter3\_neb}

\item {} 
\code{...}

\item {} 
\code{PhotFilter1\_ex}: photometric value through filter 1 for the extincted spectrum, in the same units as \code{PhotFilter1}; only present if SLUG was run with extinction enabled

\item {} 
\code{PhotFilter2\_ex}

\item {} 
\code{PhotFilter3\_ex}

\item {} 
\code{...}

\item {} 
\code{PhotFilter1\_neb\_ex}: photometric value through filter 1 for the spectrum after nebular processing and extinction, in the same units as \code{PhotFilter1}; only present if SLUG was run with both nebular processing and extinction enabled

\item {} 
\code{PhotFilter2\_neb\_ex}

\item {} 
\code{PhotFilter3\_neb\_ex}

\item {} 
\code{...}

\end{itemize}

If \code{output\_mode} is \code{ascii}, these data are output in a series of
columns, with different trials separated by lines of dashes. The
columns for photometry of the extincted spectrum are present only if
extinction was enabled when SLUG was run. Entries for some filters may
be left blank. If so, this indicates that the photon response function
provided for that filter extends beyond the wavelength range covered
by the provided extinction curve. Since the extincted spectrum cannot
be computed over the full range of the filter in this case, photometry
for that filter cannot be computed either.

If \code{output\_mode} is \code{fits}, the data are stored as a series of
columns in a binary table extension to the FITS file; the filter names
and units are included in the header information for the columns. If
SLUG was run with nebular emission enabled, for each filter \code{FILTERNAME}
there is a corresponding column \code{FILTERNAME\_neb} giving the photometric
value for the nebular-processed spectrum. Similarly, the columns
\code{FILTERNAME\_ex} and \code{FILTERNAME\_neb\_ex} give the photometric values
for the extincted stellar and stellar + nebular spectra, respectively.
Some of the extincted values may be \code{NaN}; this
indicates that the photon response function provided for that filter
extends beyond the wavelength range covered by the provided extinction
curve. In addition to the time and photometric filter values, the FITS
file contains a column specifying the trial number for that
entry. Both the ASCII- and FITS-formatted output should be fairly
self-documenting.

For binary output, the file is formatted as follows. The file starts with
\begin{itemize}
\item {} 
\code{NFilter} (stored as \code{ASCII text}): number of filters used

\item {} 
\code{FilterName} \code{FilterUnit} (\code{NFilter} entries stored as \code{ASCII
text}): the name and units for each filter are listed in ASCII, one
filter-unit pair per line

\item {} 
\code{Nebular} (\code{byte}): a single byte, with a value of 0 indicating
that nebular processing was not enabled for this run, and a value of 1
indicating that it was enabled

\item {} 
\code{Extinct} (\code{byte}): a single byte, with a value of 0 indicating
that extinction was not enabled for this run, and a value of 1
indicating that it was enabled

\end{itemize}

This is followed by a series of entries of the form
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{PhotFilter} (\code{NFilter} entries of type \code{double})

\item {} 
\code{PhotFilter\_neb} (\code{NFilter} entries of type \code{double}); only present if \code{Nebular} is 1.

\item {} 
\code{PhotFilter\_ex} (\code{NFilter} entries of type \code{double}); only present if \code{Extinct} is 1. Note that some values may be \code{NaN} if photometry could not be computed for that filter (see above).

\item {} 
\code{PhotFilter\_neb\_ex} (\code{NFilter} entries of type \code{double}); only present if \code{Nebular} and \code{Extinct} are both 1. Note that some values may be \code{NaN} if photometry could not be computed for that filter (see above).

\end{itemize}

There is one such record for each output time, with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial.


\section{The \texttt{cluster\_prop} File}
\label{output:the-cluster-prop-file}
This file contains data on the bulk physical properties of the non-disrupted star clusters in the galaxy, with one entry per cluster per time at which that cluster exists. Each entry contains the following fields
\begin{itemize}
\item {} 
\code{UniqueID}: a unique identifier number for each cluster that is preserved across times and output files

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{FormTime}: time at which that cluster formed

\item {} 
\code{Lifetime}: amount of time from birth to when the cluster will disrupt

\item {} 
\code{TargetMass}: target mass of stars in the cluster, if the IMF were perfectly sampled

\item {} 
\code{BirthMass}: actual mass of stars present in the cluster at formation

\item {} 
\code{LiveMass}: actual mass of stars produced in the cluster at this output time that have not yet reached the end of their lives (as marked by the final entry in the stellar evolution tracks)

\item {} 
\code{NumStar}: number of living stars in the cluster at this time; this count only includes those stars being treated stochastically (see the parameter \code{min\_stoch\_mass} in {\hyperref[parameters:ssec-stellar-keywords]{\emph{\DUspan{}{Stellar Model Keywords}}}})

\item {} 
\code{MaxStarMass}: mass of most massive star still living in the cluster; this only includes those stars being treated stochastically (see the parameter \code{min\_stoch\_mass} in {\hyperref[parameters:ssec-stellar-keywords]{\emph{\DUspan{}{Stellar Model Keywords}}}})

\item {} 
\code{A\_V}: visual extinction for that cluster, in mag; present only if SLUG was run with extinction enabled

\end{itemize}

If \code{output\_mode} is \code{ascii}, these data are output in a series of columns, with different trials separated by lines of dashes. If \code{output\_mode} is \code{fits}, the data are stored as a FITS binary table extension, with one column for each of the variables above, plus an additional column giving the trial number for that entry. Both the ASCII- and FITS-formatted output should be fairly self-documenting.

For \code{binary} output, the first entry in the file is a header containing
\begin{itemize}
\item {} 
\code{Extinct} (\code{byte}): a single byte, with a value of 0 indicating that extinction was not enabled for this run, and a value of 1 indicating that it was enabled

\end{itemize}

Thereafter, the file consists of a series of records, one for each output time, with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial. Each record consists of a header containing
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{NCluster} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): number of non-disrupted clusters present at this time

\end{itemize}

This is followed by \code{NCluster} entries of the following form:
\begin{itemize}
\item {} 
\code{UniqueID} (\code{unsigned long})

\item {} 
\code{FormationTime} (\code{double})

\item {} 
\code{Lifetime} (\code{double})

\item {} 
\code{TargetMass} (\code{double})

\item {} 
\code{BirthMass} (\code{double})

\item {} 
\code{LiveMass} (\code{double})

\item {} 
\code{NumStar} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long})

\item {} 
\code{MaxStarMass} (\code{double})

\item {} 
\code{A\_V} (\code{double}); present only if \code{Extinct} is 1

\end{itemize}


\section{The \texttt{cluster\_spec} File}
\label{output:the-cluster-spec-file}
This file contains the spectra of the individual clusters, and each entry contains the following fields:
\begin{itemize}
\item {} 
\code{UniqueID}: a unique identifier number for each cluster that is preserved across times and output files

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{Wavelength}: observed frame wavelength at which the stellar spectrum is evaluated

\item {} 
\code{L\_lambda}: specific luminosity at the specified wavelength, before extinction or nebular effects are applied

\item {} 
\code{Wavelength\_neb}: observed frame wavelength at which the stellar plus nebular spectrum is evaluated (present only if SLUG was run with nebular emission enabled)

\item {} 
\code{L\_lambda\_neb}: specific luminosity at the specified wavelength, after the light has been processed through the nebula (only present if SLUG was run with nebular emission enabled)

\item {} 
\code{Wavelength\_ex}: observed frame wavelength at which the extincted stellar spectrum is evaluated (present only if SLUG was run with extinction enabled)

\item {} 
\code{L\_lambda\_ex}: specific luminosity at the specified wavelength after extinction is applied, but without the effects of the nebula (only present if SLUG was run with extinction enabled)

\item {} 
\code{Wavelength\_neb\_ex}: observed frame wavelength at which the extincted stellar plus nebular spectrum is evaluated (present only if SLUG was run with nebular processing and  extinction enabled)

\item {} 
\code{L\_lambda\_neb\_ex}: specific luminosity at the specified wavelength, after the light is first processed by the nebular and then subjected to dust extinction (only present if SLUG was run with both extinction and nebular emission enabled)

\end{itemize}

If \code{output\_mode} is \code{ascii}, these data are output in a series of columns, with different trials separated by lines of dashes. The columns \code{L\_lambda\_neb}, \code{L\_lambda\_ex}, and \code{L\_lambda\_neb\_ex} are present only if SLUG was run with the appropriate options enabled. Some entries in these fields may be empty; see {\hyperref[output:ssec-int-spec-file]{\emph{\DUspan{}{The integrated\_spec File}}}}.

If \code{output\_mode} is \code{fits}, the output FITS file has two binary table extensions. The first table contains a field listing the wavelengths at which the spectra are given, in the same format as for {\hyperref[output:ssec-int-spec-file]{\emph{\DUspan{}{The integrated\_spec File}}}}. The second table has always contains the fields \code{UniqueId}, \code{Time}, \code{Trial}, and \code{L\_lambda} giving the cluster unique ID, time, trial number, and stellar spectrum. Depending on whether nebular processing and/or extinction were enabled when SLUG was run, it may also contain the fields \code{L\_lambda\_neb}, \code{L\_lambda\_ex}, and \code{L\_lambda\_neb\_ex} giving the nebular-processed, extincted, and nebular-processed plus extincted spectra. Both the ASCII- and FITS-formatted output should be fairly self-documenting.

Output in \code{binary} mode is formatted as follows.  The file starts with
\begin{itemize}
\item {} 
\code{Nebular} (\code{byte}): a single byte, with a value of 0 indicating that nebular processing was not enabled for this run, and a value of 1 indicating that it was enabled

\item {} 
\code{Extinct} (\code{byte}): a single byte, with a value of 0 indicating that extinction was not enabled for this run, and a value of 1 indicating that it was enabled

\item {} 
\code{NWavelength} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the stellar spectra

\item {} 
\code{Wavelength} (\code{NWavelength} entries of type \code{double})

\item {} 
\code{NWavelength\_neb} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the stellar plus nebular spectra; only present if \code{Nebular} is 1

\item {} 
\code{Wavelength\_neb} (\code{NWavelength\_neb} entries of type \code{double})

\item {} 
\code{NWavelength\_ex} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the extincted spectra; only present if \code{Extinct} is 1

\item {} 
\code{Wavelength\_ex} (\code{NWavelength\_ex} entries of type \code{double}); only present if \code{Extinct} is 1

\item {} 
\code{NWavelength\_neb\_ex} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the extincted nebular plus stellar spectra; only present if \code{Nebular} and \code{Extinct} are both 1

\item {} 
\code{Wavelength\_ex} (\code{NWavelength\_neb\_ex} entries of type \code{double}); only present if \code{Nebular} and \code{Extinct} are both 1

\end{itemize}

and then contains a series of records, one for each output time, with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial. Each record consists of a header containing
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{NCluster} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): number of non-disrupted clusters present at this time

\end{itemize}

This is followed by \code{NCluster} entries of the following form:
\begin{itemize}
\item {} 
\code{UniqueID} (\code{unsigned long})

\item {} 
\code{L\_lambda} (\code{NWavelength} entries of type \code{double})

\item {} 
\code{L\_lambda\_neb} (\code{NWavelength\_neb} entries of type \code{double}); only present if \code{Nebular} is 1

\item {} 
\code{L\_lambda\_ex} (\code{NWavelength\_ex} entries of type \code{double}); only present if \code{Extinct} is 1

\item {} 
\code{L\_lambda\_neb\_ex} (\code{NWavelength\_neb\_ex} entries of type \code{double}); only present if \code{Nebular} and \code{Extinct} are both 1

\end{itemize}


\section{The \texttt{cluster\_phot} File}
\label{output:the-cluster-phot-file}\label{output:ssec-cluster-phot-file}
This file contains the photometric values for the individual clusters. Each entry contains the following fields:
\begin{itemize}
\item {} 
\code{UniqueID}: a unique identifier number for each cluster that is preserved across times and output files

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{PhotFilter1}: photometric value through filter 1, where filters follow the order in which they are specified by the \code{phot\_bands} keyword; units depend on the value of \code{phot\_mode} (see {\hyperref[parameters:ssec-phot-keywords]{\emph{\DUspan{}{Photometric Filter Keywords}}}})

\item {} 
\code{PhotFilter2}

\item {} 
\code{PhotFilter3}

\item {} 
\code{...}

\item {} 
\code{PhotFilter1\_neb}: photometric value through filter 1 for the spectrum after nebular processing, in the same units as \code{PhotFilter1}; only present if SLUG was run with nebular processing enabled

\item {} 
\code{PhotFilter2\_neb}

\item {} 
\code{PhotFilter3\_neb}

\item {} 
\code{...}

\item {} 
\code{PhotFilter1\_ex}: photometric value through filter 1 for the extincted spectrum, in the same units as \code{PhotFilter1}; only present if SLUG was run with extinction enabled

\item {} 
\code{PhotFilter2\_ex}

\item {} 
\code{PhotFilter3\_ex}

\item {} 
\code{...}

\item {} 
\code{PhotFilter1\_neb\_ex}: photometric value through filter 1 for the spectrum after nebular processing and extinction, in the same units as \code{PhotFilter1}; only present if SLUG was run with both nebular processing and extinction enabled

\item {} 
\code{PhotFilter2\_neb\_ex}

\item {} 
\code{PhotFilter3\_neb\_ex}

\item {} 
\code{...}

\end{itemize}

If \code{output\_mode} is \code{ascii}, these data are output in a series of columns, with different trials separated by lines of dashes. Some of the extincted photometry columns may be blank; see {\hyperref[output:ssec-int-phot-file]{\emph{\DUspan{}{The integrated\_phot File}}}}.

If \code{output\_mode} is \code{fits}, the data are stored as a series of
columns in a binary table extension to the FITS file; the filter names
and units are included in the header information for the columns. If
SLUG was run with nebular emission enabled, for each filter \code{FILTERNAME}
there is a corresponding column \code{FILTERNAME\_neb} giving the photometric
value for the nebular-processed spectrum. Similarly, the columns
\code{FILTERNAME\_ex} and \code{FILTERNAME\_neb\_ex} give the photometric values
for the extincted stellar and stellar + nebular spectra, respectively.
Some of the extincted values may be \code{NaN}; this
indicates that the photon response function provided for that filter
extends beyond the wavelength range covered by the provided extinction
curve. In addition to the time and photometric filter values, the FITS
file contains a column specifying the trial number for that
entry. Both the ASCII- and FITS-formatted output should be fairly
self-documenting.

In \code{binary} output mode, the binary data file starts with
\begin{itemize}
\item {} 
\code{NFilter} (stored as \code{ASCII text}): number of filters used

\item {} 
\code{FilterName} \code{FilterUnit} (\code{NFilter} entries stored as \code{ASCII text}): the name and units for each filter are listed in ASCII, one filter-unit pair per line

\item {} 
\code{Nebular} (\code{byte}): a single byte, with a value of 0 indicating that nebular processing was not enabled for this run, and a value of 1 indicating that it was enabled

\item {} 
\code{Extinct} (\code{byte}): a single byte, with a value of 0 indicating that extinction was not enabled for this run, and a value of 1 indicating that it was enabled

\end{itemize}

and then contains a series of records, one for each output time , with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial. Each record consists of a header containing
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{NCluster} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): number of non-disrupted clusters present at this time

\end{itemize}

This is followed by \code{NCluster} entries of the following form:
\begin{itemize}
\item {} 
\code{UniqueID} (\code{unsigned long})

\item {} 
\code{PhotFilter} (\code{NFilter} entries of type \code{double})

\item {} 
\code{PhotFilter\_neb} (\code{NFilter} entries of type \code{double}); only present if \code{Nebular} is 1.

\item {} 
\code{PhotFilter\_ex} (\code{NFilter} entries of type \code{double}); only present if \code{Extinct} is 1. Note that some values may be \code{NaN} if photometry could not be computed for that filter (see above).

\item {} 
\code{PhotFilter\_neb\_ex} (\code{NFilter} entries of type \code{double}); only present if \code{Nebular} and \code{Extinct} are both 1. Note that some values may be \code{NaN} if photometry could not be computed for that filter (see above).

\end{itemize}


\chapter{Filters and Filter Data}
\label{filters:sec-filters}\label{filters::doc}\label{filters:filters-and-filter-data}
SLUG comes with a fairly extensive list of filters, adapted from the list maintained by Charlie Conroy as part of \href{https://code.google.com/p/fsps/}{fsps}. However, users may wish to add additional filters, and so the format of the filter list is documented here for convenience.

Filter data is stored in two ASCII text files, \code{FILTER\_LIST} and \code{allfilters.dat}, which are stored in the \code{lib/filters} directory. The \code{FILTER\_LIST} file is an index listing the available filters. In consists of five whitespace-separated columns. The first column is just an numerical index. The second is the name of the filter; this is the name that should be entered in the \code{phot\_bands} keyword (see {\hyperref[parameters:ssec-phot-keywords]{\emph{\DUspan{}{Photometric Filter Keywords}}}}) to request photometry in that filter. The third and fourth columns the value of \(\beta\) and \(\lambda_c\) (the central wavelength) for that filter -- see {\hyperref[intro:ssec-spec-phot]{\emph{\DUspan{}{Spectra and Photometry}}}} for definitions. Anything after the fourth column is regarded as a comment, and can be used freely for a description of that filter.

The \code{allfilters.dat} file contains the filter responses. The file contains a series of entires for different filters, each delineated by a header line that begins with \code{\#}. The order in which filters appear in this file matches that in which they appear in the \code{FILTER\_LIST}. After the header line, are a series of lines each containing two numbers. The first is the wavelength in Angstrom, and the second is the filter response function at that wavelength.


\chapter{slugpy -- The Python Helper Library}
\label{slugpy:slugpy-the-python-helper-library}\label{slugpy:sec-slugpy}\label{slugpy::doc}

\section{Basic Usage}
\label{slugpy:basic-usage}
SLUG comes with the python module slugpy, which contains an extensive set of routines for reading, writing, and manipulating SLUG outputs. The most common task is to read a set of SLUG outputs into memory so that they can be processed. To read the data from a SLUG run using slugpy, one can simply do the following:

\begin{Verbatim}[commandchars=\\\{\}]
from slugpy import *
idata = read\PYGZus{}integrated(\PYGZsq{}SLUG\PYGZus{}MODEL\PYGZus{}NAME\PYGZsq{})
cdata = read\PYGZus{}cluster(\PYGZsq{}SLUG\PYGZus{}MODEL\PYGZus{}NAME\PYGZsq{})
\end{Verbatim}

The \code{read\_integrated} function reads all the integrated-light data (i.e., the data stored in the \code{\_integrated\_*} files -- see {\hyperref[output:sec-output]{\emph{\DUspan{}{Output Files and Format}}}}) for a SLUG output whose name is given as the argument. This is the base name specified by the \code{model\_name} keyword (see {\hyperref[parameters:ssec-basic-keywords]{\emph{\DUspan{}{Basic Keywords}}}}), without any extensions; the slugpy library will automatically determine which outputs are available and in what format, and read the appropriate files. It returns a \code{namedtuple} containing all the output data available for that simulation. Note that some of these fields will only be present if the cloudy-slug interface (see {\hyperref[cloudy:sec-cloudy-slug]{\emph{\DUspan{}{cloudy\_slug: An Automated Interface to cloudy}}}}) was used to process the SLUG output through cloudy to predict nebular emission, and some will be present only if extinction was enabled when SLUG was run. The fields returned are as follows:
\begin{itemize}
\item {} 
time: output times

\item {} 
target\_mass: target stellar mass at each time

\item {} 
actual\_mass: actual stellar mass at each time

\item {} 
live\_mass: mass of currently-alive stars

\item {} 
cluster\_mass: mass of living stars in non-disrupted clusters

\item {} 
num\_clusters: number of non-disrupted clusters

\item {} 
num\_dis\_clusters: number of disrupted clusters

\item {} 
num\_fld\_stars: number of still-living stars that formed in the field

\item {} 
wl: wavelengths of output stellar spectra (in Angstrom)

\item {} 
spec: integrated spectrum of all stars, expressed as a specific luminosity (erg/s/Angstrom)

\item {} 
filter\_names: list of photometric filter names

\item {} 
filter\_units: list of units for photometric outputs

\item {} 
filter\_wl\_eff: effective wavelength for each photometric filter

\item {} 
filter\_wl: list of wavelengths for each filter at which the response function is given (in Angstrom)

\item {} 
filter\_response: photon response function for each filter at each wavelength (dimensionless)

\item {} 
filter\_beta: index \(\beta\) used to set the normalization for each filter -- see {\hyperref[intro:ssec-spec-phot]{\emph{\DUspan{}{Spectra and Photometry}}}}

\item {} 
filter\_wl\_c: pivot wavelength used to set the normalization for each filter for which \(\beta \neq 0\) -- see {\hyperref[intro:ssec-spec-phot]{\emph{\DUspan{}{Spectra and Photometry}}}}

\item {} 
phot: photometry of the stars in each filter

\end{itemize}

The following fields are present only if SLUG was run with nebular processing enabled:
\begin{itemize}
\item {} 
wl\_neb: same as wl, but for the spectrum that emerges after the starlight has passed through the nebulae around the emitting clusters and field stars. The nebular grid is finer than the stellar grid, because it contains wavelength extra entries around prominent lines so that the lines are resolved on the grid

\item {} 
spec\_neb: same as spec, but for the nebular-processed spectrum

\item {} 
phot\_neb: same as phot, but for the nebular-processed spectrum

\end{itemize}

The following fields are present only if SLUG was run with extinction enabled:
\begin{itemize}
\item {} 
wl\_ex: wavelengths of output stellar spectra after extinction has been applied(in Angstrom). Note that wl\_ex will generally cover a smaller wavelength range than wl, because the extinction curve used may not cover the full wavelength range of the stellar spectra. Extincted spectra are computed only over the range covered by the extinction curve.

\item {} 
spec\_ex: same as spec, but for the extincted spectrum

\item {} 
phot\_ex: same as phot, but for the extincted spectrum. Note that some values may be \code{NaN}. This indicates that photometry of the extincted spectrum could not be computed for that filter, because the filter response curve extends to wavelengths outside the range covered by the extinction curve.

\end{itemize}

The following fields are present only if SLUG was run with both nebular processing and extinction enabled:
\begin{itemize}
\item {} 
wl\_neb\_ex: same as wl\_neb, but for the extincted, nebular-processed spectrum. Will be limited to the same wavelength range as wl\_ex.

\item {} 
spec\_neb\_ex: same as spec\_neb, but with extinction applied

\item {} 
phot\_neb\_ex: same as phot\_neb, but wtih extinction applied. Note that some values may be \code{NaN}. This indicates that photometry of the extincted spectrum could not be computed for that filter, because the filter response curve extends to wavelengths outside the range covered by the extinction curve.

\end{itemize}

The following fields are present only for runs that have been processed through the cloudy\_slug interface (see {\hyperref[cloudy:sec-cloudy-slug]{\emph{\DUspan{}{cloudy\_slug: An Automated Interface to cloudy}}}}):
\begin{itemize}
\item {} 
cloudy\_wl: wavelengths of the output nebular spectra (in Angstrom)

\item {} 
cloudy\_inc: incident stellar radiation field, expressed as a specific luminosity (erg/s/Angstrom) -- should be the same as spec, but binned onto cloudy's wavelength grid; provided mainly as a bug-checking diagnostic

\item {} 
cloudy\_trans: the transmitted stellar radiation field computed by cloudy, expressed as a specific luminosity (erg/s/Angstrom) -- this is the radiation field of the stars after it has passed through the HII region, and is what one would see in an observational aperture centered on the stars with negligible contribution from the nebula

\item {} 
cloudy\_emit: the emitted nebular radiation field computed by cloudy, expressed as a specific luminosity (erg/s/Angstrom) -- this is the radiation emitted by the nebula excluding the stars, and is what one would see in an observational aperture that included the nebula but masked out the stars

\item {} 
cloudy\_trans\_emit: the sum of the transmitted stellar and emitted nebular radiation, expressed as a specific luminosity (erg/s/Angstrom) -- this is what one would see in an observational aperture covering the both the stars and the nebula

\item {} 
cloudy\_linelabel: list of emitting species for the line luminosities computed by cloudy, following cloudy's 4-letter notation

\item {} 
cloudy\_linewl: wavelengths of all the lines computed by cloudy (in Angstrom)

\item {} 
cloudy\_linelum: luminosities of the lines computed by cloudy (in erg/s)

\item {} 
cloudy\_filter\_names, cloudy\_filter\_units, cloudy\_filter\_wl\_eff, cloudy\_filter\_wl, cloudy\_filter\_response, cloudy\_filter\_beta, cloudy\_filter\_wl\_c: exactly the same as the corresponding fields without the cloudy prefix, but for the photometric filters applied to the cloudy output

\item {} 
cloudy\_phot\_trans, cloudy\_phot\_emit, and cloudy\_phot\_trans\_emit: photometry of the transmitted, emitted, and transmitted+emitted radiation field provided by cloudy\_trans, cloudy\_emit, and cloudy\_trans\_emit

\end{itemize}

For the above fields, quantities that are different for each trial and each time are stored as numpy arrays with a shape (N\_times, N\_trials) for scalar quantities (e.g., actual\_mass), or a shape (N, N\_times, N\_trials) for quantities that are vectors of length N (e.g., the spectrum).

The \code{read\_cluster} function is analogous, except that instead of reading the whole-galaxy data, it reads data on the individual star clusters, as stored in the \code{\_cluster\_*} output files. It returns the following fields:
\begin{itemize}
\item {} 
id: a unique identifier number for each cluster; this is guaranteed to be unique across both times and trials, so that if two clusters in the list have the same id number, that means that the data given are for the same cluster at two different times in its evolution

\item {} 
trial: the trial number in which that cluster appeared

\item {} 
time: the time at which the data for that cluster are computed

\item {} 
form\_time: the time at which that cluster formed

\item {} 
lifetime: the between when the cluster formed and when it will disrupt

\item {} 
target\_mass: the target stellar mass of the cluster

\item {} 
actual\_mass: the actual stellar mass of the cluter

\item {} 
live\_mass: the mass of all still-living stars in the cluster

\item {} 
num\_star: the number of stars in the cluster

\item {} 
max\_star\_mass: the mass of the single most massive still-living star in the cluster

\item {} 
A\_V: the visual extinction for this cluster, in mag; present only if SLUG was run with extinction enabled

\item {} 
All the remaining fields are identical to those listed above for integrated quantities, starting with wl

\end{itemize}

For all these fields, scalar quantities that are different for each cluster (e.g., actual\_mass) will be stored as arrays of shape (N\_cluster); vector quantities that are different for each cluster (e.g., spec) will be stored as arrays of shape (N\_cluster, N).


\section{Full Documentation of slugpy}
\label{slugpy:module-slugpy}\label{slugpy:full-documentation-of-slugpy}\index{slugpy (module)}\index{combine\_cluster() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.combine_cluster}\pysiglinewithargsret{\code{slugpy.}\bfcode{combine\_cluster}}{\emph{data}}{}
Function to combine cluster data from multiple SLUG2 runs,
treating each input run as a separate set of trials. Trial and
cluster unique ID numbers are altered as necessary to avoid
duplication between the merged data sets.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}list\_like{]}
A list containing the cluster data for each run, as
returned by read\_cluster

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{combined\_data}] \leavevmode{[}namedtuple{]}
The combined data, in the same format as each object in data

\end{description}

\end{description}

\end{fulllineitems}

\index{combine\_integrated() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.combine_integrated}\pysiglinewithargsret{\code{slugpy.}\bfcode{combine\_integrated}}{\emph{data}}{}
Function to combine integrated data from multiple SLUG2 runs,
treating each input run as a separate set of trials.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}list\_like{]}
A list containing the integrated data for each run, as
returned by read\_integrated

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{combined\_data}] \leavevmode{[}namedtuple{]}
The combined data, in the same format as each object in data

\end{description}

\end{description}

\end{fulllineitems}

\index{compute\_photometry() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.compute_photometry}\pysiglinewithargsret{\code{slugpy.}\bfcode{compute\_photometry}}{\emph{wl}, \emph{spec}, \emph{filtername}, \emph{photsystem='L\_nu'}, \emph{filter\_wl=None}, \emph{filter\_response=None}, \emph{filter\_beta=None}, \emph{filter\_wl\_c=None}, \emph{filter\_dir=None}}{}
This function takes an input spectrum and a set of response
functions for photometric filters, and returns the photometry
through those filters.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{wl}] \leavevmode{[}array{]}
Wavelength of input spectrum in Angstrom

\item[{spec}] \leavevmode{[}array{]}
Specific luminosity per unit wavelength for input spectrum, in
erg/s/A

\item[{filtername}] \leavevmode{[}string or iterable of strings{]}
Name or list of names of the filters to be used. Filter names
can also include the special filters Lbol, QH0, QHe0, and QHe1;
the values returned for these will be the bolometric luminosity
(in erg/s) and the photon luminosities (in photons/s) in the H,
He, and He+ ionizing-continua, respectively.

\item[{photsystem}] \leavevmode{[}string{]}
The photometric system to use for the output. Allowable values
are `L\_nu', `L\_lambda', `AB', `STMAG', and `Vega',
corresponding to the options defined in the SLUG code.

\item[{filter\_wl}] \leavevmode{[}array or iterable of arrays{]}
Array giving the wavelengths in Angstrom at which the filter is
response function is given. If this object is an iterable of
arrays rather than a single array, it is assumed to represent
the wavelengths for a set of filters. If this is set,
no data is read from disk. Default behavior is to read the
filter information from disk.

\item[{filter\_response}] \leavevmode{[}array or iterable of arrays{]}
Array giving the filter response function at each wavelenght
and for each filter in filter\_wl. Must be set if filter\_wl is
set, ignored otherwise.

\item[{filter\_beta}] \leavevmode{[}iterable{]}
Array-like object containing the index beta for each
filter. Must be set if filter\_wl is set, ignored otherwise.

\item[{filter\_wl\_c}] \leavevmode{[}iterable{]}
Array-like object containing the pivot wavelength for each
filter. Must be set if filter\_wl is set, ignored otherwise.

\item[{filter\_dir}] \leavevmode{[}string{]}
Directory where the filter data files can be found. If left as
None, filters will be looked for in the \$SLUG\_DIR/lib/filters
directory. This parameter is used only if filtername is not
None.

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{phot}] \leavevmode{[}array{]}
Photometric values in the requested filters. Units depend on
the choice of photometric system:
L\_nu --\textgreater{} erg/s/Hz;
L\_lambda --\textgreater{} erg/s/A;
AB --\textgreater{} absolute AB magnitude;
STMAG --\textgreater{} absolute ST magnitude;
Vega --\textgreater{} absolute Vega magnitude;

\end{description}

\end{description}

\end{fulllineitems}

\index{photometry\_convert() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.photometry_convert}\pysiglinewithargsret{\code{slugpy.}\bfcode{photometry\_convert}}{\emph{photsystem}, \emph{phot}, \emph{units}, \emph{wl\_cen=None}, \emph{filter\_last=False}, \emph{filter\_names=None}, \emph{filter\_dir=None}}{}
Function to convert photometric data between photometric systems.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{photsystem}] \leavevmode{[}string{]}
The photometric system to which to convert. Allowable values
are `L\_nu', `L\_lambda', `AB', `STMAG', and `Vega',
corresponding to the options defined in the SLUG code. If this
is set and the conversion requested involves a conversion from
a wavelength-based system to a frequency-based one, wl\_cen must
not be None.

\item[{phot}] \leavevmode{[}array{]}
array of photometric data; if the array has more than one
dimension, the first dimension is assumed to represent the
different photometric filters (unless filter\_last is True,
in which case the last dimension is represents the array of
filters)

\item[{units}] \leavevmode{[}iterable of strings{]}
iterable listing the units of the input photometric data. On
return, strings will be changed to the units of the new system.

\item[{wl\_cen}] \leavevmode{[}array{]}
central wavelengths of the filters, in Angstrom; can be left as
None if the requested conversion doesn't require going between
wavelength- and frequency-based systems.

\item[{filter\_last}] \leavevmode{[}bool{]}
If the input data have more than one dimension, by default it
is assumed that the first dimension contains values for the
different photometric filters. If this keyword is set to True,
it will instead be assumed that the last dimension contains the
values for the different filters.

\item[{filter\_names}] \leavevmode{[}iterable of strings{]}
Names of all filters, used to read the filter response
functions from disk; only needed for conversions to and from
Vega magnitudes, and ignored otherwise

\item[{filter\_dir}] \leavevmode{[}string{]}
Directory where the filter data files can be found. If left as
None, filters will be looked for in the \$SLUG\_DIR/lib/filters
directory. This parameter is used only for conversions to
and from Vega magnitudes.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\item[{Raises}] \leavevmode
ValueError, if wl\_cen is None but the requested conversion
requires going between wavelength- and frequency-based systems

\end{description}

\end{fulllineitems}

\index{read\_cluster() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_cluster}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_cluster}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{nofilterdata=False}, \emph{photsystem=None}, \emph{verbose=False}, \emph{read\_filters=None}, \emph{read\_nebular=None}, \emph{read\_extinct=None}, \emph{read\_info=None}}{}
Function to read all cluster data for a SLUG2 run.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{nofilterdata}] \leavevmode{[}bool{]}
If True, the routine does not attempt to read the filter
response data from the standard location

\item[{photsystem}] \leavevmode{[}None or string{]}
If photsystem is None, the data will be returned in the same
photometric system in which they were read. Alternately, if it
is a string, the data will be converted to the specified
photometric system. Allowable values are `L\_nu', `L\_lambda',
`AB', `STMAG', and `Vega', corresponding to the options defined
in the SLUG code. If this is set and the conversion requested
involves a conversion from a wavelength-based system to a
frequency-based one, nofilterdata must be False so that the
central wavelength of the photometric filters is available.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_filters}] \leavevmode{[}None \textbar{} string \textbar{} listlike containing strings{]}
If this is None, photometric data on all filters is
read. Otherwise only filters whose name(s) match the input
filter names ar read.

\item[{read\_nebular}] \leavevmode{[}None \textbar{} bool{]}
If True, only photometric data with the nebular contribution
is read; if False, only data without it is read. Default
behavior is to read all data.

\item[{read\_extinct}] \leavevmode{[}None \textbar{} bool{]}
If True, only photometric data with extinction applied is
read; if False, only data without it is read. Default
behavior is to read all data.

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `prop\_name',
`phot\_name', `spec\_name', `cloudyspec\_name', `cloudylines\_name'
and `format', giving the names of the files read and the format
they were in; `format' will be one of `ascii', `binary', or
`fits'. If one of the files is not present, the corresponding
\_name key will be omitted from the dict.

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:

(Always present)
\begin{description}
\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
time at which cluster's properties are being evaluated

\end{description}

(Present if the run being read contains a cluster\_prop file)
\begin{description}
\item[{form\_time}] \leavevmode{[}array{]}
time when cluster formed

\item[{lifetime}] \leavevmode{[}array{]}
time at which cluster will disrupt

\item[{target\_mass}] \leavevmode{[}array{]}
target cluster mass

\item[{actual\_mass}] \leavevmode{[}array{]}
actual mass at formation

\item[{live\_mass}] \leavevmode{[}array{]}
mass of currently living stars

\item[{num\_star}] \leavevmode{[}array, dtype ulonglong{]}
number of living stars in cluster being treated stochastically

\item[{max\_star\_mass}] \leavevmode{[}array{]}
mass of most massive living star in cluster

\end{description}

(Present if the run being read contains a cluster\_spec file)
\begin{description}
\item[{wl}] \leavevmode{[}array{]}
wavelength, in Angstrom

\item[{spec}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of each cluster at each wavelength, in erg/s/A

\item[{wl\_neb}] \leavevmode{[}array{]}
wavelength for the nebular spectrum, in Angstrom (present
only if SLUG was run with nebular emission enabled)

\item[{spec\_neb}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity at each wavelength and each time for each
trial, including emission and absorption by the HII region,
in erg/s/A (present only if SLUG was run with nebular
emission enabled)

\item[{wl\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum, in Angstrom (present
only if SLUG was run with extinction enabled)

\item[{spec\_ex}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial after extinction has been applied, in
erg/s/A (present only if SLUG was run with extinction
enabled)

\item[{wl\_neb\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum with nebular emission,
in Angstrom (present only if SLUG was run with both nebular
emission and extinction enabled)

\item[{spec\_neb\_ex}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial including emission and absorption by the
HII region, after extinction has been applied, in erg/s/A
(present only if SLUG was run with nebular emission and
extinction both enabled)

\end{description}

(Present if the run being read contains a cluster\_phot file)
\begin{description}
\item[{filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{filter\_wl\_cen}] \leavevmode{[}list{]}
central wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1; omitted if
nofilterdata is True

\item[{filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1; omitted
if nofilterdata is True

\item[{phot}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value in each filter for each cluster; units are as
indicated in the units field

\end{description}

If extinction is enabled, phot\_ex will contain photometry  
after extinction has been applied.

(Present if the run being read contains a cluster\_cloudyspec file)
\begin{description}
\item[{cloudy\_wl}] \leavevmode{[}array{]}
wavelength, in Angstrom

\item[{cloudy\_inc}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of the cluster's stellar radiation field at
each wavelength, in erg/s/A

\item[{cloudy\_trans}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of the stellar radiation field after it has
passed through the HII region, at each wavelength, in erg/s/A

\item[{cloudy\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of the radiation field emitted by the HII
region, at each wavelength, in erg/s/A

\item[{cloudy\_trans\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
the sum of the emitted and transmitted fields; this is what
would be seen by an observer looking at both the star cluster
and its nebula

\end{description}

(Present if the run being read contains a cluster\_cloudylines file)
\begin{description}
\item[{cloudy\_linelabel}] \leavevmode{[}array, dtype='S4', shape (N\_lines){]}
labels for the lines, following cloudy's 4 character line label
notation

\item[{cloudy\_linewl}] \leavevmode{[}array, shape (N\_lines){]}
rest wavelength for each line, in Angstrom

\item[{cloudy\_linelum}] \leavevmode{[}array, shape (N\_cluster, N\_lines){]}
luminosity of each line at each time for each trial, in erg/s

\end{description}

(Present if the run being read contains a cluster\_cloudyphot file)
\begin{description}
\item[{cloudy\_filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{cloudy\_filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{cloudy\_filter\_wl\_eff}] \leavevmode{[}list{]}
effective wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{cloudy\_filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1; omitted if
nofilterdata is True

\item[{cloudy\_filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1; omitted
if nofilterdata is True

\item[{cloudy\_filter\_beta}] \leavevmode{[}list{]}
powerlaw index beta for each filter; used to normalize the
photometry

\item[{cloudy\_filter\_wl\_c}] \leavevmode{[}list{]}
pivot wavelength for each filter; used to normalize the photometry

\item[{cloudy\_phot\_trans}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value for each cluster in each filter for the
transmitted light (i.e., the starlight remaining after it has
passed through the HII region); units are as indicated in
the units field

\item[{cloudy\_phot\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value for each cluster in each filter for the
emitted light (i.e., the diffuse light emitted by the HII
region); units are as indicated in the units field

\item[{cloudy\_phot\_trans\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value in each filter for each cluster for the
transmitted plus emitted light (i.e., the light coming
directly from the stars after absorption by the HII region,
plus the diffuse light emitted by the HII region); units are as
indicated in the units field

\end{description}

\item[{Raises}] \leavevmode
IOError, if no photometry file can be opened
ValueError, if photsystem is set to an unknown values

\end{description}

\end{fulllineitems}

\index{read\_cluster\_phot() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_cluster_phot}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_cluster\_phot}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{nofilterdata=False}, \emph{photsystem=None}, \emph{verbose=False}, \emph{read\_info=None}, \emph{filters\_only=False}, \emph{read\_filters=None}, \emph{read\_nebular=None}, \emph{read\_extinct=None}, \emph{phot\_only=False}}{}
Function to read a SLUG2 cluster\_phot file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\item[{nofilterdata}] \leavevmode{[}bool{]}
If True, the routine does not attempt to read the filter
response data from the standard location

\item[{photsystem}] \leavevmode{[}None or string{]}
If photsystem is None, the data will be returned in the same
photometric system in which they were read. Alternately, if it
is a string, the data will be converted to the specified
photometric system. Allowable values are `L\_nu', `L\_lambda',
`AB', `STMAG', and `Vega', corresponding to the options defined
in the SLUG code. If this is set and the conversion requested
involves a conversion from a wavelength-based system to a
frequency-based one, nofilterdata must be False so that the
central wavelength of the photometric filters is available.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\item[{filters\_only}] \leavevmode{[}bool{]}
If True, the code only reads the data on the filters, not
any of the actual photometry. If combined with nofilterdata,
this can be used to return the list of available filters
and nothing else.

\item[{read\_filters}] \leavevmode{[}None \textbar{} string \textbar{} listlike containing strings{]}
If this is None, data on all filters is read. Otherwise only
filters whose name(s) match the input filter names ar
read.

\item[{read\_nebular}] \leavevmode{[}None \textbar{} bool{]}
If True, only data with the nebular contribution is read; if
False, only data without it is read. Default behavior is to
read all data.

\item[{read\_extinct}] \leavevmode{[}None \textbar{} bool{]}
If True, only data with extinction applied is read; if
False, only data without it is read. Default behavior is to
read all data.

\item[{phot\_only}] \leavevmode{[}bool{]}
If true, id, trial, time, and filter information are not
read, only photometry

\end{description}

\item[{Returns}] \leavevmode
A namedtuple, which can contain the following fields depending
on the input options, and depending on which fields are present
in the file being read:
\begin{description}
\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
times at which cluster spectra are output, in yr

\item[{filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{filter\_wl\_eff}] \leavevmode{[}list{]}
effective wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1

\item[{filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1

\item[{filter\_beta}] \leavevmode{[}list{]}
powerlaw index beta for each filter; used to normalize the
photometry

\item[{filter\_wl\_c}] \leavevmode{[}list{]}
pivot wavelength for each filter; used to normalize the photometry

\item[{phot}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value in each filter for each cluster; units are as
indicated in the units field

\item[{phot\_neb}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
same as phot, but for the light after it has passed through
the HII region

\item[{phot\_ex}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
same as phot, but after extinction has been applied

\item[{phot\_neb\_ex}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
same as phot, but for the light after it has passed through
the HII region and then had extinction applied

\end{description}

\item[{Raises}] \leavevmode
IOError, if no photometry file can be opened
ValueError, if photsystem is set to an unknown value

\end{description}

\end{fulllineitems}

\index{read\_cluster\_prop() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_cluster_prop}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_cluster\_prop}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 cluster\_prop file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
time at which cluster's properties are being evaluated

\item[{form\_time}] \leavevmode{[}array{]}
time when cluster formed

\item[{lifetime}] \leavevmode{[}array{]}
time at which cluster will disrupt

\item[{target\_mass}] \leavevmode{[}array{]}
target cluster mass

\item[{actual\_mass}] \leavevmode{[}array{]}
actual mass at formation

\item[{live\_mass}] \leavevmode{[}array{]}
mass of currently living stars

\item[{num\_star}] \leavevmode{[}array, dtype ulonglong{]}
number of living stars in cluster being treated stochastically

\item[{max\_star\_mass}] \leavevmode{[}array{]}
mass of most massive living star in cluster

\item[{A\_V}] \leavevmode{[}array{]}
A\_V value for each cluster, in mag (present only if SLUG was
run with extinction enabled)

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_cluster\_spec() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_cluster_spec}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_cluster\_spec}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 cluster\_spec file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
times at which cluster spectra are output, in yr

\item[{wl}] \leavevmode{[}array{]}
wavelength, in Angstrom

\item[{spec}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of each cluster at each wavelength, in erg/s/A

\item[{wl\_neb}] \leavevmode{[}array{]}
wavelength for the nebular spectrum, in Angstrom (present
only if SLUG was run with nebular emission enabled)

\item[{spec\_neb}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity at each wavelength and each time for each
trial, including emission and absorption by the HII region,
in erg/s/A (present only if SLUG was run with nebular
emission enabled)

\item[{wl\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum, in Angstrom (present
only if SLUG was run with extinction enabled)

\item[{spec\_ex}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial after extinction has been applied, in
erg/s/A (present only if SLUG was run with extinction
enabled)

\item[{wl\_neb\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum with nebular emission,
in Angstrom (present only if SLUG was run with both nebular
emission and extinction enabled)

\item[{spec\_neb\_ex}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial including emission and absorption by the
HII region, after extinction has been applied, in erg/s/A
(present only if SLUG was run with nebular emission and
extinction both enabled)

\end{description}

\item[{Raises}] \leavevmode
IOError, if no spectrum file can be opened

\end{description}

\end{fulllineitems}

\index{read\_filter() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_filter}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_filter}}{\emph{filtername}, \emph{filter\_dir=None}}{}
Function to read a filter or set of filters for SLUG2. By default
this function searches the SLUG\_DIR/lib/filter directory, followed
by the current working directory. This can be overridden by the
filter\_dir keyword.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{filtername}] \leavevmode{[}string or iterable containing strings{]}
Name or names of filters to be read; for the special filters
Lbol, QH0, QHe0, and QHe1, the return value will be None

\item[{filter\_dir}] \leavevmode{[}string{]}
Directory where the filter data files can be found

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{wl\_eff}] \leavevmode{[}float or array{]}
Central wavelength of the filter, defined by 
wl\_eff = exp(int R ln lambda dln lambda / int R dln lambda)

\item[{wl}] \leavevmode{[}array or list of arrays{]}
Wavelength table for each filter, in Ang

\item[{response}] \leavevmode{[}array or list of arrays{]}
Response function per photon for each filter

\item[{beta}] \leavevmode{[}float or array{]}
Index beta for the filter

\item[{wl\_c}] \leavevmode{[}float or array{]}
Pivot wavelength for the filter; used when beta != 0 to
normalize the photometry

\end{description}

\item[{Raises}] \leavevmode
IOError, if the filter data files cannot be opened, or if the
requested filter cannot be found

\end{description}

\end{fulllineitems}

\index{read\_integrated() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_integrated}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_integrated}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{nofilterdata=False}, \emph{photsystem=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read all integrated light data for a SLUG2 run.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{nofilterdata}] \leavevmode{[}bool{]}
If True, the routine does not attempt to read the filter
response data from the standard location

\item[{photsystem}] \leavevmode{[}None or string{]}
If photsystem is None, the data will be returned in the same
photometric system in which they were read. Alternately, if it
is a string, the data will be converted to the specified
photometric system. Allowable values are `L\_nu', `L\_lambda',
`AB', `STMAG', and `Vega', corresponding to the options defined
in the SLUG code. If this is set and the conversion requested
involves a conversion from a wavelength-based system to a
frequency-based one, nofilterdata must be False so that the
central wavelength of the photometric filters is available.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `prop\_name',
`phot\_name', `spec\_name', `cloudyspec\_name', `cloudylines\_name'
and `format', giving the names of the files read and the format
they were in; `format' will be one of `ascii', `binary', or
`fits'. If one of the files is not present, the corresponding
\_name key will be omitted from the dict.

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:

(Always present)
\begin{description}
\item[{time: array}] \leavevmode
Times at which data are output

\end{description}

(Only present if an integrated\_prop file is found)
\begin{description}
\item[{target\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Target stellar mass at each time in each trial

\item[{actual\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Actual mass of stars created up to each time in each trial

\item[{live\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Mass of currently-alive stars at each time in each trial

\item[{cluster\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Mass of living stars in non-disrupted clusters at each time in
each trial

\item[{num\_clusters}] \leavevmode{[}array, shape (N\_times, N\_trials), dtype ulonglong{]}
Number of non-disrupted clusters present at each time in each
trial

\item[{num\_dis\_clusters}] \leavevmode{[}array, shape (N\_times, N\_trials), dtype ulonglong{]}
Number of disrupted clusters present at each time in each trial

\item[{num\_fld\_stars}] \leavevmode{[}array, shape (N\_times, N\_trials), dtype ulonglong{]}
Number of living field stars (excluding those in disrupted 
clusters and those being treated non-stochastically) present at
each time in each trial

\end{description}

(Only present if an integrated\_spec file is found)
\begin{description}
\item[{wl}] \leavevmode{[}array{]}
wavelengths, in Angstrom

\item[{spec}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength and each time for each
trial, in erg/s/A

\item[{wl\_neb}] \leavevmode{[}array{]}
wavelength for the nebular spectrum, in Angstrom (present
only if SLUG was run with nebular emission enabled)

\item[{spec\_neb}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength and each time for each
trial, including emission and absorption by the HII region,
in erg/s/A (present only if SLUG was run with nebular
emission enabled)

\item[{wl\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum, in Angstrom (present
only if SLUG was run with extinction enabled)

\item[{spec\_ex}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial after extinction has been applied, in
erg/s/A (present only if SLUG was run with extinction
enabled)

\item[{wl\_neb\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum with nebular emission,
in Angstrom (present only if SLUG was run with both nebular
emission and extinction enabled)

\item[{spec\_neb\_ex}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial including emission and absorption by the
HII region, after extinction has been applied, in erg/s/A
(present only if SLUG was run with nebular emission and
extinction both enabled)

\end{description}

(Only present if an integrated\_phot file is found)
\begin{description}
\item[{filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{filter\_wl\_cen}] \leavevmode{[}list{]}
central wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1; omitted if
nofilterdata is True

\item[{filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1; omitted
if nofilterdata is True

\item[{phot}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial;
units are as indicated in the units field

\end{description}

If extinction is enabled, phot\_ex will contain photometry  
after extinction has been applied.

(Only present if an integrated\_cloudyspec file is found)
\begin{description}
\item[{cloudy\_wl}] \leavevmode{[}array{]}
wavelength, in Angstrom

\item[{cloudy\_inc}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity of the stellar radiation field at each
wavelength and each time for each trial, in erg/s/A

\item[{cloudy\_trans}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity of the stellar radiation field after it has
passed through the HII region, at each wavelength and each time
for each trial, in erg/s/A

\item[{cloudy\_emit}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity of the radiation field emitted by the HII
region, at each wavelength and each time for each trial, in
erg/s/A

\item[{cloudy\_trans\_emit}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
the sum of emitted and transmitted; this is what would be seen
by an observer looking at both the star cluster and its nebula

\end{description}

(Only present if an integrated\_cloudylines file is found)
\begin{description}
\item[{cloudy\_linelabel}] \leavevmode{[}array, dtype='S4', shape (N\_lines){]}
labels for the lines, following cloudy's 4 character line label
notation

\item[{cloudy\_linewl}] \leavevmode{[}array, shape (N\_lines){]}
rest wavelength for each line, in Angstrom

\item[{cloudy\_linelum}] \leavevmode{[}array, shape (N\_lines, N\_times, N\_trials){]}
luminosity of each line at each time for each trial, in erg/s

\end{description}

(Only present if an integrated\_cloudyphot file is found)
\begin{description}
\item[{cloudy\_filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{cloudy\_filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{cloudy\_filter\_wl\_eff}] \leavevmode{[}list{]}
effective wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{cloudy\_filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1; omitted if
nofilterdata is True

\item[{cloudy\_filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1; omitted
if nofilterdata is True

\item[{cloudy\_filter\_beta}] \leavevmode{[}list{]}
powerlaw index beta for each filter; used to normalize the
photometry

\item[{cloudy\_filter\_wl\_c}] \leavevmode{[}list{]}
pivot wavelength for each filter; used to normalize the photometry

\item[{cloudy\_phot\_trans}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial for
the transmitted light (i.e., the starlight remaining after it
has passed through the HII region); units are as indicated in
the units field

\item[{cloudy\_phot\_emit}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial for
the emitted light (i.e., the diffuse light emitted by the HII
region); units are as indicated in the units field

\item[{cloudy\_phot\_trans\_emit}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial for
the transmitted plus emitted light (i.e., the light coming
directly from the stars after absorption by the HII region,
plus the diffuse light emitted by the HII region); units are as
indicated in the units field

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_integrated\_phot() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_integrated_phot}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_integrated\_phot}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{nofilterdata=False}, \emph{photsystem=None}, \emph{verbose=False}, \emph{read\_info=None}, \emph{filters\_only=False}, \emph{read\_filters=None}, \emph{read\_nebular=None}, \emph{read\_extinct=None}}{}
Function to read a SLUG2 integrated\_phot file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\item[{nofilterdata}] \leavevmode{[}bool{]}
If True, the routine does not attempt to read the filter
response data from the standard location

\item[{photsystem}] \leavevmode{[}None or string{]}
If photsystem is None, the data will be returned in the same
photometric system in which they were read. Alternately, if it
is a string, the data will be converted to the specified
photometric system. Allowable values are `L\_nu', `L\_lambda',
`AB', `STMAG', and `Vega', corresponding to the options defined
in the SLUG code. If this is set and the conversion requested
involves a conversion from a wavelength-based system to a
frequency-based one, nofilterdata must be False so that the
central wavelength of the photometric filters is available.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\item[{filters\_only}] \leavevmode{[}bool{]}
If True, the code only reads the data on the filters, not
any of the actual photometry. If combined with nofilterdata,
this can be used to return the list of available filters
and nothing else.

\item[{read\_filters}] \leavevmode{[}None \textbar{} string \textbar{} listlike containing strings{]}
If this is None, data on all filters is read. Otherwise only
filters whose name(s) match the input filter names ar
read.

\item[{read\_nebular}] \leavevmode{[}None \textbar{} bool{]}
If True, only data with the nebular contribution is read; if
False, only data without it is read. Default behavior is to
read all data.

\item[{read\_extinct}] \leavevmode{[}None \textbar{} bool{]}
If True, only data with extinction applied is read; if
False, only data without it is read. Default behavior is to
read all data.

\end{description}

\item[{Returns}] \leavevmode
A namedtuple , which can contain the following fields depending
on the input options, and depending on which fields are present
in the file being read:
\begin{description}
\item[{time}] \leavevmode{[}array, shape (N\_times) or shape (N\_trials){]}
Times at which data are output; shape is either N\_times (if
the run was done with fixed output times) or N\_trials (if
the run was done with random output times)

\item[{filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{filter\_wl\_eff}] \leavevmode{[}list{]}
effective wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1

\item[{filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1

\item[{filter\_beta}] \leavevmode{[}list{]}
powerlaw index beta for each filter; used to normalize the
photometry

\item[{filter\_wl\_c}] \leavevmode{[}list{]}
pivot wavelength for each filter; used to normalize the photometry

\item[{phot}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial;
units are as indicated in the units field

\item[{phot\_neb}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
same as phot, but for the light after it has passed through
the HII region

\item[{phot\_ex}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
same as phot, but after extinction has been applied

\item[{phot\_neb\_ex}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
same as phot, but for the light after it has passed through
the HII region and then had extinction applied

\end{description}

\item[{Raises}] \leavevmode
IOError, if no photometry file can be opened
ValueError, if photsystem is set to an unknown value

\end{description}

\end{fulllineitems}

\index{read\_integrated\_prop() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_integrated_prop}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_integrated\_prop}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 integrated\_prop file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{time}] \leavevmode{[}array, shape (N\_times) or shape (N\_trials){]}
Times at which data are output; shape is either N\_times (if
the run was done with fixed output times) or N\_trials (if
the run was done with random output times)

\item[{target\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Target stellar mass at each time

\item[{actual\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Actual mass of stars created up to each time in each trial

\item[{live\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Mass of currently-alive stars at each time in each trial

\item[{cluster\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Mass of living stars in non-disrupted clusters at each time in
each trial

\item[{num\_clusters}] \leavevmode{[}array, shape (N\_times, N\_trials), dtype ulonglong{]}
Number of non-disrupted clusters present at each time in each
trial

\item[{num\_dis\_clusters}] \leavevmode{[}array, shape (N\_times, N\_trials), dtype ulonglong{]}
Number of disrupted clusters present at each time in each trial

\item[{num\_fld\_stars}] \leavevmode{[}array, shape (N\_times, N\_trials), dtype ulonglong{]}
Number of living field stars (excluding those in disrupted 
clusters and those being treated non-stochastically) present at
each time in each trial

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_integrated\_spec() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_integrated_spec}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_integrated\_spec}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 integrated\_spec file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{time}] \leavevmode{[}array, shape (N\_times) or shape (N\_trials){]}
Times at which data are output; shape is either N\_times (if
the run was done with fixed output times) or N\_trials (if
the run was done with random output times)

\item[{wl}] \leavevmode{[}array{]}
wavelength, in Angstrom

\item[{spec}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength and each time for each
trial, in erg/s/A

\item[{wl\_neb}] \leavevmode{[}array{]}
wavelength for the nebular spectrum, in Angstrom (present
only if SLUG was run with nebular emission enabled)

\item[{spec\_neb}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength and each time for each
trial, including emission and absorption by the HII region,
in erg/s/A (present only if SLUG was run with nebular
emission enabled)

\item[{wl\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum, in Angstrom (present
only if SLUG was run with extinction enabled)

\item[{spec\_ex}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial after extinction has been applied, in
erg/s/A (present only if SLUG was run with extinction
enabled)

\item[{wl\_neb\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum with nebular emission,
in Angstrom (present only if SLUG was run with both nebular
emission and extinction enabled)

\item[{spec\_neb\_ex}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial including emission and absorption by the
HII region, after extinction has been applied, in erg/s/A
(present only if SLUG was run with nebular emission and
extinction both enabled)

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_summary() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_summary}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_summary}}{\emph{model\_name}, \emph{output\_dir=None}}{}
Function to open a SLUG output summary file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{summary}] \leavevmode{[}dict{]}
A dict containing all the keywords stored in the output file

\end{description}

\item[{Raises}] \leavevmode
IOError, if a summary file for the specified model cannot be found

\end{description}

\end{fulllineitems}

\index{slug\_open() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.slug_open}\pysiglinewithargsret{\code{slugpy.}\bfcode{slug\_open}}{\emph{filename}, \emph{output\_dir=None}, \emph{fmt=None}}{}
Function to open a SLUG2 output file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{filename}] \leavevmode{[}string{]}
Name of the file to open, without any extension. The following
extensions are tried, in order: .txt, .bin, .fits

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the
SLUG\_DIR/output directory if the SLUG\_DIR environment variable
is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{fp}] \leavevmode{[}file or astropy.io.fits.hdu.hdulist.HDUList{]}
A file object pointing the file that has been opened

\item[{fname}] \leavevmode{[}string{]}
Name of the file that was opened

\end{description}

\item[{Raises}] \leavevmode
IOError, if a file of the specified name cannot be found

\end{description}

\end{fulllineitems}

\index{write\_cluster() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.write_cluster}\pysiglinewithargsret{\code{slugpy.}\bfcode{write\_cluster}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Function to write a set of output cluster files in SLUG2 format,
starting from a cluster data set as returned by read\_cluster.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Cluster data to be written, in the namedtuple format returned
by read\_cluster

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the output file; `txt' and `ascii' produce ASCII
text files, `bin' or `binary' produce binary files, and
`fits' or `fits2' product FITS files; `fits2' uses an
ordering that allows for more efficient querying of outputs
too large to fit in memory

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}

\index{write\_integrated() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.write_integrated}\pysiglinewithargsret{\code{slugpy.}\bfcode{write\_integrated}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Function to write a set of output integrated files in SLUG2 format,
starting from an integrated data set as returned by
read\_integrated.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Integrated data to be written, in the namedtuple format returned
by read\_integrated

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}



\chapter{cloudy\_slug: An Automated Interface to cloudy}
\label{cloudy:cloudy-slug-an-automated-interface-to-cloudy}\label{cloudy::doc}\label{cloudy:sec-cloudy-slug}
SLUG stochastically generates stellar spectra, but it does not compute
the nebular lines produced when those photons interact with the
interstellar medium. To perform such calculations, SLUG includes an
automated interface to \href{http://nublado.org/}{cloudy} (\href{http://adsabs.harvard.edu/abs/2013RMxAA..49..137F}{Ferland et
al., 2013, RMxAA, 49, 137}). This can be
used to post-process the output of a SLUG run in order to compute
nebular emission.


\section{cloudy\_slug Basics}
\label{cloudy:cloudy-slug-basics}
The basic steps (described in greater detail below) are as follows:
\begin{enumerate}
\item {} 
Get cloudy installed and compiled, following the directions on the
\href{http://nublado.org/}{cloudy website}.

\item {} 
Set the environment variable \code{\$CLOUDY\_DIR} to the directory where
the cloudy executable \code{cloudy.exe} is located.  If you are using
a \code{bash}-like shell, the syntax for this is:

\begin{Verbatim}[commandchars=\\\{\}]
export CLOUDY\PYGZus{}DIR = /path/to/cloudy
\end{Verbatim}

while for a \code{csh}-like shell, it is:

\begin{Verbatim}[commandchars=\\\{\}]
setenv CLOUDY\PYGZus{}DIR /path/to/cloudy
\end{Verbatim}

\item {} 
If you desire, edit the cloudy input template
\code{cloudy\_slug/cloudy.in\_template} and the line list
\code{cloudy\_slug/LineList\_HII.dat}. There are the template input files
that will be used for all the cloudy runs, and their syntax follows
the standard cloudy syntax. They control things like the density and
element abundances in the nebula -- see {\hyperref[cloudy:ssec-cloudy-template]{\emph{\DUspan{}{The cloudy\_slug Input Template}}}}
for more details.

\item {} 
Perform the desired SLUG simulation. The SLUG simulation outputs
must include spectra and photometry, and one of the photometric
bands output must be \code{QH0} (see {\hyperref[parameters:ssec-phot-keywords]{\emph{\DUspan{}{Photometric Filter Keywords}}}}). If
running in integrated mode (the default -- see
{\hyperref[cloudy:ssec-cloudy-cluster]{\emph{\DUspan{}{The cloudy\_slug Physical Model: Integrated Mode Versus Cluster Mode}}}}), integrated specta and photometry are
required, and if running in cluster mode, cluster spectra and
photometry are required.

\item {} 
Invoke the cloudy\_slug interface script via:

\begin{Verbatim}[commandchars=\\\{\}]
python cloudy\PYGZus{}slug/cloudy\PYGZus{}slug.py SLUG\PYGZus{}MODEL\PYGZus{}NAME
\end{Verbatim}

where \code{SLUG\_MODEL\_NAME} is the name of the SLUG run to be
processed. See {\hyperref[cloudy:ssec-cloudy-cluster]{\emph{\DUspan{}{The cloudy\_slug Physical Model: Integrated Mode Versus Cluster Mode}}}} for more information on
the underlying physical model assumed in the calculation, and
{\hyperref[cloudy:ssec-cloudy-slug-options]{\emph{\DUspan{}{The cloudy\_slug Interface Script}}}} for more details on the python
script and its options.

\item {} 
The output will be stored as a series of additional output files of
with names of the form SLUG\_MODEL\_NAME\_*cloudy*.ext, where the
extension is .txt, .bin, or .fits, depending on the format in which
the orignal SLUG output was stored. These files can be processed
automatically by the slugpy helper routines (see
{\hyperref[slugpy:sec-slugpy]{\emph{\DUspan{}{slugpy -- The Python Helper Library}}}}). See {\hyperref[cloudy:ssec-cloudy-output]{\emph{\DUspan{}{Full Description of cloudy\_slug Output}}}} for a description
of the outputs.

\end{enumerate}


\section{The cloudy\_slug Physical Model: Integrated Mode Versus Cluster Mode}
\label{cloudy:ssec-cloudy-cluster}\label{cloudy:the-cloudy-slug-physical-model-integrated-mode-versus-cluster-mode}
Associating nebular emission with the stellar populations produced by
SLUG requires some assumptions about geometry, and some choices about
what quantities one is interested in computed. SLUG outputs both
integrated spectra for all the stars in a galaxy, and spectra for
individual clusters. One on hand, one could make the exteme assumption
that all the star clusters are spatially close enough to one another
that one can think of the entire galaxy as a single giant HII region,
and compute the nebular emission for the galaxy as a whole. This may
be a reasonable assumption for galaxies where the star formation is
highly spatially-concentrated. At the other extreme, one may assume that
there is no overlap whatsoever between the HII regions surrounding
different star clusters, so that nebular emission should be computed
for each one independently. This may be a reasonable assumption for
extended, slowly star-forming systems like the outer disk of the Milky
Way. Each of these assumptions entails somewhat different choices
about how to set the inner radius and inner density the HII region, as
required by cloudy. The cloudy\_slug interface can compute nebular
emission under either of these scenarios; we refer to the former as
integrated mode, and to the latter as cluster mode. Note that, in
either mode, the spectrum that is used to compute the nebular emission
will be the \emph{unextincted, non-redshifted} spectrum computed by SLUG.


\subsection{Integrated Mode}
\label{cloudy:sssec-cloudy-integrated-mode}\label{cloudy:integrated-mode}
In integrated mode, cloudy\_slug will read all the spectra contained in
the SLUG integrated\_spec output file, and for each stellar spectrum it
will perform a cloudy run to produce a calculation of the nebular
emission produced by that stellar spectrum interacting with a
surrounding HII region. The density in the first zone of the HII
region will be as specified by the standard \code{hden} keyword in the
cloudy input template (see {\hyperref[cloudy:ssec-cloudy-template]{\emph{\DUspan{}{The cloudy\_slug Input Template}}}}). The inner
radius of the HII region will be computed automatically, and will be
set to \(10^{-3}\) of the Stromgren radius for that density, where
\begin{gather}
\begin{split}r_{\mathrm{St}} = \left(\frac{3.0 Q(\mathrm{H}^0)}{4\pi
\alpha_B n_{\mathrm{H}}^2}\right)^{1/3}\end{split}\notag
\end{gather}
where \(Q(\mathrm{H}^0)\) is the ionizing luminosity computed by
SLUG, \(n_{\mathrm{H}}\) is the hydrogen number density stored in
the cloudy input template, and \(\alpha_B\) is the case B
recombination coefficient, which is taken to have a value of
\(2.59\times 10^{-13}\;\mathrm{cm}^3\;\mathrm{s}^{-1}\).


\subsection{Cluster Mode}
\label{cloudy:cluster-mode}\label{cloudy:sssec-cloudy-cluster-mode}
In cluster mode, cloudy\_slug will read all the individual cluster
spectra contained in the SLUG cluster\_spec file, and for each one it
will perform a cloudy calculation to determine the corresponding
nebular emission. By default the density and radius are handled somewhat
differently in this case, since, for a mono-age stellar population, it
is possible to compute the time evolution of the HII region radius and
density.

Default behavior in cluster mode is as follows:
the hydrogen number density \(n_{\mathrm{H}}\)
stored in the cloudy input template (see {\hyperref[cloudy:ssec-cloudy-template]{\emph{\DUspan{}{The cloudy\_slug Input Template}}}})
is taken to specify the density of the \emph{neutral} gas around the HII
region, not the density of the gas inside the HII region. The outer
radius of the HII region is then computed using the approximate
analytic solution for the expansion of an HII region into a uniform
medium, including the effects of radiation presssure and stellar wind
momentum deposition, given by \href{http://adsabs.harvard.edu/abs/2009ApJ...703.1352K}{Krumholz \& Matzner (2009, ApJ,
703, 1352)}. The
radius is computed from the ionizing luminosity
\(Q(\mathrm{H}^0)\), hydrogen number density
\(n_{\mathrm{H}}\), and star cluster age \(t\) as
\begin{gather}
\begin{split}r_{\mathrm{II}} & = r_{\mathrm{ch}}
\left(x_{\mathrm{II,rad}}^{7/2} +
x_{\mathrm{II,gas}}^{7/2}\right)^{2/7} \\\end{split}\notag\\\begin{split}x_{\mathrm{II,rad}} &= (2\tau^2)^{1/4} \\\end{split}\notag\\\begin{split}x_{\mathrm{II,gas}} &= (49\tau^2/36)^{2/7} \\\end{split}\notag\\\begin{split}\tau &= t/t_{\mathrm{ch}} \\\end{split}\notag\\\begin{split}r_{\mathrm{ch}} & = \frac{\alpha_B}{12\pi\phi}
\left(\frac{\epsilon_0}{2.2 k_B T_{\mathrm{II}}}\right)^2
f_{\mathrm{trap}}^2 \frac{\psi^2 Q(\mathrm{H}^0)}{c^2} \\\end{split}\notag\\\begin{split}t_{\mathrm{ch}} & = \left(\frac{4\pi \mu m_{\mathrm{H}}
n_{\mathrm{H}} c r_{\mathrm{ch}}^4}{3 f_{\mathrm{trap}}
Q(\mathrm{H}^0) \psi \epsilon_0}\right)^{1/2}\end{split}\notag
\end{gather}
where \(\alpha_B = 2.59\times
10^{-13}\;\mathrm{cm}^3\;\mathrm{s}^{-1}\) is the case B recombination
coefficient, \(\phi = 0.73\) is the fraction of ionizing photons absorbed
by hydrogen atoms rather than dust, \(\epsilon_0 =
13.6\;\mathrm{eV}\) is the hydrogen ionization potential,
\(T_{\mathrm{II}} = 10^4\;\mathrm{K}\) is the temperature inside
the HII region, \(f_{\mathrm{trap}} = 2\) is the trapping factor
that accounts for stellar wind and trapped infrared radiation
pressure, \(\psi = 3.2\) is the mean photon energy in Rydberg for
a fully sampled IMF at zero age, and \(\mu = 1.33\) is the mean
mass per hydrogen nucleus for gas of the standard cosmic
composition. See \href{http://adsabs.harvard.edu/abs/2009ApJ...703.1352K}{Krumholz \& Matzner (2009)} for a discussion
of the fiducial choices of these factors.

Once the outer radius is known, cloudy\_slug sets the starting radius
for the cloudy calculation to \(10^{-3} r_{\mathrm{II}}\), and
sets the starting density to the value expected for photoionization
equilibrum in a uniform HII region,
\begin{gather}
\begin{split}n_{\mathrm{II}} = \left(\frac{3
Q(\mathrm{H}^0)}{4.4 \pi\alpha_B
r_{\mathrm{II}}^3}\right)^{1/2}\end{split}\notag
\end{gather}
Note that this approximation will be highly inaccurate if
\(r_{\mathrm{II}} \ll r_{\mathrm{ch}}\), but no better analytic
approximation is available, and this phase should be very short-lived
for most clusters.


\section{The cloudy\_slug Input Template}
\label{cloudy:the-cloudy-slug-input-template}\label{cloudy:ssec-cloudy-template}
The cloudy\_slug interface operates by reading SLUG output spectra and
using them as inputs to a cloudy calculation. However, cloudy
obviously requires many input parameters beyond simply the spectrum of
the input radiation field. These parameters are normally provided by
an input file whose format is as described in the \href{http://nublado.org}{cloudy documentation}. The cloudy\_slug interface works by reading a
\emph{template} input file that specifies all these parameter, and which
will be used as a basis for the final cloudy input files that will
contain the SLUG spectra.

In general the template input file looks just like an ordinary cloudy
input file, subject to the following restrictions:
\begin{enumerate}
\item {} 
The input file \emph{must not} contain any commands that specify the
luminosity, intensity, or the spectral shape. These will be
inserted automatically by the cloudy\_slug script.

\item {} 
The input file \emph{must not} contain a radius command. This too will
be computed automatically by the cloudy\_slug script.

\item {} 
The input file \emph{must} contain an entry \code{hden N} where \code{N} is
the log base 10 of the hydrogen density. This will be interpreted
differently depending on whether cloudy\_slug is being run in
cluster mode or integrated mode -- see {\hyperref[cloudy:ssec-cloudy-cluster]{\emph{\DUspan{}{The cloudy\_slug Physical Model: Integrated Mode Versus Cluster Mode}}}}.

\item {} 
Any outputs to be written (specified using the \code{save} or
\code{punch} keywords) must give file names containing the string
\code{OUTPUT\_FILENAME}. This string will be replaced by the
cloudy\_slug script to generate a unique file name for each cloudy
run, and to read back these outputs for post-processing.

\item {} 
The cloudy\_slug output will contain output spectra only if the
cloudy input file contains a \code{save last continuum} command. See
{\hyperref[cloudy:ssec-cloudy-output]{\emph{\DUspan{}{Full Description of cloudy\_slug Output}}}}.

\item {} 
The cloudy\_slug output will contain output line luminosities only
if the cloudy input file contains a \code{save last line list emergent
absolute column} command. See {\hyperref[cloudy:ssec-cloudy-output]{\emph{\DUspan{}{Full Description of cloudy\_slug Output}}}}.

\item {} 
If any other outputs are produced by the input file, they will
neither be processed nor moved, deleted, or otherwise changed by
the cloudy\_slug script.

\item {} 
Running cloudy in grid mode is not currently supported.

\end{enumerate}

An example cloudy input file with reasonable parameter choices is
provided as \code{cloudy\_slug/cloudy\_in.template} in the main directory
of the SLUG repository.

In addition to the input file, the default template makes use of a
cloudy line list file to specify which line luminosities should be
output (see the \href{http://nublado.org}{cloudy documentation} for
details). The template points to the file
\code{cloudy\_slug/LineList\_HII.data} (which is identical to cloudy's
default line list for HII regions), but any other valid cloudy line
list file would work as well.


\section{The cloudy\_slug Interface Script}
\label{cloudy:ssec-cloudy-slug-options}\label{cloudy:the-cloudy-slug-interface-script}
The \code{cloudy\_slug.py} script provides the interface between SLUG and
cloudy. Usage for this script is as follows:

\begin{Verbatim}[commandchars=\\\{\}]
cloudy\PYGZus{}slug.py [\PYGZhy{}h] [\PYGZhy{}a AGEMAX] [\PYGZhy{}\PYGZhy{}cloudypath CLOUDYPATH]
               [\PYGZhy{}\PYGZhy{}cloudytemplate CLOUDYTEMPLATE] [\PYGZhy{}cm]
               [\PYGZhy{}cf COVERINGFAC] [\PYGZhy{}hd HDEN] [\PYGZhy{}ip IONPARAM] [\PYGZhy{}nd]
               [\PYGZhy{}nl NICELEVEL] [\PYGZhy{}n NPROC] [\PYGZhy{}s] [\PYGZhy{}\PYGZhy{}slugpath SLUGPATH]
               [\PYGZhy{}v]
               slug\PYGZus{}model\PYGZus{}name [start\PYGZus{}spec] [end\PYGZus{}spec]
\end{Verbatim}

The positional arguments are as follows:
\begin{itemize}
\item {} 
\code{slug\_model\_name}: this is the name of the SLUG output to be used
as a basis for the cloudy calculation. This should be the same as
the \code{model\_name} parameter used in the SLUG simulation, with the
optional addition of a path specification in front.

\item {} 
\code{start\_spec}: default behavior is to run cloudy on all the
integrated spectra (in {\hyperref[cloudy:sssec-cloudy-integrated-mode]{\emph{\DUspan{}{Integrated Mode}}}}) or
cluster spectra (in {\hyperref[cloudy:sssec-cloudy-cluster-mode]{\emph{\DUspan{}{Cluster Mode}}}}). If this
argument is set, cloudy will only be run in spectra starting with
the specified trial number (in {\hyperref[cloudy:sssec-cloudy-integrated-mode]{\emph{\DUspan{}{Integrated Mode}}}})
or cluster number (in {\hyperref[cloudy:sssec-cloudy-cluster-mode]{\emph{\DUspan{}{Cluster Mode}}}}); numbers are
0-offset, so the first trial/cluster is 0, the next is 1, etc.

\item {} 
\code{end\_spec}: default behavior is to run cloudy on all the
integrated spectra (in {\hyperref[cloudy:sssec-cloudy-integrated-mode]{\emph{\DUspan{}{Integrated Mode}}}}) or
cluster spectra (in {\hyperref[cloudy:sssec-cloudy-cluster-mode]{\emph{\DUspan{}{Cluster Mode}}}}). If this
argument is set, cloudy will only be run on spectra up to the
specified trial number (in {\hyperref[cloudy:sssec-cloudy-integrated-mode]{\emph{\DUspan{}{Integrated Mode}}}}) or
cluster number (in {\hyperref[cloudy:sssec-cloudy-cluster-mode]{\emph{\DUspan{}{Cluster Mode}}}}); numbers are
0-offset, so the first trial/cluster is 0, the next is 1, etc.

\end{itemize}

The optional arguments are as follows:
\begin{itemize}
\item {} 
\code{-h, -{-}help}: prints a help message and then exits

\item {} 
\code{-a AGEMAX, -{-}agemax AGEMAX}: maximum cluster age in Myr for
cloudy computation. Cloudy will not be run on clusters older than
this value, and the predicted nebular emission for such clusters
will be recorded as zero. Default value is 10 Myr. This argument only
has an effect if running in {\hyperref[cloudy:sssec-cloudy-cluster-mode]{\emph{\DUspan{}{Cluster Mode}}}};
otherwise it is ignored.

\item {} 
\code{-{-}cloudypath CLOUDYPATH}: path to the cloudy executable; default
is \code{\$CLOUDY\_DIR/cloudy.exe}

\item {} 
\code{-{-}cloudytemplate CLOUDYTEMPLATE}: cloudy input file template (see
{\hyperref[cloudy:ssec-cloudy-template]{\emph{\DUspan{}{The cloudy\_slug Input Template}}}}); default is
\code{\$SLUG\_DIR/cloudy\_slug/cloudy.in\_template}

\item {} 
\code{-cm, -{-}clustermode}: if this argument is set, then cloudy\_slug
will run in {\hyperref[cloudy:sssec-cloudy-cluster-mode]{\emph{\DUspan{}{Cluster Mode}}}}; default behavior is to
run in {\hyperref[cloudy:sssec-cloudy-integrated-mode]{\emph{\DUspan{}{Integrated Mode}}}}

\item {} 
\code{-cf COVERINGFRAC, -{-}coveringfrac COVERINGFRAC}: this sets the
covering fraction of the HII region; formally, the effect of this
parameter is that the ionizing luminosity that is passed to cloudy
is reduced by the specified factor; defaults to 1.0, i.e., the full
ionizing luminosity is passed to cloudy

\item {} 
\code{-hd HDEN, -{-}hden HDEN}: hydrogen density; if set, this value
overrides the value of hden found in the cloudy template file

\item {} 
\code{-ip IONPARAM, -{-}ionparam IONPARAM}: ionization parameter; this is
an alternate means of setting the hydrogen density, which will set
the density \(n_{\mathrm{II}}\) so that the volume-averaged
ionization parameter \(\mathcal{U}\) for a spherically-symmetric
uniform-density HII region is \code{IONPARAM}. The relationship used to
define the density is

\end{itemize}
\begin{gather}
\begin{split}\mathcal{U} = \left[\frac{81 \alpha_B^2 n_{\mathrm{II}}
Q(\mathrm{H}^0)}{288 \pi c^3}\right]^{1/3}\end{split}\notag
\end{gather}\begin{itemize}
\item {} 
\code{-nd, -{-}nodynamic}: this disables the dynamical calculation of the
HII region radius and density for calculations in
{\hyperref[cloudy:sssec-cloudy-cluster-mode]{\emph{\DUspan{}{Cluster Mode}}}}, and instead treats the density as
the starting density for the cloudy calculation, exactly as in
{\hyperref[cloudy:sssec-cloudy-integrated-mode]{\emph{\DUspan{}{Integrated Mode}}}}

\item {} 
\code{-nl NICELEVEL, -{-}nicelevel NICELEVEL}: if this is set, then the
cloudy processes launched by the script will be run at this nice
level. If it is not set, they will not be nice'd. Note that this
option will only work correctly on platforms that support nice.

\item {} 
\code{-n NPROC, -{-}nproc NPROC}: number of simultaneous cloudy processes
to run; default is the number of cores available on the system

\item {} 
\code{-s, -{-}save}: by default, cloudy\_slug will extract line and
spectral data from the cloudy outputs and store them as described in
{\hyperref[cloudy:ssec-cloudy-output]{\emph{\DUspan{}{Full Description of cloudy\_slug Output}}}}, then delete the cloudy output files. If
this option is set, the cloudy output files will NOT be deleted, and
will be left in place. WARNING: cloudy's outputs are written in
ASCII and are quite voluminous, so only choose this option if you
are only running cloudy on a small number of SLUG spectra and/or you
are prepared to store hundreds of GB more more.

\item {} 
\code{-{-}slugpath SLUGPATH}: path to the SLUG output data. If not set,
cloudy\_slug searches for an appropriately-named set of output files
first in the current working directory, and next in
\code{\$SLUG\_DIR/output}

\item {} 
\code{-v, -{-}verbose}: if this option is set, cloudy\_slug produces
verbose output as it runs

\end{itemize}


\section{Full Description of cloudy\_slug Output}
\label{cloudy:ssec-cloudy-output}\label{cloudy:full-description-of-cloudy-slug-output}
The cloudy\_slug script will automatically process the cloudy output
and produce a series of new output files, which will be written to the
same directory where the input SLUG files are located, and using the
same output mode (ASCII text, raw binary, or FITS -- see
{\hyperref[output:sec-output]{\emph{\DUspan{}{Output Files and Format}}}}). If cloudy\_slug is run in
{\hyperref[cloudy:sssec-cloudy-integrated-mode]{\emph{\DUspan{}{Integrated Mode}}}}, the three output files will be
\code{MODEL\_NAME\_integrated\_cloudylines.ext},
\code{MODEL\_NAME\_integrated\_cloudyphot.ext}, and
\code{MODEL\_NAME\_integrated\_cloudyspec.ext}, where the extension \code{.ext}
is one of \code{.txt}, \code{.bin}, or \code{.fits}, depending on the
\code{output\_mode}. If cloudy\_slug is run in
{\hyperref[cloudy:sssec-cloudy-cluster-mode]{\emph{\DUspan{}{Cluster Mode}}}}, the three output files will be
\code{MODEL\_NAME\_cluster\_cloudylines.ext},
\code{MODEL\_NAME\_cluster\_cloudyphot.ext}, and
\code{MODEL\_NAME\_cluster\_cloudyspec.ext}. All of these output files will
be read and processed automatically if the outputs are read using
\code{read\_integrated} or \code{read\_cluster} in the {\hyperref[slugpy:sec-slugpy]{\emph{\DUspan{}{slugpy -- The Python Helper Library}}}}
library.

The format of those files is described below.


\subsection{The \texttt{integrated\_cloudylines} File}
\label{cloudy:the-integrated-cloudylines-file}
This file contains data on the nebular line emission produced by the
interaction of the stellar radiation field with the ISM. It consists
of a series of entries containing the following fields:
\begin{itemize}
\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{LineLabel}: four letter code labeling each line. These codes
are the codes used by cloudy (see the \href{http://nublado.org}{cloudy documentation})

\item {} 
{}`{}` Wavelength{}`{}`: wavelength of the line, in Angstrom. Note that
default cloudy behavior is to round wavelengths to the nearest
Angstrom.

\item {} 
\code{Luminosity}: line luminosity, in erg/s

\end{itemize}

If the SLUG data input to cloudy\_slug were written in \code{ascii} mode,
these data are output as a text file containing a series of columns,
with different trials separated by lines of dashes.

If the SLUG data input to cloudy\_slug were written in \code{fits} mode,
the data are written in a FITS file containing two binary table
extensions. The first extension contains two fields, \code{Line\_label} and
\code{Wavelength}, giving the four-letter cloudy line codes and central
wavelengths. The second extension contains three columns, giving the
trial number, time, and line luminosity for each line at each time in
each trial.

If the SLUG data input to cloudy\_slug were written in \code{binary} mode,
the data are written in a raw binary file. The file starts with a
header consisting of
\begin{itemize}
\item {} 
\code{NLine} (python \code{int}, equivalent to C \code{long}): number of lines

\item {} 
\code{LineLabel} (\code{NLine} entries stored as \code{ASCII text}): line
labels listed in ASCII, one label per line

\end{itemize}

This is followed by a series of entries of the form
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{LineLum} (\code{NLine} entries of type numpy \code{float64})

\end{itemize}

There is one such record for each output time, with different trials
ordered sequentially, so that all the times for one trial are output
before the first time for the next trial.


\subsection{The \texttt{integrated\_cloudyspec} File}
\label{cloudy:the-integrated-cloudyspec-file}\label{cloudy:sssec-int-cloudyspec-file}
This file contains data on the spectrum produced by interaction
between the stellar radiation field and the nebula. Each entry in the
output file contains the folling fields:
\begin{itemize}
\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{Wavelength}: the wavelength at which the spectrum is evaluated,
in Angstrom

\item {} 
\code{Incident}: specific luminosity in erg/s/Angstrom at the specified
wavelength. In cloudy's terminology, this is the \emph{incident}
spectrum, i.e., the stellar radiation field entering the nebula. It
should be the same as the spectrum contained in the SLUG
\code{integrated\_spec} file for the corresponding time and trial,
except interpolated onto the wavelength grid used by cloudy.

\item {} 
\code{Transmitted}:  specific luminosity in erg/s/Angstrom at the specified
wavelength. In cloudy's terminology, this is the \emph{transmitted}
spectrum, i.e., the stellar spectrum exiting the HII region, not
including any emission produced within the nebula. This is what
would be detected by an observing aperture that included only the
stars, and none of the nebula.

\item {} 
\code{Emitted}:  specific luminosity in erg/s/Angstrom at the specified
wavelength. In cloudy's terminology, this is the \emph{emitted}
spectrum, i.e., the spectrum emitted by the diffuse gas in the HII
region, excluding any light from the stars themselves. This is what
would be seen by an observer whose aperture covered the nebula, but
masked the stars.

\item {} 
\code{Transmitted\_plus\_emitted}: this is just the sum of
\code{Transmitted} and \code{Emitted}. It represents what would be
observed in an aperture including both the stars and the HII
region.

\end{itemize}

If the SLUG data input to cloudy\_slug were written in \code{ascii} mode,
these data are output as a text file containing a series of columns,
with different trials separated by lines of dashes.

If the SLUG data input to cloudy\_slug were written in \code{fits} mode,
these data are written in a FITS file containing two binary table
extensions. The first extension contains one field, \code{Wavelength},
which gives the wavelengths of the spectra in Angstrom. The second
extension contains six fields: \code{Trial}, \code{Time},
\code{Incident\_spectrum}, \code{Transmitted\_spectrum}, \code{Emitted\_spectrum},
and \code{Transmitted\_plus\_emitted\_spectrum}. The first two of these give
the trial number and time, and the remaining four give the incident,
transmitted, emitted, and transmitted plus emitted spectra for the
corresponding time and trial.

If the SLUG data input to cloudy\_slug were written in \code{binary} mode,
these data are written in a raw binary file that is formatted as
follows. The file begins with a header consisting of
\begin{itemize}
\item {} 
\code{NWavelength} (numpy \code{int64}): number of wavelengths

\item {} 
\code{Wavelength} (\code{NWavelength} entries of numpy \code{float64})

\end{itemize}

and then contains a series of records of the form
\begin{itemize}
\item {} 
\code{Time} (numpy \code{float64})

\item {} 
\code{Incident} (\code{NWavelength} entries of numpy \code{float64})

\item {} 
\code{Transmitted} (\code{NWavelength} entries of numpy \code{float64})

\item {} 
\code{Emitted} (\code{NWavelength} entries of numpy \code{float64})

\item {} 
\code{Transmitted\_plus\_emitted} (\code{NWavelength} entries of numpy
\code{float64})

\end{itemize}

There is one such record for each output time, with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial.


\subsection{The \texttt{integrated\_cloudyphot} File}
\label{cloudy:the-integrated-cloudyphot-file}
This file contains photometric data computed for the spectra produced
by the interaction between the stellar radiation field and the HII
region. The file consists of a series of entries containing the
following fields:
\begin{itemize}
\item {} 
\code{Time}: evolution time at which the output is computed

\item {} 
\code{PhotFilter1\_trans}: photometric value for the \emph{Transmitted}
radiation field through filter 1, where filter 1 here is the same as
filter 1 in {\hyperref[output:ssec-int-phot-file]{\emph{\DUspan{}{The integrated\_phot File}}}}; units are also the same as
in that file.

\item {} 
\code{PhotFilter1\_emit}: photometric value for the \emph{Emitted}
radiation field through filter 1

\item {} 
\code{PhotFilter1\_trans\_emit}: photometric value for the
\emph{Transmitted\_plus\_emitted} radiation field through filter 1

\item {} 
\code{PhotFilter2\_trans}

\item {} 
\code{PhotFilter2\_emit}

\item {} 
\code{PhotFilter2\_trans\_emit}

\item {} 
\code{...}

\end{itemize}

For distinctions between the \emph{Transmitted}, \emph{Emitted}, and
\emph{Transmitted\_plus\_emitted} radiation fields, see
{\hyperref[cloudy:sssec-int-cloudyspec-file]{\emph{\DUspan{}{The integrated\_cloudyspec File}}}}, or the \href{http://nublado.org}{cloudy documentaiton}. Note that we do not record photometry for the
incident spectrum, since that would be, up to the accuracy of the
numerical integration, identical to the photometry already recorded in
the {\hyperref[output:ssec-int-phot-file]{\emph{\DUspan{}{The integrated\_phot File}}}}.

If the SLUG data input to cloudy\_slug were written in \code{ascii} mode,
these data are output as a text file containing a series of columns,
with different trials separated by lines of dashes.

If the SLUG data input to cloudy\_slug were written in \code{fits} mode,
these data are written in a FITS file containing one binary table
extension, consisting of a series of columns. The columns are
\code{Trial}, \code{Time}, \code{Filter1\_Transmitted}, \code{Filter1\_Emitted},
\code{Filter1\_Transmitted\_plus\_emitted}, \code{...}. The first two columns
give the trial number and the time, and the remainder give the
photometric values for the transmitted, emitted, and transmitted plus
emitted spectra in each filter.

If the SLUG data input to cloudy\_slug were written in \code{binary} mode,
these data are written to a raw binary file that is formatted as
follows. The file starts with an ASCII header consisting of the
following, each on a separate line:
\begin{itemize}
\item {} 
\code{NFilter} (stored as \code{ASCII text}): number of filters used

\item {} 
\code{FilterName} \code{FilterUnit} (\code{NFilter} entries stored as \code{ASCII
text}): the name and units for each filter are listed in ASCII, one
filter-unit pair per line

\end{itemize}

This is followed by a series of entries of the form:
\begin{itemize}
\item {} 
\code{PhotFilter\_Transmitted} (\code{NFilter} entries of numpy
\code{float64}), giving the transmitted photometry in each filter

\item {} 
\code{PhotFilter\_Emitted} (\code{NFilter} entries of numpy
\code{float64}), giving the emitted photometry in each filter

\item {} 
\code{PhotFilter\_Transmitted\_plus\_emitted} (\code{NFilter} entries of numpy
\code{float64}), giving the transmitted plus emitted photometry in each
filter

\end{itemize}

There is one such record for each output time, with different trials
ordered sequentially, so that all the times for one trial are output
before the first time for the next trial.


\subsection{The \texttt{cluster\_cloudylines} File}
\label{cloudy:the-cluster-cloudylines-file}
This file contains data on the nebular line emission produced by the
interaction of the stellar radiation field with the ISM around each
cluster. It consists of a series of entries containing the following
fields:
\begin{itemize}
\item {} 
\code{UniqueID}: a unique identifier number for each cluster that is
preserved across times and output files

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{LineLabel}: four letter code labeling each line. These codes
are the codes used by cloudy (see the \href{http://nublado.org}{cloudy documentation})

\item {} 
{}`{}` Wavelength{}`{}`: wavelength of the line, in Angstrom. Note that
default cloudy behavior is to round wavelengths to the nearest
Angstrom.

\item {} 
\code{Luminosity}: line luminosity, in erg/s

\end{itemize}

If the SLUG data input to cloudy\_slug were written in \code{ascii} mode,
these data are output as a text file containing a series of columns,
with different trials separated by lines of dashes.

If the SLUG data input to cloudy\_slug were written in \code{fits} mode,
the data are written in a FITS file containing two binary table
extensions. The first extension contains two fields, \code{Line\_label} and
\code{Wavelength}, giving the four-letter cloudy line codes and central
wavelengths. The second extension contains four columns, giving the
unique ID, trial number, time, and line luminosity for each line at
each time in each trial.

If the SLUG data input to cloudy\_slug were written in \code{binary} mode,
the data are written in a raw binary file. The file starts with a
header consisting of
\begin{itemize}
\item {} 
\code{NLine} (python \code{int}, equivalent to C \code{long}): number of lines

\item {} 
\code{LineLabel} (\code{NLine} entries stored as \code{ASCII text}): line
labels listed in ASCII, one label per line

\end{itemize}

This is followed by a series of records, one for each output time, with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial. Each record consists of a header containing
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{NCluster} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): number of non-disrupted clusters present at this time

\end{itemize}

This is followed by \code{NCluster} entries of the following form:
\begin{itemize}
\item {} 
\code{UniqueID} (numpy \code{uint64})

\item {} 
\code{LineLum} (\code{NLine} entries of numpy \code{float64})

\end{itemize}


\subsection{The \texttt{cluster\_cloudyspec} File}
\label{cloudy:the-cluster-cloudyspec-file}
This file contains data on the spectra produced by the interaction of
the stellar radiation field with the ISM around each cluster. It
consists of a series of entries containing the following fields:
\begin{itemize}
\item {} 
\code{UniqueID}: a unique identifier number for each cluster that is
preserved across times and output files

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{Wavelength}: observed frame wavelength at which the spectrum is evaluated

\item {} 
\code{Incident}: specific luminosity in erg/s/Angstrom at the specified
wavelength for the \emph{incident} radiation field

\item {} 
\code{Transmitted}: specific luminosity in erg/s/Angstrom at the specified
wavelength for the \emph{transmitted} radiation field

\item {} 
\code{Emitted}: specific luminosity in erg/s/Angstrom at the specified
wavelength for the \emph{emitted} radiation field

\item {} 
\code{Transmitted\_plus\_emitted}: specific luminosity in erg/s/Angstrom
at the specified wavelength for the \emph{transmitted plus emitted}
radiation field

\end{itemize}

For explanations of the distinction between the incident, transmitted,
emitted, and transmitted plus emitted radiation fields, see
{\hyperref[cloudy:sssec-int-cloudyspec-file]{\emph{\DUspan{}{The integrated\_cloudyspec File}}}}.

If the SLUG data input to cloudy\_slug were written in \code{ascii} mode,
these data are output as a text file containing a series of columns,
with different trials separated by lines of dashes.

If the SLUG data input to cloudy\_slug were written in \code{fits} mode,
these data are written in a FITS file containing two binary table
extensions. The first table contains a column \code{Wavelength} listing
the wavelengths at which the spectra are given. The second table
consists of seven columns: \code{Trial}, \code{UniqueID}, \code{Time},
\code{Incident\_spectrum}, \code{Transmitted\_spectrum}, \code{Emitted\_spectrum},
and \code{Transmitted\_plus\_emitted\_spectrum}. The first three of these
give the trial number, unique ID of the cluster, and the time. The
remaining four give the incident, transmitted, emitted, and
transmitted plus emitted spectra for the corresponding cluster.

If the SLUG data input to cloudy\_slug were written in \code{binary} mode,
these data are written to a raw binary file formatted as follows. The
file starts with
\begin{itemize}
\item {} 
\code{NWavelength} (numpy \code{int64}): the number of wavelength entries in the spectra

\item {} 
\code{Wavelength} (\code{NWavelength} entries of type \code{double})

\end{itemize}

and then contains a series of records, one for each output time, with
different trials ordered sequentially, so that all the times for one
trial are output before the first time for the next trial. Each record
consists of a header containing
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{NCluster} (python \code{int}): number of non-disrupted clusters present at this time

\end{itemize}

This is followed by \code{NCluster} entries of the following form:
\begin{itemize}
\item {} 
\code{UniqueID} (\code{unsigned long})

\item {} 
\code{Incident} (\code{NWavelength} entries of numpy \code{float64})

\item {} 
\code{Transmitted} (\code{NWavelength} entries of numpy \code{float64})

\item {} 
\code{Emitted} (\code{NWavelength} entries of numpy \code{float64})

\item {} 
\code{Transmitted\_plus\_emitted} (\code{NWavelength} entries of numpy
\code{float64})

\end{itemize}


\subsection{The \texttt{cluster\_cloudyphot} File}
\label{cloudy:the-cluster-cloudyphot-file}
This file contains data on the photometry of the spectra produced by
the interaction of the stellar radiation field with the ISM around
each cluster. It consists of a series of entries containing the
following fields:
\begin{itemize}
\item {} 
\code{UniqueID}: a unique identifier number for each cluster that is
preserved across times and output files

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{PhotFilter1\_trans}: photometric value for the \emph{Transmitted}
radiation field through filter 1, where filter 1 here is the same as
filter 1 in {\hyperref[output:ssec-int-phot-file]{\emph{\DUspan{}{The integrated\_phot File}}}}; units are also the same as
in that file.

\item {} 
\code{PhotFilter1\_emit}: photometric value for the \emph{Emitted}
radiation field through filter 1

\item {} 
\code{PhotFilter1\_trans\_emit}: photometric value for the
\emph{Transmitted\_plus\_emitted} radiation field through filter 1

\item {} 
\code{PhotFilter2\_trans}

\item {} 
\code{PhotFilter2\_emit}

\item {} 
\code{PhotFilter2\_trans\_emit}

\item {} 
\code{...}

\end{itemize}

For distinctions between the \emph{Transmitted}, \emph{Emitted}, and
\emph{Transmitted\_plus\_emitted} radiation fields, see
{\hyperref[cloudy:sssec-int-cloudyspec-file]{\emph{\DUspan{}{The integrated\_cloudyspec File}}}}, or the \href{http://nublado.org}{cloudy documentaiton}. Note that we do not record photometry for the
incident spectrum, since that would be, up to the accuracy of the
numerical integration, identical to the photometry already recorded in
the {\hyperref[output:ssec-cluster-phot-file]{\emph{\DUspan{}{The cluster\_phot File}}}}.

If the SLUG data input to cloudy\_slug were written in \code{ascii} mode,
these data are output as a text file containing a series of columns,
with different trials separated by lines of dashes.

If the SLUG data input to cloudy\_slug were written in \code{fits} mode,
these data are written in a FITS file containing one binary table
extension. The columns in this FITS file are \code{Trial}, \code{UniqueID},
\code{Time}, \code{Filter1\_Transmitted}, \code{Filter1\_Emitted},
\code{Filter1\_Transmitted\_plus\_emitted}, \code{...}. The first three columns
give the trial number, cluster unique ID, and the time, and the
remainder give the photometric values for the transmitted, emitted,
and transmitted plus emitted spectra in each filter.

If the SLUG data input to cloudy\_slug were written in \code{binary} mode,
these data are written in a raw binary file that is formatted as
follows. The file starts with an ASCII text header consisting of the
following, each on a separate line:
\begin{itemize}
\item {} 
\code{NFilter} (stored as \code{ASCII text}): number of filters used

\item {} 
\code{FilterName} \code{FilterUnit} (\code{NFilter} entries stored as \code{ASCII
text}): the name and units for each filter are listed in ASCII, one
filter-unit pair per line

\end{itemize}

This is followed by a series of entries of that each begin with a
header
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{NCluster} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): number of non-disrupted clusters present at this time

\end{itemize}

This is followed by \code{NCluster} entries of the following form:
\begin{itemize}
\item {} 
\code{UniqueID} (\code{unsigned long})

\item {} 
\code{PhotFilter\_Transmitted} (\code{NFilter} entries of numpy
\code{float64}), giving the transmitted photometry in each filter

\item {} 
\code{PhotFilter\_Emitted} (\code{NFilter} entries of numpy
\code{float64}), giving the emitted photometry in each filter

\item {} 
\code{PhotFilter\_Transmitted\_plus\_emitted} (\code{NFilter} entries of numpy
\code{float64}), giving the transmitted plus emitted photometry in each
filter

\end{itemize}


\section{Full Documentation of slugpy.cloudy}
\label{cloudy:module-slugpy.cloudy}\label{cloudy:full-documentation-of-slugpy-cloudy}\index{slugpy.cloudy (module)}\index{read\_cloudy\_continuum() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_cloudy_continuum}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_cloudy\_continuum}}{\emph{filename}, \emph{r0=None}}{}
Reads a cloudy continuum output, produced by save last continuum
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{filename}] \leavevmode{[}string{]}
name of the file to be read

\item[{r0}] \leavevmode{[}float{]}
inner radius, in cm; if included, the quantities returned will
be total energies instead of energy emission rates instead of
rates per unit area

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{wl}] \leavevmode{[}array{]}
wavelengths in Angstrom

\item[{incident}] \leavevmode{[}array{]}
incident radiation field intensity

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_cloudy\_linelist() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_cloudy_linelist}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_cloudy\_linelist}}{\emph{filename}}{}
Reads a cloudy line list output, produced by save last line list
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{filename}] \leavevmode{[}string{]}
name of the file to be read

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{labels}] \leavevmode{[}array, dtype `S4'{]}
list of line labels

\item[{wl}] \leavevmode{[}array{]}
array of line wavelengths, in Angstrom

\item[{lum}] \leavevmode{[}array{]}
array of line luminosities; this will be in whatever units the
cloudy output is in

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_cluster\_cloudyphot() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_cluster_cloudyphot}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_cluster\_cloudyphot}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{nofilterdata=False}, \emph{photsystem=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 cluster\_cloudyphot file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{nofilterdata}] \leavevmode{[}bool{]}
If True, the routine does not attempt to read the filter
response data from the standard location

\item[{photsystem}] \leavevmode{[}None or string{]}
If photsystem is None, the data will be returned in the same
photometric system in which they were read. Alternately, if it
is a string, the data will be converted to the specified
photometric system. Allowable values are `L\_nu', `L\_lambda',
`AB', `STMAG', and `Vega', corresponding to the options defined
in the SLUG code. If this is set and the conversion requested
involves a conversion from a wavelength-based system to a
frequency-based one, nofilterdata must be False so that the
central wavelength of the photometric filters is available.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
times at which cluster spectra are output, in yr

\item[{cloudy\_filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{cloudy\_filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{cloudy\_filter\_wl\_eff}] \leavevmode{[}list{]}
effective wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{cloudy\_filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1; omitted if
nofilterdata is True

\item[{cloudy\_filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1; omitted
if nofilterdata is True

\item[{cloudy\_filter\_beta}] \leavevmode{[}list{]}
powerlaw index beta for each filter; used to normalize the
photometry

\item[{cloudy\_filter\_wl\_c}] \leavevmode{[}list{]}
pivot wavelength for each filter; used to normalize the photometry

\item[{cloudy\_phot\_trans}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value for each cluster in each filter for the
transmitted light (i.e., the starlight remaining after it has
passed through the HII region); units are as indicated in
the units field

\item[{cloudy\_phot\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value for each cluster in each filter for the
emitted light (i.e., the diffuse light emitted by the HII
region); units are as indicated in the units field

\item[{cloudy\_phot\_trans\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value in each filter for each cluster for the
transmitted plus emitted light (i.e., the light coming
directly from the stars after absorption by the HII region,
plus the diffuse light emitted by the HII region); units are as
indicated in the units field

\end{description}

\item[{Raises}] \leavevmode
IOError, if no photometry file can be opened;
ValueError, if photsystem is set to an unknown value

\end{description}

\end{fulllineitems}

\index{read\_cluster\_cloudylines() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_cluster_cloudylines}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_cluster\_cloudylines}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 cluster\_cloudylines file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
times at which cluster spectra are output, in yr

\item[{cloudy\_linelabel}] \leavevmode{[}array, dtype='S4', shape (N\_lines){]}
labels for the lines, following cloudy's 4 character line label
notation

\item[{cloudy\_linewl}] \leavevmode{[}array, shape (N\_lines){]}
rest wavelength for each line, in Angstrom

\item[{cloudy\_linelum}] \leavevmode{[}array, shape (N\_cluster, N\_lines){]}
luminosity of each line at each time for each trial, in erg/s

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_cluster\_cloudyspec() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_cluster_cloudyspec}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_cluster\_cloudyspec}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 cluster\_cloudyspec file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
times at which cluster spectra are output, in yr

\item[{cloudy\_wl}] \leavevmode{[}array{]}
wavelength, in Angstrom

\item[{cloudy\_inc}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of the cluster's stellar radiation field at
each wavelength, in erg/s/A

\item[{cloudy\_trans}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of the stellar radiation field after it has
passed through the HII region, at each wavelength, in erg/s/A

\item[{cloudy\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of the radiation field emitted by the HII
region, at each wavelength, in erg/s/A

\item[{cloudy\_trans\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
the sum of the emitted and transmitted fields; this is what
would be seen by an observer looking at both the star cluster
and its nebula

\end{description}

\item[{Raises}] \leavevmode
IOError, if no spectrum file can be opened

\end{description}

\end{fulllineitems}

\index{read\_integrated\_cloudylines() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_integrated_cloudylines}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_integrated\_cloudylines}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 integrated\_cloudylines file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{time}] \leavevmode{[}array, shape (N\_times) or shape (N\_trials){]}
Times at which data are output; shape is either N\_times (if
the run was done with fixed output times) or N\_trials (if
the run was done with random output times)

\item[{cloudy\_linelabel}] \leavevmode{[}array, dtype='S4', shape (N\_lines){]}
labels for the lines, following cloudy's 4 character line label
notation

\item[{cloudy\_linewl}] \leavevmode{[}array, shape (N\_lines){]}
rest wavelength for each line, in Angstrom

\item[{cloudy\_linelum}] \leavevmode{[}array, shape (N\_lines, N\_times, N\_trials){]}
luminosity of each line at each time for each trial, in erg/s

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_integrated\_cloudyphot() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_integrated_cloudyphot}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_integrated\_cloudyphot}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{nofilterdata=False}, \emph{photsystem=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 integrated\_cloudyphot file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{nofilterdata}] \leavevmode{[}bool{]}
If True, the routine does not attempt to read the filter
response data from the standard location

\item[{photsystem}] \leavevmode{[}None or string{]}
If photsystem is None, the data will be returned in the same
photometric system in which they were read. Alternately, if it
is a string, the data will be converted to the specified
photometric system. Allowable values are `L\_nu', `L\_lambda',
`AB', `STMAG', and `Vega', corresponding to the options defined
in the SLUG code. If this is set and the conversion requested
involves a conversion from a wavelength-based system to a
frequency-based one, nofilterdata must be False so that the
central wavelength of the photometric filters is available.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{time}] \leavevmode{[}array, shape (N\_times) or shape (N\_trials){]}
Times at which data are output; shape is either N\_times (if
the run was done with fixed output times) or N\_trials (if
the run was done with random output times)

\item[{cloudy\_filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{cloudy\_filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{cloudy\_filter\_wl\_eff}] \leavevmode{[}list{]}
effective wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{cloudy\_filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1; omitted if
nofilterdata is True

\item[{cloudy\_filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1; omitted
if nofilterdata is True

\item[{cloudy\_filter\_beta}] \leavevmode{[}list{]}
powerlaw index beta for each filter; used to normalize the
photometry

\item[{cloudy\_filter\_wl\_c}] \leavevmode{[}list{]}
pivot wavelength for each filter; used to normalize the photometry

\item[{cloudy\_phot\_trans}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial for
the transmitted light (i.e., the starlight remaining after it
has passed through the HII region); units are as indicated in
the units field

\item[{cloudy\_phot\_emit}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial for
the emitted light (i.e., the diffuse light emitted by the HII
region); units are as indicated in the units field

\item[{cloudy\_phot\_trans\_emit}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial for
the transmitted plus emitted light (i.e., the light coming
directly from the stars after absorption by the HII region,
plus the diffuse light emitted by the HII region); units are as
indicated in the units field

\end{description}

\item[{Raises}] \leavevmode
IOError, if no photometry file can be opened;
ValueError, if photsystem is set to an unknown value

\end{description}

\end{fulllineitems}

\index{read\_integrated\_cloudyspec() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_integrated_cloudyspec}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_integrated\_cloudyspec}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 integrated\_cloudyspec file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{time}] \leavevmode{[}array, shape (N\_times) or shape (N\_trials){]}
Times at which data are output; shape is either N\_times (if
the run was done with fixed output times) or N\_trials (if
the run was done with random output times)

\item[{cloudy\_wl}] \leavevmode{[}array{]}
wavelength, in Angstrom

\item[{cloudy\_inc}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity of the stellar radiation field at each
wavelength and each time for each trial, in erg/s/A

\item[{cloudy\_trans}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity of the stellar radiation field after it has
passed through the HII region, at each wavelength and each time
for each trial, in erg/s/A

\item[{cloudy\_emit}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity of the radiation field emitted by the HII
region, at each wavelength and each time for each trial, in
erg/s/A

\item[{cloudy\_trans\_emit}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
the sum of emitted and transmitted; this is what would be seen
by an observer looking at both the star cluster and its nebula

\end{description}

\end{description}

\end{fulllineitems}

\index{write\_cluster\_cloudyphot() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.write_cluster_cloudyphot}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{write\_cluster\_cloudyphot}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Write out photometry for nebular emission computed by cloudy on a
slug spectrum for a series of clusters
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Cluster cloudy photometry data to be written; a namedtuple
containing the fields id, time, cloudy\_filter\_names, 
cloudy\_filter\_units, cloudy\_phot\_trans, cloudy\_phot\_emit,
and cloudy\_phot\_trans\_emit

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}

\index{write\_cluster\_cloudylines() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.write_cluster_cloudylines}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{write\_cluster\_cloudylines}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Write out data computed by cloudy on a slug spectrum
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Cloudy spectral data for clusters to be written; a namedtuple
containing the fields time, cloudy\_linelist, cloudy\_linewl, 
cloudy\_linelum

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}

\index{write\_cluster\_cloudyspec() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.write_cluster_cloudyspec}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{write\_cluster\_cloudyspec}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Write out data computed by cloudy on a slug spectrum
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Cloudy spectral data for clusters to be written; a namedtuple
containing the fields id, time, cloudy\_wl, cloudy\_inc, cloudy\_trans,
cloudy\_emit, and cloudy\_trans\_emit

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}

\index{write\_integrated\_cloudylines() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.write_integrated_cloudylines}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{write\_integrated\_cloudylines}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Write out line luminosities computed by cloudy on a slug spectrum
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Integrated cloudy line data to be written; a namedtuple
containing the fields time, cloudy\_linelist, cloudy\_linewl, 
cloudy\_linelum

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}

\index{write\_integrated\_cloudyphot() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.write_integrated_cloudyphot}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{write\_integrated\_cloudyphot}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Write out photometry for nebular emission computed by cloudy on a
slug spectrum
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Integrated cloudy photometry data to be written; a namedtuple
containing the fields time, cloudy\_filter\_names, 
cloudy\_filter\_units, cloudy\_phot\_trans, cloudy\_phot\_emit,
and cloudy\_phot\_trans\_emit

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}

\index{write\_integrated\_cloudyspec() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.write_integrated_cloudyspec}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{write\_integrated\_cloudyspec}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Write out data computed by cloudy on a slug spectrum
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Integrated cloudy spectral data to be written; a namedtuple
containing the field time, cloudy\_wl, cloudy\_inc, cloudy\_trans,
cloudy\_emit, and cloudy\_trans\_emit

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}



\chapter{bayesphot: Bayesian Inference for Stochastic Stellar Populations}
\label{bayesphot:sec-bayesphot}\label{bayesphot::doc}\label{bayesphot:bayesphot-bayesian-inference-for-stochastic-stellar-populations}

\section{What Does bayesphot Do?}
\label{bayesphot:what-does-bayesphot-do}
Bayesphot is a package for performing Bayesian inference for the physical properties of a stellar system using its measured photometric properties, in a case where the photometric properties vary non-deterministically with the physical properties. Formally, bayesphot answers the following question: consider a stellar system characterized by a vector of \(\mathbf{x} = (x_1, x_2, \ldots x_N)\) physical properties. We have a physical model that lets us sample the expected photometric properties as a function of physical properties, i.e., that for some sample of \(K\) systems with physical properties \(\mathbf{x}_k\) we are able to compute the corresponding photometric properties \(\mathbf{y}_k = \mathbf{y} = (y_1, y_2, \ldots y_M)_k\). Now suppose that we observe such a system, and we observe it to have photometric properties \(\mathbf{y}_{\mathrm{obs}}\), with some set of photometric errors \(\mathbf{\sigma}_{\mathbf{y}} = (\sigma_{y_1}, \sigma_{y_2}, \ldots \sigma_{y_M})\), which are assumed to be Gaussian-distributed. What should we infer about the posterior probability distribution of the physical properties, i.e., given a set of prior probabilities \(p(\mathbf{x})\), plus our measurements, what is \(p(\mathbf{x} \mid \mathbf{y}_{\mathrm{obs}}, \mathbf{\sigma}_{\mathrm{y}})\)?

The kernel density estimation algorithm that bayesphot uses to answer this question is described and derived in the slug methods paper. Bayesphot is implemented in two parts: a shared object library that is implemented in c, and that is compiled at the same time that slug is built, and a python wrapper class called \code{bp} that is included in the slugpy.bayesphot module. The following sections describe how to use \code{bp} objects to generate posterior PDFs.


\section{Creating \texttt{bp} Objects}
\label{bayesphot:creating-bp-objects}
The \code{bp} class can be imported via:

\begin{Verbatim}[commandchars=\\\{\}]
from slugpy.bayesphot import *
\end{Verbatim}

or:

\begin{Verbatim}[commandchars=\\\{\}]
from slugpy.bayesphot import bp\PYG{l+s}{{}`{}`}
\end{Verbatim}

Once imported, a \code{bp} object can be instantiated. The call signature for the \code{bp} constructor class is:

\begin{Verbatim}[commandchars=\\\{\}]
def \PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}(self, dataset, nphys, filters=None, bandwidth=\PYGZsq{}auto\PYGZsq{},
             ktype=\PYGZsq{}gaussian\PYGZsq{}, priors=None, sample\PYGZus{}density=None,
             reltol=1.0e\PYGZhy{}3, abstol=1.0e\PYGZhy{}10, leafsize=16):
\end{Verbatim}

A full description of all options is included in the {\hyperref[bayesphot:ssec-slugpy-bayesphot]{\emph{\DUspan{}{Full Documentation of slugpy.bayesphot}}}}, but the essential features are summarized here.

The argument \code{dataset} is an array of shape (N, M) that contains the library of N models that represents the training set for the Bayesian analysis. Each model consists of M properties; the first \code{nphys} of these are physical properties that are the quantities to be inferred from the observations, while the remaining ones are photometric properties. An important point is that the \code{dataset} object is NOT copied, so altering it after the \code{bp} object is created will result in erroneous results.

The \code{priors} and \code{sample\_density} arguments are used to compute the weighting to apply to the input models. The \code{priors} argument specifies the prior probability to assign to each model; it can be either an array giving a prior probability directly, or a callable that can take the physical properties of models as an input and return the prior probability as an output. Similarly, the \code{sample\_density} argument specifies the probablity distribution from which the physical models were selected; as with \code{priors}, it can be an array or a callable.

The \code{bandwidth} argument specifies the bandwidth to use in the kernel density estimation; this need not be the same in each dimension. The \code{bandwidth} can be specified as a float, in which case it is the same for every dimension, or as an array of M elements giving the bandwidth for every dimension. Finally, it can be set to the string \code{auto}, in which case the \code{bp} will attempt to make a reasonable choice of bandwidth autonomously. However, this autonomous choice will probably perform less well than something that is hand-chosen by the user based on their knowledge of the library. As a rule of thumb, bandwidths should be chosen so that, for typical input photometric values, there are \textasciitilde{}10 simulations within the 1 kernel size.

Note that both \code{priors} and \code{bandwidth} are properties of the \code{bp} class, and can be altered after the \code{bp} object is created. This makes it possible to alter the priors and bandwidth without incurring the computational or memory cost of generating an entirely new \code{bp} object.


\section{Using \texttt{bp} Objects}
\label{bayesphot:using-bp-objects}
Once a \code{bp} object is instantiated, it can be used to compute likelihood functions, marginal probabilities, and MCMC sample ensembles, and to search the library for the best matches to an input set of photometry.

The likelihood function is implemented via the \code{bp.logL} method, which has the call signature:

\begin{Verbatim}[commandchars=\\\{\}]
def logL(self, physprop, photprop, photerr=None):
\end{Verbatim}

The argument \code{physprop} is a set of physical properties, the argument \code{photprop} is a set of photometric properties, and the argument \code{photerr} is an (optional) set of photometric errors. All of these must be arrays, the size of whose trailing dimension matches the number of physical properties (for \code{physprop}) or the number of photometric properties (for \code{photprop} and \code{photerr}); the leading dimensions of these arrays are broadcast together using normal broadcasting rules. The quantity returned is the log of the joint probability distribution of physical and photometric properties. Specifically, the quantity returned for each input set of physical and photometric properties is
\begin{gather}
\begin{split}\log p(\mathbf{x}, \mathbf{y}, \sigma_{\mathbf{y}}) = \log A \sum_{i=1}^N w_i G(\mathbf{x}, \mathbf{y}; \mathbf{h}')\end{split}\notag
\end{gather}
where \(A\) is a normalization constant chosen to ensure that the PDF integrated over all space is unity, \(\mathbf{x}\) is the vector of physical properties, \(\mathbf{y}\) is the vector of photometric properties, \(\sigma_\mathbf{y}\) is the vector of photomtric errors, \(w_i\) is the weight of the ith model as determined by the priors and sample density,
\begin{gather}
\begin{split}G\left(\mathbf{x}, \mathbf{y}; \mathbf{h}'\right) \propto \exp\left[-\left(\frac{x_1^2}{2h_{x_1}'^2} + \cdots + \frac{x_N^2}{2h_{x_N}'^2} + \frac{y_1^2}{2h_{y_1}'^2} + \cdots + \frac{y_M^2}{2h_{y_M}'^2} \right)\right]\end{split}\notag
\end{gather}
is the N-dimensional Gaussian function, and
\begin{gather}
\begin{split}\mathbf{h'} = \sqrt{\mathbf{h}^2 + \sigma_{\mathbf{y}}^2}\end{split}\notag
\end{gather}
is the modified bandwidth, which is equal to the bandwidth used for kernel density estimation added in quadrature sum with the errors in the photometric quantities (see the slug method paper for details).

Estimation of marginal PDFs is done via the \code{bp.mpdf} method, which has the call signature:

\begin{Verbatim}[commandchars=\\\{\}]
def mpdf(self, idx, photprop, photerr=None, ngrid=128,
         qmin=None, qmax=None, grid=None, norm=True):
\end{Verbatim}

The argument \code{idx} is an int or a list of ints between 0 and nphys-1, which specifies for which physical quantity or physical quantities the marginal PDF is to be computed. These indices refer to the indices in the \code{dataset} array that was input when the \code{bp} object was instantiated. The arguments \code{photprop} and \code{photerr} give the photometric measurements and their errors for which the marginal PDFs are to be computed; they must be arrays whose trailing dimension is equal to the number of photometric quantities. The leading dimensions of these arrays are broadcast together following the normal broadcasting rules. By default each physical quantity will be estimated on a grid of 128 points, evenly spaced from the lowest value of that physical property in the model library to the highest value. The parameters \code{qmin}, \code{qmax}, \code{ngrid}, and \code{grid} can be used to override this behavior and set the grid of evaluation points manually. The function returns a tuple \code{grid\_out, pdf}; here \code{grid\_out} is the grid of points on which the marginal PDF has been computed, and \code{pdf} is the value of the marginal PDF evaluated at those gridpoints.

MCMC calculations are implemented via the method \code{bp.mcmc}; this method relies on the \href{http://dan.iel.fm/emcee/current/}{emcee} python module, and will only function if it is installed. The call signature is:

\begin{Verbatim}[commandchars=\\\{\}]
def mcmc(self, photprop, photerr=None, mc\PYGZus{}walkers=100,
         mc\PYGZus{}steps=500, mc\PYGZus{}burn\PYGZus{}in=50):
\end{Verbatim}

The quantities \code{photprop} and \code{photerr} have the same meaning as for \code{bp.mpdf}, and the quantities \code{mc\_walkers}, \code{mc\_steps}, and \code{mc\_burn\_in} are passed directly to \code{emcee}, and are described in \href{http://dan.iel.fm/emcee/current/}{emcee's documentation}. The quantity returned is an array of sample points computed by the MCMC; its format is also described in \href{http://dan.iel.fm/emcee/current/}{emcee's documentation}. Note that, although \code{bp.mcmc} can be used to compute marginal PDFs of the physical quantities, for marginal PDFs of 1 quantity or joint PDFs of 2 quantities it is almost always faster to use \code{bp.mpdf} than \code{bp.mcmc}. This is because \code{bp.mpdf} takes advantage of the fact that integrals of cuts through N-dimensional Gaussians can be integrated analytically to compute the marginal PDFs directly, though needing to evaluate the likelihood function point by point. In contrast, the general MCMC algorithm used by \code{emcee} effectively does the integral numerically.

The \code{bp.bestmatch} method searches through the model library and finds the N library entries that are closest to an input set of photometry. The call signature is:

\begin{Verbatim}[commandchars=\\\{\}]
def bestmatch(self, phot, nmatch=1, bandwidth\PYGZus{}units=False):
\end{Verbatim}

Here \code{phot} is the set of photometric properties, which is identical to the \code{photprop} parameter used by \code{logL}, \code{mpdf}, and \code{mcmc}. The argument \code{nmatch} specifies how many matches to return, and the argument \code{bandwidth\_units} specifies whether distances are to be measured using an ordinary Euclidean metric, or in units of the kernel bandwidth in a given direction. The function returns, for each input set of photometry, the physical and photometric properties of the \code{nmatch} models in the library that are closest to the input photometric values. This can be used to judge if a good match to the input photometry is present in the library.


\section{Full Documentation of slugpy.bayesphot}
\label{bayesphot:module-slugpy.bayesphot.bp}\label{bayesphot:ssec-slugpy-bayesphot}\label{bayesphot:full-documentation-of-slugpy-bayesphot}\index{slugpy.bayesphot.bp (module)}
This defines a class that can be used to estimate the PDF of physical
quantities from a set of input photometry in various bands, together
with a training data set.
\index{bp (class in slugpy.bayesphot.bp)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp}\pysiglinewithargsret{\strong{class }\code{slugpy.bayesphot.bp.}\bfcode{bp}}{\emph{dataset}, \emph{nphys}, \emph{filters=None}, \emph{bandwidth='auto'}, \emph{ktype='gaussian'}, \emph{priors=None}, \emph{sample\_density=None}, \emph{reltol=0.01}, \emph{abstol=1e-06}, \emph{leafsize=16}}{}
A class that can be used to estimate the PDF of the physical
properties of stellar population from a training set plus a set of
measured photometric values.
\begin{description}
\item[{Properties}] \leavevmode\begin{description}
\item[{priors}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} None{]}
prior probability on each data point; interpretation
depends on the type passed; array, shape (N): values are
interpreted as the prior probability of each data point;
callable: the callable must take as an argument an array
of shape (N, nphys), and return an array of shape (N)
giving the prior probability at each data point; None:
all data points have equal prior probability

\item[{bandwidth}] \leavevmode{[}`auto' \textbar{} float \textbar{} array, shape (M){]}
bandwidth for kernel density estimation; if set to
`auto', the bandwidth will be estimated automatically; if
set to a scalar quantity, the same bandwidth is used for all
dimensions

\end{description}

\end{description}
\index{\_\_init\_\_() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{dataset}, \emph{nphys}, \emph{filters=None}, \emph{bandwidth='auto'}, \emph{ktype='gaussian'}, \emph{priors=None}, \emph{sample\_density=None}, \emph{reltol=0.01}, \emph{abstol=1e-06}, \emph{leafsize=16}}{}
Initialize a bp object.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{dataset}] \leavevmode{[}array, shape (N, M){]}
training data set; this is a set of N sample stellar
populations, having M properties each; the first npys
represent physical properties (e.g., log mass, log age),
while the next M - nphys represent photometric
properties

\item[{npys}] \leavevmode{[}int{]}
number of physical properties in dataset

\item[{filters}] \leavevmode{[}listlike of strings{]}
names of photometric filters; not used, but can be
stored for convenience

\item[{bandwidth}] \leavevmode{[}`auto' \textbar{} float \textbar{} array, shape (M){]}
bandwidth for kernel density estimation; if set to
`auto', the bandwidth will be estimated automatically; if
set to a scalar quantity, the same bandwidth is used for all
dimensions

\item[{ktype}] \leavevmode{[}string{]}
type of kernel to be used in densty estimation; allowed
values are `gaussian' (default), `epanechnikov', and
`tophat'; only Gaussian can be used with error bars

\item[{priors}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} None{]}
prior probability on each data point; interpretation
depends on the type passed; array, shape (N): values are
interpreted as the prior probability of each data point;
callable: the callable must take as an argument an array
of shape (N, nphys), and return an array of shape (N)
giving the prior probability at each data point; None:
all data points have equal prior probability

\item[{sample\_density}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} `auto' \textbar{} None{]}
the density of the data samples at each data point; this
need not match the prior density; interpretation depends
on the type passed; array, shape (N): values are
interpreted as the density of data sampling at each
sample point; callable: the callable must take as an
argument an array of shape (N, nphys), and return an
array of shape (N) giving the sampling density at each
point; `auto': the sample density will be computed
directly from the data set; note that this can be quite
slow for large data sets, so it is preferable to specify
this analytically if it is known; None: data are assumed
to be uniformly sampled

\item[{reltol}] \leavevmode{[}float{]}
relative error tolerance; errors on all returned
probabilities p will satisfy either
abs(p\_est - p\_true) \textless{}= reltol * p\_est   OR
abs(p\_est - p\_true) \textless{}= abstol,
where p\_est is the returned estimate and p\_true is the
true value

\item[{abstol}] \leavevmode{[}float{]}
absolute error tolerance; see above

\item[{leafsize}] \leavevmode{[}int{]}
number of data points in each leaf of the KD tree

\end{description}

\item[{Returns}] \leavevmode
Nothing

\item[{Raises}] \leavevmode
IOError, if the bayesphot c library cannot be found

\end{description}

\end{fulllineitems}

\index{\_\_weakref\_\_ (slugpy.bayesphot.bp.bp attribute)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.__weakref__}\pysigline{\bfcode{\_\_weakref\_\_}}
list of weak references to the object (if defined)

\end{fulllineitems}

\index{bandwidth (slugpy.bayesphot.bp.bp attribute)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.bandwidth}\pysigline{\bfcode{bandwidth}}
The current bandwidth

\end{fulllineitems}

\index{bestmatch() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.bestmatch}\pysiglinewithargsret{\bfcode{bestmatch}}{\emph{phot}, \emph{photerr=None}, \emph{nmatch=1}, \emph{bandwidth\_units=False}}{}
Searches through the simulation library and returns the closest
matches to an input set of photometry.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{phot}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors, which must have the
same shape as phot; if this is not None,
then distances will be measured in units of the
photometric error if bandwidth\_units is False, or in
units of the bandwidth added in quadrature with the
errors if it is True

\item[{nmatch}] \leavevmode{[}int{]}
number of matches to return; returned matches will be
ordered by distance from the input

\item[{bandwidth\_units}] \leavevmode{[}bool{]}
if False, distances are computed based on the
logarithmic difference in luminosity; if True, they are
measured in units of the bandwidth

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{matches}] \leavevmode{[}array, shape (..., nmatch, nphys + nfilter){]}
best matches to the input photometry; shape in the
leading dimensions will be the same as for phot, and if
nmatch == 1 then that dimension will be omitted

\item[{dist}] \leavevmode{[}array, shape (..., nmatch){]}
distances between the matches and the input photometry

\end{description}

\end{description}

\end{fulllineitems}

\index{logL() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.logL}\pysiglinewithargsret{\bfcode{logL}}{\emph{physprop}, \emph{photprop}, \emph{photerr=None}}{}
This function returns the natural log of the likelihood
function evaluated at a particular log mass, log age,
extinction, and set of log luminosities
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{physprop}] \leavevmode{[}arraylike, shape (nphys) or (..., nphys){]}
array giving values of the physical properties; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photprop}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{logL}] \leavevmode{[}float or arraylike{]}
natural log of the likelihood function

\end{description}

\end{description}

\end{fulllineitems}

\index{mcmc() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.mcmc}\pysiglinewithargsret{\bfcode{mcmc}}{\emph{photprop}, \emph{photerr=None}, \emph{mc\_walkers=100}, \emph{mc\_steps=500}, \emph{mc\_burn\_in=50}}{}
This function returns a sample of MCMC walkers sampling the
physical parameters at a specified set of photometric values.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{photprop}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\item[{mc\_walkers}] \leavevmode{[}int{]}
number of walkers to use in the MCMC

\item[{mc\_steps}] \leavevmode{[}int{]}
number of steps in the MCMC

\item[{mc\_burn\_in}] \leavevmode{[}int{]}
number of steps to consider ``burn-in'' and discard

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{samples}] \leavevmode{[}array{]}
array of sample points returned by the MCMC

\end{description}

\end{description}

\end{fulllineitems}

\index{mpdf() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.mpdf}\pysiglinewithargsret{\bfcode{mpdf}}{\emph{idx}, \emph{photprop}, \emph{photerr=None}, \emph{ngrid=128}, \emph{qmin=None}, \emph{qmax=None}, \emph{grid=None}, \emph{norm=True}}{}
Returns the marginal probability for one or mode physical
quantities for one or more input sets of photometric
properties. Output quantities are computed on a grid of
values, in the same style as meshgrid
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{idx}] \leavevmode{[}int or listlike containing ints{]}
index of the physical quantity whose PDF is to be
computed; if this is an iterable, the joint distribution of
the indicated quantities is returned

\item[{photprop}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\item[{ngrid}] \leavevmode{[}int or listlike containing ints{]}
number of points in each dimension of the output grid;
if this is an iterable, it must have the same number of
elements as idx

\item[{qmin}] \leavevmode{[}float or listlike{]}
minimum value in the output grid in each quantity; if
left as None, defaults to the minimum value in the
library; if this is an iterable, it must contain the
same number of elements as idx

\item[{qmax}] \leavevmode{[}float or listlike{]}
maximum value in the output grid in each quantity; if
left as None, defaults to the maximum value in the
library; if this is an iterable, it must contain the
same number of elements as idx

\item[{grid}] \leavevmode{[}listlike of arrays{]}
set of values defining the grid on which the PDF is to
be evaluated, in the same format used by meshgrid

\item[{norm}] \leavevmode{[}bool{]}
if True, returned pdf's will be normalized to integrate
to 1

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{grid\_out}] \leavevmode{[}array{]}
array of values at which the PDF is evaluated; contents
are the same as returned by meshgrid

\item[{pdf}] \leavevmode{[}array{]}
array of marginal posterior probabilities at each point
of the output grid, for each input cluster; the leading
dimensions match the leading dimensions produced by
broadcasting the leading dimensions of photprop and
photerr together, while the trailing dimensions match
the dimensions of the output grid

\end{description}

\end{description}

\end{fulllineitems}

\index{priors (slugpy.bayesphot.bp.bp attribute)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.priors}\pysigline{\bfcode{priors}}
The current set of prior probabilities for every
simulation in the library

\end{fulllineitems}


\end{fulllineitems}



\chapter{cluster\_slug: Bayesian Inference of Star Cluster Properties}
\label{cluster_slug:sec-cluster-slug}\label{cluster_slug::doc}\label{cluster_slug:cluster-slug-bayesian-inference-of-star-cluster-properties}
The slugpy.cluster\_slug module computes posterior probabilities for the mass, age, and extinction of star clusters from a set of input photometry.  It is implemented as a wrapper around {\hyperref[bayesphot:sec-bayesphot]{\emph{\DUspan{}{bayesphot: Bayesian Inference for Stochastic Stellar Populations}}}}, so for details on how the calculation is performed see the bayesphot documentation.


\section{Getting the Default Library}
\label{cluster_slug:getting-the-default-library}
The cluster\_slug module requires a pre-computed library of slug simulations to use as a ``training set'' for its calculations. Due to its size, the default library \emph{is not} included in the slug git repository. Instead, it is provided for download from the \href{http://www.slugsps.com/data}{SLUG data products website}. Download the two files \code{clusterslug\_mw\_cluster\_phot.fits} and \code{clusterslug\_mw\_cluster\_prop.fits} and save them in the \code{cluster\_slug} directory of the main respository. If you do not do so, and do not provide your own library when you attempt to use cluster\_slug, you will be prompted to download the default library.


\section{Basic Usage}
\label{cluster_slug:basic-usage}
For an example of how to use cluster\_slug, see the file \code{cluster\_slug/cluster\_slug\_example.py} in the repository. All funtionality is provided through the cluster\_slug class. The basic steps are as follows:
\begin{enumerate}
\item {} 
Import the library and instantiate an \code{sfr\_slug} object (see {\hyperref[cluster_slug:ssec-cluster-slug-full]{\emph{\DUspan{}{Full Documentation of slugpy.cluster\_slug}}}} for full details):

\begin{Verbatim}[commandchars=\\\{\}]
from slugpy.cluster\PYGZus{}slug import cluster\PYGZus{}slug
cs = cluster\PYGZus{}slug(photsystem=photsystem)
\end{Verbatim}

\end{enumerate}

This creates a cluster\_slug object, using the default simulation library, \$SLUG\_DIR/sfr\_slug/clusterslug\_mw. If you have another library of simulations you'd rather use, you can use the \code{libname} keyword to the \code{cluster\_slug} constructor to select it. The optional argument \code{photsystem} specifies the photometric system you will be using for your data. Possible values are \code{L\_nu} (flux per unit frequency, in erg/s/Hz), \code{L\_lambda} (flux per unit wavelength, in erg/s/Angstrom), \code{AB} (AB magnitudes), \code{STMAG} (ST magnitudes), and \code{Vega} (Vega magnitudes). If left unspecified, the photometric system will be whatever the library was written in; the default library is in the \code{L\_nu} system.
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
Specify your filter(s), for example:

\begin{Verbatim}[commandchars=\\\{\}]
cs.add\PYGZus{}filters([\PYGZsq{}WFC3\PYGZus{}UVIS\PYGZus{}F336W\PYGZsq{}, \PYGZsq{}WFC3\PYGZus{}UVIS\PYGZus{}F438W\PYGZsq{}, \PYGZsq{}WFC3\PYGZus{}UVIS\PYGZus{}F555W\PYGZsq{},
                \PYGZsq{}WFC3\PYGZus{}UVIS\PYGZus{}F814W\PYGZsq{}, \PYGZsq{}WFC3\PYGZus{}UVIS\PYGZus{}F657N\PYGZsq{}])
\end{Verbatim}

\end{enumerate}

The \code{add\_filter} method takes as an argument a string or list of strings specifying which filters you're going to point mass SFRs based on. You can have more than one set of filters active at a time (just by calling \code{add\_filters} more than once), and then specify which set of filters you're using for any given calculation.
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Specify your priors, for example:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{} Set priors to be flat in log T and A\PYGZus{}V, but vary with log M as
\PYGZsh{} p(log M) \PYGZti{} 1/M
def priorfunc(physprop):
   \PYGZsh{} Note: physprop is an array of shape (N, 3) where physprop[:,0] =
   \PYGZsh{} log M, physprop[:,1] = log T, physprop[:,2] = A\PYGZus{}V
   return 1.0/exp(physprop[:,0])
cs.priors = prorfunc
\end{Verbatim}

\end{enumerate}

The \code{priors} property specifies the assumed prior probability distribution on the physical properties of star clusters. It can be either \code{None} (in which case all simulations in the library are given equal prior probability), an array with as many elements as there are simulations in the library giving the prior for each one, or a callable that takes a vector of physical properties as input and returns the prior for it.
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Generate a marginal posterior probability distribuiton via:

\begin{Verbatim}[commandchars=\\\{\}]
logm, pdf = cs.mpdf(idx, phot, photerr = photerr)
\end{Verbatim}

\end{enumerate}

The first argument \code{idx} is an index for which posterior distribution should be computed -- a value of 0 generates the posterior in log mass, a value of 1 generates the posterion on log age, and a value of generates the posterior in A\_V. The second argument \code{phot} is an array giving the photometric values in the filters specified in step 2; make sure you're using the same photometric system you used in step 1. For the array \code{phot}, the trailing dimension must match the number of filters, and the marginal posterior-finding exercise is repeated over every value in the leading dimensions. If you have added two or more filter sets, you need to specify which one you want to use via the \code{filters} keyword. The optional argument \code{photerr} can be used to provide errors on the photometric values. The shape rules on it are the same as on \code{phot}, and the two leading dimensions of the two arrays will be broadcast together using normal broadcasting rules.

The \code{cluster\_slug.mpdf} method returns a tuple of two quantities. The first is a grid of values for log M, log T, or A\_V, depending on the value of \code{idx}. The second is the posterior probability distribution at each value of of the grid. Posteriors are normalized to have unit integral. If the input consisted of multiple sets of photometric values, the output will contains marginal posterior probabilities for each input. The output grid will be created automatically be default, but all aspects of it (shape, size, placement of grid points) can be controlled by keywords -- see {\hyperref[cluster_slug:ssec-cluster-slug-full]{\emph{\DUspan{}{Full Documentation of slugpy.cluster\_slug}}}}.


\section{Making Your Own Library}
\label{cluster_slug:making-your-own-library}
You can generate your own library by running slug; you might want to do this, for example, to have a library that works at different metallicity or for a different set of stellar tracks. An example parameter file (the one that was used to generate the default clusterslug\_mw library) is included in the \code{cluster\_slug} directory. This file uses slug's capability to pick the output time and the cluster mass from specified PDFs.

One subtle post-processing step you should take once you've generated your library is to read it in using {\hyperref[slugpy:sec-slugpy]{\emph{\DUspan{}{slugpy -- The Python Helper Library}}}} and then write the photometry back out using the \code{slugpy.write\_cluster\_phot} routine with the format set to \code{fits2}. This uses an alternative FITS format that is faster to search when you want to load only a few filters out of a large library. For large data sets, this can reduce cluster\_slug load times by an order of magnitude. (To be precise: the default format for FITS outputs to put all filters into a single binary table HDU, whilte the \code{fits2} format puts each filter in its own HDU. This puts all data for a single filter into a contiguous block, rather than all the data for a single cluster into a contiguous block, and is therefore faster to load when one wants to load the data filter by filter.)


\section{Full Documentation of slugpy.cluster\_slug}
\label{cluster_slug:ssec-cluster-slug-full}\label{cluster_slug:full-documentation-of-slugpy-cluster-slug}\index{cluster\_slug (class in slugpy.cluster\_slug)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug}\pysiglinewithargsret{\strong{class }\code{slugpy.cluster\_slug.}\bfcode{cluster\_slug}}{\emph{libname=None}, \emph{filters=None}, \emph{photsystem=None}, \emph{bw\_phys=0.1}, \emph{bw\_phot=None}, \emph{ktype='gaussian'}, \emph{priors=None}, \emph{sample\_density=None}, \emph{reltol=0.01}, \emph{abstol=1e-08}, \emph{leafsize=16}, \emph{use\_nebular=True}}{}
A class that can be used to estimate the PDF of star cluster
properties (mass, age, extinction) from a set of input photometry
in various bands.
\begin{description}
\item[{Properties}] \leavevmode\begin{description}
\item[{priors}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} None{]}
prior probability on each data point; interpretation
depends on the type passed; array, shape (N): values are
interpreted as the prior probability of each data point;
callable: the callable must take as an argument an array
of shape (N, nphys), and return an array of shape (N)
giving the prior probability at each data point; None:
all data points have equal prior probability

\item[{abstol}] \leavevmode{[}float{]}
absolute error tolerance for kernel density estimation

\item[{reltol}] \leavevmode{[}float{]}
relative error tolerance for kernel density estimation

\end{description}

\item[{Methods}] \leavevmode\begin{description}
\item[{filters()}] \leavevmode{[}{]}
returns list of filters available in the library

\item[{filter\_units() :}] \leavevmode
returns units for available filters

\item[{add\_filters()}] \leavevmode{[}{]}
adds a set of filters for use in parameter estimation

\item[{logL()}] \leavevmode{[}{]}
compute log likelihood at a particular set of physical and
photometric parameters

\item[{mpdf()}] \leavevmode{[}{]}
computer marginal posterior probability distribution for a
set of photometric measurements

\item[{mcmc() :}] \leavevmode
due MCMC estimation of the posterior PDF on a set of
photometric measurments

\item[{bestmatch()}] \leavevmode{[}{]}
find the simulations in the library that are the closest
matches to the input photometry

\end{description}

\end{description}
\index{\_\_init\_\_() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{libname=None}, \emph{filters=None}, \emph{photsystem=None}, \emph{bw\_phys=0.1}, \emph{bw\_phot=None}, \emph{ktype='gaussian'}, \emph{priors=None}, \emph{sample\_density=None}, \emph{reltol=0.01}, \emph{abstol=1e-08}, \emph{leafsize=16}, \emph{use\_nebular=True}}{}
Initialize a cluster\_slug object.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{libname}] \leavevmode{[}string{]}
name of the SLUG model to load; if left as None, the default
is \$SLUG\_DIR/cluster\_slug/modp020\_chabrier\_MW

\item[{filters}] \leavevmode{[}iterable of stringlike{]}
list of filter names to be used for inferenence

\item[{photsystem}] \leavevmode{[}None or string{]}
If photsystem is None, the library will be left in
whatever photometric system was used to write
it. Alternately, if it is a string, the data will be
converted to the specified photometric system. Allowable
values are `L\_nu', `L\_lambda', `AB', `STMAG', and
`Vega', corresponding to the options defined in the SLUG
code. Once this is set, any subsequent photometric data
input are assumed to be in the same photometric system.

\item[{bw\_phys}] \leavevmode{[}`auto' \textbar{} float \textbar{} array, shape (3){]}
bandwidth for the physical quantities in the kernel
density estimation; if set to `auto', the bandwidth will
be estimated automatically; if set to a scalar quantity,
this will be used for all physical quantities

\item[{bw\_phot}] \leavevmode{[}None \textbar{} `auto' \textbar{} float \textbar{} array{]}
bandwidth for the photometric quantities; if set to
None, defaults to 0.25 mag / 0.1 dex; if set to `auto',
bandwidth is estimated automatically; if set to a float,
this bandwidth is used for all photometric dimensions;
if set to an array, the array must have the same number
of dimensions as len(filters)

\item[{ktype}] \leavevmode{[}string{]}
type of kernel to be used in densty estimation; allowed
values are `gaussian' (default), `epanechnikov', and
`tophat'; only Gaussian can be used with error bars

\item[{priors}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} None{]}
prior probability on each data point; interpretation
depends on the type passed; array, shape (N): values are
interpreted as the prior probability of each data point;
callable: the callable must take as an argument an array
of shape (N, nphys), and return an array of shape (N)
giving the prior probability at each data point; None:
all data points have equal prior probability

\item[{sample\_density}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} `auto' \textbar{} None{]}
the density of the data samples at each data point; this
need not match the prior density; interpretation depends
on the type passed; array, shape (N): values are
interpreted as the density of data sampling at each
sample point; callable: the callable must take as an
argument an array of shape (N, nphys), and return an
array of shape (N) giving the sampling density at each
point; `auto': the sample density will be computed
directly from the data set; note that this can be quite
slow for large data sets, so it is preferable to specify
this analytically if it is known; None: data are assumed
to be uniformly sampled, or to be sampled as the default
library is if libname is also None

\item[{reltol}] \leavevmode{[}float{]}
relative error tolerance; errors on all returned
probabilities p will satisfy either
abs(p\_est - p\_true) \textless{}= reltol * p\_est   OR
abs(p\_est - p\_true) \textless{}= abstol,
where p\_est is the returned estimate and p\_true is the
true value

\item[{abstol}] \leavevmode{[}float{]}
absolute error tolerance; see above

\item[{leafsize}] \leavevmode{[}int{]}
number of data points in each leaf of the KD tree

\item[{use\_nebular}] \leavevmode{[}bool{]}
if True, photometry including nebular emission will be
used if available; if not, nebular emission will be
omitted

\end{description}

\item[{Returns}] \leavevmode
Nothing

\item[{Raises}] \leavevmode
IOError, if the library cannot be found

\end{description}

\end{fulllineitems}

\index{\_\_weakref\_\_ (slugpy.cluster\_slug.cluster\_slug attribute)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.__weakref__}\pysigline{\bfcode{\_\_weakref\_\_}}
list of weak references to the object (if defined)

\end{fulllineitems}

\index{add\_filters() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.add_filters}\pysiglinewithargsret{\bfcode{add\_filters}}{\emph{filters}, \emph{bandwidth=None}}{}
Add a set of filters to use for cluster property estimation
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{filters}] \leavevmode{[}iterable of stringlike{]}
list of filter names to be used for inferenence

\item[{bandwidth}] \leavevmode{[}None \textbar{} `auto' \textbar{} float \textbar{} array{]}
bandwidth for the photometric quantities; if set to
None, defaults to 0.3 mag / 0.12 dex; if set to `auto',
bandwidth is estimated automatically; if set to a float,
this bandwidth is used for all physical photometric
dimensions; if set to an array, the array must have the
same number of entries as 3+len(filters)

\end{description}

\item[{Returns}] \leavevmode
nothing

\end{description}

\end{fulllineitems}

\index{bestmatch() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.bestmatch}\pysiglinewithargsret{\bfcode{bestmatch}}{\emph{phot}, \emph{photerr=None}, \emph{nmatch=1}, \emph{bandwidth\_units=False}, \emph{filters=None}}{}
Searches through the simulation library and returns the closest
matches to an input set of photometry.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{phot}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{nmatch}] \leavevmode{[}int{]}
number of matches to return; returned matches will be
ordered by distance from the input

\item[{bandwidth\_units}] \leavevmode{[}bool{]}
if False, distances are computed based on the
logarithmic difference in luminosity; if True, they are
measured in units of the bandwidth

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{matches}] \leavevmode{[}array, shape (..., nmatch, 3 + nfilter){]}
best matches to the input photometry; shape in the
leading dimensions will be the same as for phot, and if
nmatch == 1 then that dimension will be omitted; in the
final dimension, the first 3 elements give log M, log T,
and A\_V, while the last nfilter give the photometric
values

\item[{dist}] \leavevmode{[}array, shape (..., nmatch){]}
distances between the matches and the input photometry

\end{description}

\end{description}

\end{fulllineitems}

\index{filter\_units() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.filter_units}\pysiglinewithargsret{\bfcode{filter\_units}}{}{}
Returns list of all available filter units
\begin{description}
\item[{Parameters:}] \leavevmode
None

\item[{Returns:}] \leavevmode\begin{description}
\item[{units}] \leavevmode{[}list of strings{]}
list of available filter units

\end{description}

\end{description}

\end{fulllineitems}

\index{filters() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.filters}\pysiglinewithargsret{\bfcode{filters}}{}{}
Returns list of all available filters
\begin{description}
\item[{Parameters:}] \leavevmode
None

\item[{Returns:}] \leavevmode\begin{description}
\item[{filters}] \leavevmode{[}list of strings{]}
list of available filter names

\end{description}

\end{description}

\end{fulllineitems}

\index{load\_data() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.load_data}\pysiglinewithargsret{\bfcode{load\_data}}{\emph{filter\_name}}{}
Loads photometric data for the specified filter into memory
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{filter\_name}] \leavevmode{[}string{]}
name of filter to load

\end{description}

\item[{Returns:}] \leavevmode
None

\item[{Raises:}] \leavevmode
ValueError, if filter\_name is not one of the available
filters

\end{description}

\end{fulllineitems}

\index{logL() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.logL}\pysiglinewithargsret{\bfcode{logL}}{\emph{physprop}, \emph{photprop}, \emph{photerr=None}, \emph{filters=None}}{}
This function returns the natural log of the likelihood
function evaluated at a particular log mass, log age,
extinction, and set of log luminosities
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{physprop}] \leavevmode{[}arraylike, shape (3) or (..., 3){]}
array giving values of the log M, log T, and A\_V; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photprop}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{logL}] \leavevmode{[}float or arraylike{]}
natural log of the likelihood function

\end{description}

\end{description}

\end{fulllineitems}

\index{mcmc() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.mcmc}\pysiglinewithargsret{\bfcode{mcmc}}{\emph{photprop}, \emph{photerr=None}, \emph{mc\_walkers=100}, \emph{mc\_steps=500}, \emph{mc\_burn\_in=50}, \emph{filters=None}}{}
This function returns a sample of MCMC walkers for cluster
mass, age, and extinction
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{photprop}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\item[{mc\_walkers}] \leavevmode{[}int{]}
number of walkers to use in the MCMC

\item[{mc\_steps}] \leavevmode{[}int{]}
number of steps in the MCMC

\item[{mc\_burn\_in}] \leavevmode{[}int{]}
number of steps to consider ``burn-in'' and discard

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{samples}] \leavevmode{[}array{]}
array of sample points returned by the MCMC

\end{description}

\end{description}

\end{fulllineitems}

\index{mpdf() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.mpdf}\pysiglinewithargsret{\bfcode{mpdf}}{\emph{idx}, \emph{photprop}, \emph{photerr=None}, \emph{ngrid=128}, \emph{qmin=None}, \emph{qmax=None}, \emph{grid=None}, \emph{norm=True}, \emph{filters=None}}{}
Returns the marginal probability for one or mode physical
quantities for one or more input sets of photometric
properties. Output quantities are computed on a grid of
values, in the same style as meshgrid
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{idx}] \leavevmode{[}int or listlike containing ints{]}
index of the physical quantity whose PDF is to be
computed; 0 = log M, 1 = log T, 2 = A\_V; if this is an
iterable, the joint distribution of the indicated
quantities is returned

\item[{photprop}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\item[{ngrid}] \leavevmode{[}int or listlike containing ints{]}
number of points in each dimension of the output grid;
if this is an iterable, it must have the same number of
elements as idx

\item[{qmin}] \leavevmode{[}float or listlike{]}
minimum value in the output grid in each quantity; if
left as None, defaults to the minimum value in the
library; if this is an iterable, it must contain the
same number of elements as idx

\item[{qmax}] \leavevmode{[}float or listlike{]}
maximum value in the output grid in each quantity; if
left as None, defaults to the maximum value in the
library; if this is an iterable, it must contain the
same number of elements as idx

\item[{grid}] \leavevmode{[}listlike of arrays{]}
set of values defining the grid on which the PDF is to
be evaluated, in the same format used by meshgrid

\item[{norm}] \leavevmode{[}bool{]}
if True, returned pdf's will be normalized to integrate
to 1

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{grid\_out}] \leavevmode{[}array{]}
array of values at which the PDF is evaluated; contents
are the same as returned by meshgrid

\item[{pdf}] \leavevmode{[}array{]}
array of marginal posterior probabilities at each point
of the output grid, for each input cluster; the leading
dimensions match the leading dimensions produced by
broadcasting the leading dimensions of photprop and
photerr together, while the trailing dimensions match
the dimensions of the output grid

\end{description}

\end{description}

\end{fulllineitems}


\end{fulllineitems}



\chapter{sfr\_slug: Bayesian Inference of Star Formation Rates}
\label{sfr_slug:sfr-slug-bayesian-inference-of-star-formation-rates}\label{sfr_slug::doc}\label{sfr_slug:sec-sfr-slug}
The slugy.sfr\_slug module computes posterior probabilities on star formation rates given a set of star formation rates estimated using the ``point mass estimate'' (i.e., the estimate you would get for a fully sampled stellar population) for the SFR based on the ionizing, FUV, or bolometric luminosity. It is implemented as a wrapper around {\hyperref[bayesphot:sec-bayesphot]{\emph{\DUspan{}{bayesphot: Bayesian Inference for Stochastic Stellar Populations}}}}, so for details on how the calculation is performed see the bayesphot documentation.


\section{Getting the Default Library}
\label{sfr_slug:getting-the-default-library}
The sfr\_slug module requires a pre-computed library of slug simulations to use as a ``training set'' for its calculations. Due to its size, the default library \emph{is not} included in the slug git repository. Instead, it is provided for download from the \href{http://www.slugsps.com/data}{SLUG data products website}. Download the two files \code{SFR\_SLUG\_integrated\_phot.fits} and \code{SFR\_SLUG\_integrated\_prop.fits} and save them in the \code{sfr\_slug} directory of the main respository. If you do not do so, and do not provide your own library when you attempt to use sfr\_slug, you will be prompted to download the default library.


\section{Basic Usage}
\label{sfr_slug:basic-usage}
The \code{sfr\_slug/sfr\_slug\_example.py} file in the repository provides an example of how to use sfr\_slug. Usage of is simple, as the functionality is all implemented through a single class, sfr\_slug. The required steps are as follows:
\begin{enumerate}
\item {} 
Import the library and instantiate an \code{sfr\_slug} object (see {\hyperref[sfr_slug:sec-sfr-slug-full]{\emph{\DUspan{}{Full Documentation of slugpy.sfr\_slug}}}} for full details):

\begin{Verbatim}[commandchars=\\\{\}]
from slugpy.sfr\PYGZus{}slug import sfr\PYGZus{}slug
sfr\PYGZus{}estimator = sfr\PYGZus{}slug()
\end{Verbatim}

\end{enumerate}

This creates an sfr\_slug object, using the default simulation library, \$SLUG\_DIR/sfr\_slug/SFR\_SLUG. If you have another library of simulations you'd rather use, you can use the \code{libname} keyword to the \code{sfr\_slug} constructor to select it.
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
Specify your filter(s), for example:

\begin{Verbatim}[commandchars=\\\{\}]
sfr\PYGZus{}estimator.add\PYGZus{}filters(\PYGZsq{}QH0\PYGZsq{})
\end{Verbatim}

\end{enumerate}

The \code{add\_filter} method takes as an argument a string or list of strings specifying which filters you're going to point mass SFRs based on. You can have more than one set of filters active at a time (just by calling \code{add\_filters} more than once), and then specify which set of filters you're using for any given calculation.
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Specify your priors, for example:

\begin{Verbatim}[commandchars=\\\{\}]
sfr\PYGZus{}estimator.priors = \PYGZsq{}schechter\PYGZsq{}
\end{Verbatim}

\end{enumerate}

The \code{priors} property specifies the assumed prior probability distribution on the star formation rate. It can be either \code{None} (in which case all simulations in the library are given equal prior probability), an array with as many elements as there are simulations in the library giving the prior for each one, a callable that takes a star formation rate as input and returns the prior for it, or a string whose value is either ``flat'' or ``prior''. The two strings specify, respectively, a prior distribution that is either flat in log SFR or follows the Schechter function SFR distribution from \href{http://adsabs.harvard.edu/abs/2011MNRAS.415.1815B}{Bothwell et al. (2011)}:
\begin{gather}
\begin{split}p(\log\mathrm{SFR}) \propto \mathrm{SFR}^{\alpha} \exp(-\mathrm{SFR}/\mathrm{SFR}_*)\end{split}\notag
\end{gather}
with \(\alpha = -0.51\) and \(\mathrm{SFR}_* = 9.2\,M_\odot\,\mathrm{yr}^{-1}\).
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Generate the posterior probability distribuiton of SFR via:

\begin{Verbatim}[commandchars=\\\{\}]
logSFR, pdf = sfr\PYGZus{}estimator.mpdf(logSFR\PYGZus{}in, logSFRphoterr = logSFR\PYGZus{}err)
\end{Verbatim}

\end{enumerate}

The argument \code{logSFR\_in} can be a float or an array specifying one or more point mass estimates of the SFR in your chosen filter. For a case with two or more filters, then \code{logSFR\_in} must be an array whose trailing dimension matches the number of filters. If you have added two or more filter sets, you need to specify which one you want to use via the \code{filters} keyword. The optional argument \code{logSFRphoterr} can be used to provide errors on the photometric SFRs. Like \code{logSFR\_in}, it can be a float or an array.

The \code{sfr\_slug.mpdf} method returns a tuple of two quantities. The first is a grid of log SFR values, and the second is the posterior probability distribution at each value of log SFR. If the input consisted of multiple photometric SFRs, the output will contains posterior probabilities for each input. The output grid will be created automatically be default, but all aspects of it (shape, size, placement of grid points) can be controlled by keywords -- see {\hyperref[sfr_slug:sec-sfr-slug-full]{\emph{\DUspan{}{Full Documentation of slugpy.sfr\_slug}}}}.


\section{Full Documentation of slugpy.sfr\_slug}
\label{sfr_slug:sec-sfr-slug-full}\label{sfr_slug:full-documentation-of-slugpy-sfr-slug}\index{sfr\_slug (class in slugpy.sfr\_slug)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug}\pysiglinewithargsret{\strong{class }\code{slugpy.sfr\_slug.}\bfcode{sfr\_slug}}{\emph{libname=None}, \emph{detname=None}, \emph{filters=None}, \emph{bandwidth=0.1}, \emph{ktype='gaussian'}, \emph{priors=None}, \emph{sample\_density='read'}, \emph{reltol=0.001}, \emph{abstol=1e-10}, \emph{leafsize=16}}{}
A class that can be used to estimate the PDF of true star
formation rate from a set of input point mass estimates of the
star formation rate.
\begin{description}
\item[{Properties}] \leavevmode\begin{description}
\item[{priors}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} `flat' \textbar{} `schechter' \textbar{} None{]}
prior probability on each data point; interpretation
depends on the type passed; array, shape (N): values are
interpreted as the prior probability of each data point;
callable: the callable must take as an argument an array
of shape (N, nphys), and return an array of shape (N)
giving the prior probability at each data point; None:
all data points have equal prior probability; the values
`flat' and `schechter' use priors p(log SFR) \textasciitilde{} constant and
p(log SFR) \textasciitilde{} SFR\textasciicircum{}alpha exp(-SFR/SFR\_*), respectively, where
alpha = -0.51 and SFR\_* = 9.2 Msun/yr are the values
measured by Bothwell et al. (2011)

\item[{bandwidth}] \leavevmode{[}`auto' \textbar{} array, shape (M){]}
bandwidth for kernel density estimation; if set to
`auto', the bandwidth will be estimated automatically

\end{description}

\end{description}
\index{\_\_init\_\_() (slugpy.sfr\_slug.sfr\_slug method)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{libname=None}, \emph{detname=None}, \emph{filters=None}, \emph{bandwidth=0.1}, \emph{ktype='gaussian'}, \emph{priors=None}, \emph{sample\_density='read'}, \emph{reltol=0.001}, \emph{abstol=1e-10}, \emph{leafsize=16}}{}~\begin{quote}

Initialize an sfr\_slug object.
\end{quote}
\begin{description}
\item[{Parameters}] \leavevmode\begin{quote}
\begin{description}
\item[{libname}] \leavevmode{[}string{]}
name of the SLUG model to load; if left as None, the default
is \$SLUG\_DIR/sfr\_slug/SFR\_SLUG

\item[{detname}] \leavevmode{[}string{]}
name of a SLUG model run with the same parameters but no
stochasticity; used to establish the non-stochastic
photometry to SFR conversions; if left as None, the default
is libname\_DET

\item[{filters}] \leavevmode{[}iterable of stringlike{]}
list of filter names to be used for inferenence

\item[{bandwidth}] \leavevmode{[}`auto' \textbar{} float \textbar{} array, shape (M){]}
bandwidth for kernel density estimation; if set to
`auto', the bandwidth will be estimated automatically;
if set to a float, the same bandwidth is used in all
dimensions

\item[{ktype}] \leavevmode{[}string{]}
type of kernel to be used in densty estimation; allowed
values are `gaussian' (default), `epanechnikov', and
`tophat'; only Gaussian can be used with error bars

\item[{priors}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} None{]}
prior probability on each data point; interpretation
depends on the type passed; array, shape (N): values are
interpreted as the prior probability of each data point;
callable: the callable must take as an argument an array
of shape (N, nphys), and return an array of shape (N)
giving the prior probability at each data point; None:
all data points have equal prior probability

\item[{sample\_density}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} `auto' \textbar{} `read' \textbar{} None{]}
the density of the data samples at each data point; this
need not match the prior density; interpretation depends
on the type passed; array, shape (N): values are
interpreted as the density of data sampling at each
sample point; callable: the callable must take as an
argument an array of shape (N, nphys), and return an
array of shape (N) giving the sampling density at each
point; `auto': the sample density will be computed
directly from the data set; note that this can be quite
slow for large data sets, so it is preferable to specify
this analytically if it is known; `read': the sample
density is to be read from a numpy save file whose name
matches that of the library, with the extension \_density.npy
added; None: data are assumed to be uniformly sampled

\item[{reltol}] \leavevmode{[}float{]}
relative error tolerance; errors on all returned
probabilities p will satisfy either
abs(p\_est - p\_true) \textless{}= reltol * p\_est   OR
abs(p\_est - p\_true) \textless{}= abstol,
where p\_est is the returned estimate and p\_true is the
true value

\item[{abstol}] \leavevmode{[}float{]}
absolute error tolerance; see above

\item[{leafsize}] \leavevmode{[}int{]}
number of data points in each leaf of the KD tree

\end{description}
\end{quote}
\begin{description}
\item[{Returns}] \leavevmode
Nothing

\item[{Raises}] \leavevmode
IOError, if the library cannot be found

\end{description}

\end{description}

\end{fulllineitems}

\index{\_\_weakref\_\_ (slugpy.sfr\_slug.sfr\_slug attribute)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug.__weakref__}\pysigline{\bfcode{\_\_weakref\_\_}}
list of weak references to the object (if defined)

\end{fulllineitems}

\index{add\_filters() (slugpy.sfr\_slug.sfr\_slug method)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug.add_filters}\pysiglinewithargsret{\bfcode{add\_filters}}{\emph{filters}}{}
Add a set of filters to use for cluster property estimation
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{filters}] \leavevmode{[}iterable of stringlike{]}
list of filter names to be used for inferenence

\end{description}

\item[{Returns}] \leavevmode
nothing

\end{description}

\end{fulllineitems}

\index{filters() (slugpy.sfr\_slug.sfr\_slug method)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug.filters}\pysiglinewithargsret{\bfcode{filters}}{}{}
Returns list of all available filters
\begin{description}
\item[{Parameters:}] \leavevmode
None

\item[{Returns:}] \leavevmode\begin{description}
\item[{filters}] \leavevmode{[}list of strings{]}
list of available filter names

\end{description}

\end{description}

\end{fulllineitems}

\index{logL() (slugpy.sfr\_slug.sfr\_slug method)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug.logL}\pysiglinewithargsret{\bfcode{logL}}{\emph{logSFR}, \emph{logSFRphot}, \emph{logSFRphoterr=None}, \emph{filters=None}}{}
This function returns the natural log of the likelihood
function evaluated at a particular log SFR and set of log
luminosities
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{logSFR}] \leavevmode{[}float or arraylike{]}
float or array giving values of the log SFR; for an
array, the operation is vectorized

\item[{logSFRphot}] \leavevmode{[}float or arraylike, shape (nfilter) or (..., nfilter){]}
float or array giving the SFR inferred from photometry using a
deterministic conversion; for an array, the operation is
vectorized over the leading dimensions

\item[{logSFRphoterr}] \leavevmode{[}float arraylike, shape (nfilter) or (..., nfilter){]}
float or array giving photometric SFR errors; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters used for the SFR estimation;
if left as None, and only 1 set of photometric filters
has been defined for the sfr\_slug object, that set will
be used by default

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{logL}] \leavevmode{[}float or arraylike{]}
natural log of the likelihood function

\end{description}

\end{description}

\end{fulllineitems}

\index{mcmc() (slugpy.sfr\_slug.sfr\_slug method)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug.mcmc}\pysiglinewithargsret{\bfcode{mcmc}}{\emph{photprop}, \emph{photerr=None}, \emph{mc\_walkers=100}, \emph{mc\_steps=500}, \emph{mc\_burn\_in=50}, \emph{filters=None}}{}
This function returns a sample of MCMC walkers for log SFR
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{photprop}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\item[{mc\_walkers}] \leavevmode{[}int{]}
number of walkers to use in the MCMC

\item[{mc\_steps}] \leavevmode{[}int{]}
number of steps in the MCMC

\item[{mc\_burn\_in}] \leavevmode{[}int{]}
number of steps to consider ``burn-in'' and discard

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{samples}] \leavevmode{[}array{]}
array of sample points returned by the MCMC

\end{description}

\end{description}

\end{fulllineitems}

\index{mpdf() (slugpy.sfr\_slug.sfr\_slug method)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug.mpdf}\pysiglinewithargsret{\bfcode{mpdf}}{\emph{logSFRphot}, \emph{logSFRphoterr=None}, \emph{ngrid=128}, \emph{qmin=None}, \emph{qmax=None}, \emph{grid=None}, \emph{norm=True}, \emph{filters=None}}{}
Returns the marginal probability of log SFR for one or more
input sets of photometric properties. Output quantities are
computed on a grid of values, in the same style as meshgrid
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{logSFRphot}] \leavevmode{[}float or arraylike{]}
float or array giving the log SFR inferred from
photometry using a deterministic conversion; if the
argument is an array, the operation is vectorized over
it

\item[{logSFRphoterr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\item[{ngrid}] \leavevmode{[}int{]}
number of points in the output log SFR grid

\item[{qmin}] \leavevmode{[}float{]}
minimum value in the output log SFR grid

\item[{qmax}] \leavevmode{[}float{]}
maximum value in the output log SFR grid

\item[{grid}] \leavevmode{[}array{]}
set of values defining the grid of SFR values at which
to evaluate; if set, overrides ngrid, qmin, and qmax

\item[{norm}] \leavevmode{[}bool{]}
if True, returned pdf's will be normalized to integrate
to 1

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{grid\_out}] \leavevmode{[}array{]}
array of log SFR values at which the PDF is evaluated

\item[{pdf}] \leavevmode{[}array{]}
array of marginal posterior probabilities at each point
of the output grid, for each input photometric value;
the leading dimensions match the leading dimensions
produced by broadcasting the leading dimensions of
photprop and photerr together, while the trailing
dimensions match the dimensions of the output grid

\end{description}

\end{description}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Test Problems}
\label{tests:test-problems}\label{tests::doc}\label{tests:sec-tests}
This section describes a set of problems that can be used to test and explore the different capabilities of SLUG. SLUG ships a
set of problems \code{problemname} that are specified by a parameter file \code{param/problemname.param}. Problems that require
multiple simulations are described instead by multiple paramater files, each with unique ID XX:  \code{param/problemnameXX.param}.
Users can reproduce the output of the test problems with the provided executable scripts  \code{test/run\_problemname.sh}.
For each problem, a script for analysis is distributed  in \code{test/problemname.py}. Details for each test problem are given below. Throughout this section, it is assumed that the \code{SLUG\_DIR} has been properly set.
These test problems are designed to work with outputs in FITS format, but that can be easily changed in the
\code{.param} files. To run all the problems and the analysis scripts in one go, the user can simply
run \code{test/run\_alltest.sh}. It will take around 15 minutes
for the script to complete on a standard laptop. About 700MB of data are generated.
If SLUG is correctly installed and working, the first part of the script (i.e. the SLUG
simulations) should run flawlessly. The second part of the script relies instead on external python procedures,
including slugpy, numpy, and matplotlib. While these packages are fairly standard, the user needs to ensure that
they are properly installed and visible to the script. This script has been written for and tested with Python 2.7.


\section{Problem \texttt{example\_galaxy}: basic galaxy simulation}
\label{tests:problem-example-galaxy-basic-galaxy-simulation}
This problem illustrates the basic usage of slugin \code{galaxy} mode by running 48 realizations of a galaxy with constant
\(\mathrm{SFR}=0.001\; M_\odot\;\mathrm{yr}^{-1}\), up to a maximum time of \(2\times 10^8\) yr. By issuing the
command \code{test/run\_example\_galaxy.sh} the output files \code{SLUG\_GALAXY\_EXAMPLE*} are generated. Once the models are ready,
\code{python test/plot\_example\_galaxy.py} produces a multi-panel figure \code{test/SLUG\_GALAXY\_EXAMPLE\_f1.pdf}.

The top-left panel shows the actual mass produced by SLUG for each of the 48 models at different time steps as a
function of the targeted mass. One can see that SLUG realizations only approximate the desired mass, which is a consequence
of SLUG core algorithm. The 1:1 relation is shown by a red dashed line.
The remaining panels show examples of integrated photometry (as labeled) of all simulated galaxies
at different time steps, as a function of the actual mass. Due to its stochastic nature, SLUG produces
distributions rather than single values for each time step. The expected rate of ionizing
photon and the bolometric luminosities for a deterministic model with a
continuous star formation rate of \(\mathrm{SFR}=0.001\; M_\odot\;\mathrm{yr}^{-1}\) are shown
by red dashed lines in the relevant panels.


\section{Problem \texttt{example\_cluster}: basic cluster simulation}
\label{tests:problem-example-cluster-basic-cluster-simulation}
This problem illustrates the basic usage of SLUG in \code{cluster} mode by running 1000 realizations of a cluster
with mass 500 \(M_\odot\), up to a maximum time of 10 Myr. By issuing the command
\code{test/run\_example\_cluster.sh} the output files \code{SLUG\_CLUSTER\_EXAMPLE*} are
generated. Once the models are ready, \code{python test/plot\_example\_cluster.py} produces a multi-panel
figure \code{test/SLUG\_CLUSTER\_EXAMPLE\_f1.pdf}.

This figure is divided in two columns: the left one shows outputs at the first time step, 1 Myr, while
the second one shows outputs at the last time step, 10 Myr.  The top row shows the actual cluster mass for an
input mass of \(500\;M_\odot\).
In \code{cluster} mode, all clusters are generated at the first time step and they evolve
passively after that. Thus, the mass does not change. As a consequence of the
random drawing from the IMF, masses are distributed around the input mass.
As the wanted mass is large enough to allow for many stars to be drawn, the
actual mass distribution is narrow.

The second row shows instead the distribution of the maximum mass of all stars that are still
alive at a given time step. At 1 Myr, this distribution is a good approximation of the
input distribution, which is the result of random draws from the IMF. At 10 Myr, which is the
typical lifetime of a 15-20 \(M_\odot\) star, the most massive stars have died, and
SLUG stops following them. The distribution of luminosities, and particularly those
most sensitive to the presence of massive stars, change accordingly
(third and fourth row for \(Q_{H_0}\) and FUV).


\section{Problem \texttt{constsampl}: importance of constrained sampling}
\label{tests:probsampl-label}\label{tests:problem-constsampl-importance-of-constrained-sampling}
This problem illustrates in more detail the effects of constrained sampling on SLUG simulations.
This is the first key ingredient in the core algorithm of SLUG. With the command \code{test/run\_constsampl.sh},
three different \code{cluster} simulations are run, each with 1000 trials, but with masses of \(50\;M_\odot\),
\(250\;M_\odot\), and \(500\;M_\odot\). A single timestep of \(10^6\) yr is generated.
The analysis script \code{python test/plot\_constsampl.py} produces a multi-panel
figure \code{test/SLUG\_CONSTSAMPL\_f1.pdf}.

This figure shows the maximum mass of the stars in these realizations (top row), the
rate of ionizing photons \(Q_{H_0}\) (central row), and the FUV luminosity (bottom row).
Histograms refer, form left to right, to clusters with \(50\;M_\odot\), \(250\;M_\odot\),
and \(500\;M_\odot\).

Due to the small timestep, the distributions of stellar masses shown in the top panels reflect
to good approximation the distribution of the maximum stellar masses that are drawn from the IMF by
SLUG in each realization. For a cluster of \(50\;M_\odot\), the vast majority of the
stars are drawn below  \(20-50\;M_\odot\). This is an obvious consequence of the
fact that a cluster cannot contain stars much more massive than its own mass. However, stars
more massive then the targeted mass are not impossible realizations for the default
sampling algorithm (see below). For instance, if the first star to be drawn has
mass \(60\;M_\odot\), then SLUG would add it to the cluster and stop. Leaving this star out
would indeed be a worse approximation than overshooting the targeted cluster mass by only
\(10\;M_\odot\).  From left to right, one can see that, as the targeted cluster mass increases, the
histogram shifts to progressively higher masses. In the limit of an infinite cluster,
all stellar masses would be represented, and the histogram would peak at \(120\;M_\odot\).
Essentially, this constrained sampling introduces a stochastic (and not deterministic)
variation in the IMF. An IMF truncated above \(60\;M_\odot\) would roughly
approximate the results of the left column; however, a deterministic cut-off
would not correctly reproduce the non-zero tail at higher masses, thus artificially
reducing the scatter introduced by random sampling.

The second and third row simply reflect what said above: for large clusters that can host
stars at all masses, the luminosity peaks around what is expected according to a deterministic
stellar population synthesis codes. At lower cluster masses, ionizing and UV fluxes
are instead suppresses, due to the lack of massive stars. However, tails to high values exist
in all cases.


\section{Problem \texttt{sampling}: different sampling techniques}
\label{tests:problem-sampling-different-sampling-techniques}
As highlighted in the previous section, the method with which stars are sampled from the
IMF has a great influence on the final output. Starting from v2, SLUG has the capability of
specifying the desired sampling algorithm for a given PDF.
The command  \code{test/run\_sampling.sh} runs four \code{cluster} simulations, each with 1000 trials
of masses of \(50\;M_\odot\), and a Kroupa (2002) IMF.
The following four sampling methods are chosen for each simulation: 1) \code{stop\_nearest},
which is the default in SLUG; 2) \code{stop\_before}; 3) \code{stop\_after}; 4) \code{sorted\_sampling}.
A description of each method is provided in Section {\hyperref[pdfs:sampling-metod-label]{\emph{\DUspan{}{Sampling Methods}}}}.
The analysis script \code{python test/plot\_sampling.py} produces a multi-panel
figure \code{test/SLUG\_SAMPLING\_f1.pdf}.

By comparing the panels in each column, one can understand the fundamental differences
induced by the sampling technique. The top row shows the maximum stellar mass drawn from the
IMF in each realization. The targeted cluster mass is also shown with red vertical lines.
In the default mode, SLUG is allowed to overshoot the targeted mass if that constitutes
a good approximation for the total cluster mass. Thus, a tail at stellar masses above the
targeted cluster mass is visible. This tail is accentuated when the stop after method
is selected (third column). In this case, SLUG always overshoots the cluster mass, and thus
extreme realizations above \(100\;M_\odot\)  are possible. Conversely, in the
stop after method (second column), SLUG always under-fills the clusters, and (in this case)
the cluster mass becomes a limit to the maximum stellar mass that can be drawn. A similar effect
is seen when sorted sampling is enable (fourth column). However, the correspondence between the
cluster mass and the maximum stellar mass is not trivially established, as it depends on the
shape of the IMF. The second and third row show how the sampling techniques affect the output
photometry.


\section{Problem \texttt{imfchoice}: different IMF implementations}
\label{tests:problem-imfchoice-different-imf-implementations}\label{tests:probimf-label}
This problem highlights how SLUG can handle different IMF implementations by running
three simulations with a Kroupa, a Salpeter, and a Chabrier IMF. However, SLUG is not
restricted to these choices, as the user can in fact easily input an arbitrary IMF.
The command  \code{test/run\_imfchoice.sh} runs three \code{cluster} simulations, each with 1000 trials
of masses of \(500\;M_\odot\) and different IMF. The analysis script
\code{python test/plot\_imfchoice.py} produces a multi-panel figure \code{test/SLUG\_IMFCHOICE\_f1.pdf}.
Each column shows different statistics for the three IMF. From top to bottom, these are:
the maximum stellar mass in a cluster, the number of stars that SLUG treats stochastically,
and the distributions of \(Q_{H_0}\)  and bolometric luminosities.
As expected for a steep lower-end of the IMF, in the Salpeter case SLUG prefers to fill the
clusters with a higher number of low mass stars.


\section{Problem \texttt{clfraction}: cluster fraction at work}
\label{tests:problem-clfraction-cluster-fraction-at-work}
With the exception of the first example, these test problems have focused on how SLUG handles
cluster simulations, and how these clusters are filled with stars drawn from the IMF.
This new problem highlights instead the presence of additional stochasticity induced by a
second level in the hierarchy of \code{galaxy} simulations: how clusters are drawn from the CMF to satisfy the
targeted galaxy mass. Although it may not appear obvious at first,
the fraction of stars that are formed in clusters, \(f_c\), is a very important parameter that regulates
the stochastic behavior of SLUG. This can be understood by considering two limiting cases.
In the limit \(f_c \rightarrow 0\), SLUG fills a galaxy by drawing stars from the
IMF. Thus, because the mass of a galaxy is typically much larger than the mass of the upper
end of the IMF, the effects of mass-constrained sampling highlighted in {\hyperref[tests:probsampl-label]{\emph{\DUspan{}{Problem constsampl: importance of constrained sampling}}}} are simply
not relevant anymore. In this case, stochasticity is minimal.
Conversely, in the limit \(f_c \rightarrow 1\), not only the IMF sampling contributes to the
stochastic behavior of SLUG, but also clusters themselves contribute to additional stochasticity,
as clusters are now drawn from the CMF to fill the targeted galaxy mass following the similar rules
to those specified for the IMF draws. Thus, in this case, constrained mass sampling applies to both
stars in clusters and clusters in galaxies, and stochasticity is amplified.

The command  \code{test/run\_clfraction.sh} runs three \code{galaxy} simulations, each with 500 trials
of continuous  SFR \(=0.001\rm\;M_\odot\;yr^{-1}\) which are evolved for a
single timestep of  \(2\times 10^6\rm\;yr\). A Chabrier IMF and a cluster mass function
\(\propto M^{-2}\) are adopted. Cluster disruption is disabled. The three simulations
differ only for the fraction of stars formed in clusters, respectively \(f_c=1,0.5,0.01\).
The analysis script \code{python test/plot\_clfraction.py} produces a multi-panel figure
\code{test/SLUG\_CLFRACTION\_f1.pdf}. Each column shows properties of simulations for different
fractions of stars formed in clusters.

The top row shows the maximum stellar mass in clusters. Clearly, \(f_c\) has no effect on the way
clusters are filled up with stars, but the normalization changes. Thus,  the least probable realizations
in the tail of the distribution simply do not appear for \(f_c \rightarrow 0\). The second row
shows the number of stars in clusters. Obviously, this scales directly with  \(f_c\), as it does the number
of field stars in the third row. This is expected as, by definition, \(f_c\) regulates the number of stars in
clusters versus the field. However, as discussed, \(f_c\) also affects the stochastic behavior of the
simulation. The fourth row shows histograms of the actual galaxy mass versus the targeted mass (red line).
As \(f_c\) increases, one can see that the spread around the targeted mass increase. This is again
a consequence of the mass-constrained sampling and the stop-nearest condition. For \(f_c \rightarrow 0\),
the code tries to fill a galaxy of mass \(0.001\rm\;M_\odot\;yr^{-1} \times 2\times 10^6\rm\;yr\)
with stars. Thus, since the targeted mass is at least a factor of 10 larger than the mass of the
building block, SLUG can approximate the desired mass very well (to better than \(120\rm\;M_\odot\), in fact).
Conversely, for \(f_c \rightarrow 1\), SLUG is using clusters as building blocks. As the typical
mass of the building blocks is now more comparable to the targeted galaxy mass, the problem of the
mass constrained sampling becomes a relevant one. Not only \(f_c\) affects the precision with which
SLUG builds galaxies, but, as shown in the bottom row, it also affects photometry. One can see that
\(Q_{H_0}\) increases as \(f_c\) decreases (the red lines indicate medians).
The reason for this behavior should now be clear:
in the case of clustered star formation (\(f_c \rightarrow 1\)), the mass of the most massive stars
is subject to the mass constrained sampling of the IMF at the cluster level, reducing the occurrence of
very massive stars and thus suppressing the flux of ionizing radiation. Conversely, for non clustered star formation
(\(f_c \rightarrow 0\)), the sampling of the IMF is constrained only at the galaxy mass level, and since this
is typically much greater than the mass of the most massive stars, one recovers higher fluxes on average.


\section{Problem \texttt{cmfchoice}: different CMF implementations}
\label{tests:problem-cmfchoice-different-cmf-implementations}
Given the ability of SLUG v2 to handle generic PDFs, the user can specify arbitrary CMF,
similarly to what shown in  {\hyperref[tests:probimf-label]{\emph{\DUspan{}{Problem imfchoice: different IMF implementations}}}}.
The command  \code{test/run\_cmfchoice.sh} runs three \code{galaxy} simulations, each with 500 trials
of continuous  SFR \(=0.001\rm\;M_\odot\;yr^{-1}\) which are evolved for a
single timestep of  \(2\times 10^6\rm\;yr\). A Chabrier IMF and \(f_c=1\)
are adopted. Cluster disruption is disabled. The three simulations
differ only for the cluster mass function, which are:
1) the default powerlaw \(M^{-2}\) between \(20-10^{7}~\rm M_\odot\);
2) a truncated powerlaw \(M^{-2}\) between \(20-100~\rm M_\odot\);
3) a mass-independent CMF \(M^{0}\) between \(20-10^3~\rm M_\odot\).
The analysis script \code{python test/plot\_cmfchoice.py} produces a multi-panel figure
\code{test/SLUG\_CMFCHOICE\_f1.pdf}. Each column shows properties of simulations for the different
cluster mass functions.

The top row shows the maximum stellar mass in clusters. Compared to the default case,
the histogram of the truncated CMF is steeper towards low masses. Given that the upper end of the
CMF is comparable to the maximum stellar mass of the chosen IMF, low stellar masses are typically
preferred  as a result of the stop-nearest condition. A flat CMF
prefers instead more massive clusters on average, which in turn results in higher probabilities
of drawing massive stars. In this case, the residual slope of the distribution towards
low stellar masses is a result of the shape of the IMF. A reflection of the effects induced by the
shape of the CMF are also apparent in the bottom row, which shows the distribution of
ionizing photons from these simulations. The second row shows instead the difference
between the targeted galaxy mass (red line), and the distribution of actual masses.
The spread is minimal for the truncated CMF because, as discussed above, SLUG is using
small building blocks, and it can approximate the targeted galaxy mass very well.
Larger spread is visible in the case of the flat CMF, as this choice allows for clusters with masses
up to \(10^3~\rm M_\odot\), without imposing an excess of probability at the low
mass end. The largest scatter is visible for the default case, as this CMF is virtually
a pure powerlaw without cutoff at the high mass end, and thus clusters as massive as the entire galaxy
are accessible to SLUG.


\section{Problem \texttt{sfhsampling}: realizations of SFH}
\label{tests:problem-sfhsampling-realizations-of-sfh}
The algorithm at the heart of SLUG is quite simple: for a given star formation history
\(\dot\psi(t)\) a stellar population with mass \(\dot\psi(t)\times \Delta t\)
is generated at each timestep, according to the constraints set by IMF, CMF and other
controlling parameters. As discussed in the previous examples, SLUG builds a best
approximation for the targeted mass \(\dot\psi(t)\times \Delta t\). This means that
the input SFH and the output SFHs are not identical. SLUG receives an input SFH which
is used to constrain the rate with which clusters and stars are drawn to achieve the
desired targeted mass in each timestep. However, the output SFHs are only realizations
and not exact copies  of the input SFH. This problem is designed to illustrate this behavior.

The command  \code{test/run\_sfhsampling.sh} runs two \code{galaxy} simulations, each with 100 trials
of continuous  SFR \(=0.0001\rm\;M_\odot\;yr^{-1}\) which are evolved for a
10 timesteps of  \(5\times 10^6\rm\;yr\). A Chabrier IMF and a \(M^{-2}\)
CMF are adopted. Cluster disruption is disabled. The two simulations
differ only for the fraction of stars in clusters, \(f_c = 1\) and \(f_c = 0\) respectively.
The analysis script \code{python test/plot\_sfhsampling.py} produces a two-panel figure
\code{test/SLUG\_SFHSAMPLING\_f1.pdf}, showing the box plot for the output SFH of the two simulations
(\(f_c = 1\) top, and \(f_c = 0\) bottom).

In each panel, the median SFH over 100 trials is represented by the red lines, while the red squares
show the mean. The box sizes represent instead the first and third quartile, with the
ends of the whiskers representing the 5th and 95th percentiles. One can see that the input
SFH at \(\dot\psi(t)=10^{-4}\rm\;M_\odot\;yr^{-1}\) is recovered on average, albeit with
significant variation in each realization. The reason for this variation lies in the fact that,
at low SFRs, SLUG samples the input SFH with coarse sampling points, which are clusters and stars.
One can also notice a widely different scatter between the \(f_c = 1\) and \(f_c = 0\)
case. In the former case, the basic elements used by SLUG to sample the targeted mass in  a
given interval are clusters. In the latter case, they are stars. Given that the typical mass of a
cluster is of the same order of the targeted mass in each interval, the output SFH for
the \(f_c = 1\) case are more sensitive to the history of drawings from the CMF.
Conversely, for  \(f_c = 0\), the sampling elements are less massive than the
targeted mass in a given interval, resulting in an output SFH distribution which is
better converged towards the input value. Clearly, a comparable amplitude in the scatter
will be present in the output photometry, especially for the traces that are more sensitive
to variations in the SFHs on short timescales.


\section{Problem \texttt{cldisrupt}: cluster disruption at work}
\label{tests:problem-cldisrupt-cluster-disruption-at-work}
One additional ingredient in SLUG is the lifetime distribution for clusters. Since v2, SLUG is flexible in
controlling the rate with which clusters are disrupted. This problem shows a comparison between
two simulations with and without cluster disruption.

The command  \code{test/run\_cldisrup.sh} runs two \code{galaxy} simulations, each with 100 trials
which are evolved in timesteps of  \(5\times 10^5\rm\;yr\) up to a maximum age of
\(1\times 10^7\rm\;yr\). Both simulations are characterized by a burst of star formation
\(=0.001\rm\;M_\odot\;yr^{-1}\) within the first Myr. A Chabrier IMF and a \(M^{-2}\)
CMF are adopted, and \(f_c = 1\). For the first simulation, cluster disruption is
disabled. In the second simulation, cluster disruption operates at times \(>1\rm\;Myr\),
with a cluster lifetime function which is a powerlaw of index -1.9.
The analysis script \code{python test/plot\_cldisrup.py} produces the figure \code{test/SLUG\_CLDISRUP\_f1.pdf}.
The two columns show results with (right) and without (left) cluster disruption.

The first row shows the median stellar mass of the 100 trials as a function of time.
The blue dashed lines show the mass inside the galaxy, while the black solid lines show the
median mass in clusters. The red band shows the first and fourth quartile of the distribution.
One can see that in both cases the galaxy mass rises in the first Myr up to the desired
targeted mass of \(=1000\rm\;M_\odot\) given the input SFH. After 1Myr, star formation
stops and the galaxy mass does not evolve with time. Conversely, the cluster mass (black line, red
regions) evolves differently. In the case without cluster disruption, because \(f_c = 1\),
the cluster mass tracks the galaxy mass at all time. When cluster disruption is enabled (right),
one can see that the mass in clusters rise following the galaxy mass in the first Myr. Past that time,
clusters start being disrupted and the mass in clusters declines.
The same behavior is visible in the second row, which shows the median number of alive (black) and
disrupted (black) clusters. To the left, without cluster disruption, the number of clusters alive
tracks the galaxy mass. Conversely, this distribution declines with time to the right when cluster disruption is
enabled. The complementary quantity (number of disrupted clusters) rises accordingly.
The last two rows show instead the integrated fluxes in FUV and bolometric luminosity.
Again, medians are in black and the first and third quartiles in red. One can see a nearly identical distribution
in the left and right panels. In these simulations, the controlling factors of the integrated photometry
are the SFH and the sampling techniques, which do not depend on the cluster disruption rate. Clearly, the
photometry of stars in cluster would exhibit instead a similar dependence to what shown in the top panels.


\section{Problem \texttt{spectra}: full spectra}
\label{tests:problem-spectra-full-spectra}
Since v2, SLUG is able to generate spectra for star clusters and for galaxies, which can also be computed for
arbitrary redshifts. This problem highlights the new features.
It also demonstrates how SLUG can handle dust extinction, both in a deterministic and stochastic way.

The command  \code{test/run\_spectra.sh} runs four \code{galaxy} simulations, each with 500 trials
of continuous SFR \(=0.001\rm\;M_\odot\;yr^{-1}\) which are evolved for a
single timestep of  \(2\times 10^6\rm\;yr\). A Chabrier IMF and a \(M^{-2}\)
CMF are adopted, cluster disruption is disabled, and \(f_c = 1\).
The simulations differ in the following way:
1) the reference model, computed without extinction and at \(z = 0\);
2) same as the reference model, but at \(z = 3\);
3) same as the reference model at \(z = 0\), but with a deterministic extinction of \(A_V = 0.5\) and
a Calzetti+2000 starburst attenuation curve;
4) same as model number 3, but with stochastic extinction.
The analysis script \code{python test/plot\_spectra.py} produces the figure \code{test/SLUG\_SPECTRA\_f1.pdf},
which shows a gallery of galaxy SEDs for each model. The median SED is shown in black, the blue region
corresponds to the first and third quartile of the distribution, and the red shaded region
marks the 5 and 95 percentiles.

The top panel shows the default model, where stochasticity occurs as detailed in the previous examples.
The second panel from the top shows instead a model with deterministic extinction. This is simply
a scaled-down version of the reference model, according to the input dust law and normalization
coefficient \(A_V\). As the dust law extends only to 915 Angstrom the output SED is truncated.
The third panel shows that, once SLUG handles dust  in a stochastic way, the intrinsic scatter is
amplified. This is a simple consequence of applying dust extinction with varying normalizations, which
enhances the final scatter about the median. Finally, the bottom panel shows the trivial case in which
the spectrum is shifted in wavelength by a constant factor \((1+z)\). Obviously, redshift enhances
the stochasticity in the optical due to a simple shift of wavelengths.


\chapter{Contributors and Acknowledgements}
\label{acknowledgements::doc}\label{acknowledgements:contributors-and-acknowledgements}
The following people contributed to slug2:
\begin{itemize}
\item {} 
Mark Krumholz: primary author of slug2

\item {} 
Michele Fumagalli: primary author of the slug2 test suite, co-author of version 1 of slug

\item {} 
Robert da Silva: primary author of version 1 of slug and of sfr\_slug, wrote the first prototype version of slug2 and sfr\_slug

\item {} 
Jonathan Parra: contributed code that become part of the slug\_PDF module

\item {} 
Teddy Rendahl: wrote the first version of cloudy\_slug

\item {} 
Michelle Myers: contributed to the development of cluster\_slug

\end{itemize}

In addition to these direct contributors, we gratefully acknowledge the following people who provided some of the data on which slug relies:
\begin{itemize}
\item {} 
The library of stellar evolutionary tracks and stellar atmospheres is taken from Claus Leitherer's \href{http://www.stsci.edu/science/starburst99/docs/default.htm}{starburst99} package.

\item {} 
Much of the library of photometric filters is taken from Charlie Conroy's \href{https://code.google.com/p/fsps/}{FSPS} package.

\item {} 
Daniela Calzetti provided the extinction curves

\end{itemize}


\chapter{Indices and tables}
\label{index:indices-and-tables}\begin{itemize}
\item {} 
\DUspan{xref,std,std-ref}{genindex}

\item {} 
\DUspan{xref,std,std-ref}{modindex}

\item {} 
\DUspan{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{theindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{s}
\item {\texttt{slugpy}}, \pageref{slugpy:module-slugpy}
\item {\texttt{slugpy.bayesphot.bp}}, \pageref{bayesphot:module-slugpy.bayesphot.bp}
\item {\texttt{slugpy.cloudy}}, \pageref{cloudy:module-slugpy.cloudy}
\end{theindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}
