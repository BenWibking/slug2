% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\usepackage{iftex}

\ifPDFTeX
  \usepackage[utf8]{inputenc}
\fi
\ifdefined\DeclareUnicodeCharacter
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}
\usepackage{eqparbox}


\addto\captionsenglish{\renewcommand{\figurename}{Fig.\@ }}
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\SetupFloatingEnvironment{literal-block}{name=Listing }

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{1}


\title{slug Documentation}
\date{Mar 24, 2017}
\release{2.0}
\author{Mark Krumholz, Michele Fumagalli, et al.}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ch\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@mb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@cpf\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\renewcommand\PYGZsq{\textquotesingle}

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}


Contents:


\chapter{License}
\label{license::doc}\label{license:welcome-to-slug-s-documentation}\label{license:license}
SLUG is distributed under the terms of the \href{http://www.gnu.org/copyleft/gpl.html}{GNU General Public License version 3.0}. The text of the license is included in the main directory of the repository as \code{GPL-3.0.txt}.


\chapter{Getting SLUG}
\label{getting:getting-slug}\label{getting::doc}
SLUG is available at \url{https://bitbucket.org/krumholz/slug2}. The easiest way to download a copy is via \href{http://git-scm.com/}{git}. If you have git, you can download SLUG by doing:

\begin{Verbatim}[commandchars=\\\{\}]
git clone https://krumholz@bitbucket.org/krumholz/slug2.git
\end{Verbatim}

In addition to the core SLUG code, the Bayesian inference tools cluster\_slug and sfr\_slug require large libraries of simulations on which to operate. These are not included in the git repository due to their sizes. You can download these from \url{http://www.slugsps.com/data}.


\chapter{Introduction to SLUG}
\label{intro:introduction-to-slug}\label{intro::doc}
This is a guide for users of the SLUG software package. SLUG is distributed under the terms of the \href{http://www.gnu.org/licenses/gpl.html}{GNU General Public License v. 3.0}. A copy of the license notification is included in the main SLUG directory. If you use SLUG in any published work, please cite the SLUG method papers, \href{http://adsabs.harvard.edu/abs/2012ApJ...745..145D}{da Silva, R. L., Fumagalli, M., \& Krumholz, M. R., 2012, The Astrophysical Journal, 745, 145} and \href{http://adsabs.harvard.edu/abs/2015MNRAS.452.1447K}{Krumholz, M. R., Fumagalli, M., da Silva, R. L., Rendahl, T., \& Parra, J. 2015, Monthly Notices of the Royal Astronomical Society, 452, 1447}.


\section{What Does SLUG Do?}
\label{intro:what-does-slug-do}
SLUG is a stellar population synthesis (SPS) code, meaning that, for a specified stellar initial mass function (IMF), star formation history (SFH), cluster mass function (CMF), cluster lifetime function (CLF), and (optionally) distribution of extinctions (A\_V), it predicts the spectra and photometry of both individual star clusters and the galaxies (or sub-regions of galaxies) that contain them. It also predicts the yields of various isotopes. In this regard, SLUG operates much like any other SPS code. The main difference is that SLUG regards the functions describing the stellar population as probability distributions, and the resulting stellar population as being the result of a draw from them. SLUG performs a Monte Carlo simulation to determine the PDF of the light and yields produced by the stellar populations that are drawn from these distributions. The remainder of this section briefly describes the major conceptual pieces of a SLUG simulation. For a more detailed description, readers are referred to \href{http://adsabs.harvard.edu/abs/2012ApJ...745..145D}{da Silva, Fumagalli, \& Krumholz (2012)}.


\section{Cluster Simulations and Galaxy Simulations}
\label{intro:cluster-simulations-and-galaxy-simulations}
SLUG can simulate either a simple stellar population (i.e., a group of stars all born at one time) or a composite stellar population, consisting of stars born at a distribution of times. We refer to the former case as a ``cluster'' simulation, and the latter as a ``galaxy'' simulation, since one can be thought of as approximating the behavior of a single star cluster, and the other as approximating a whole galaxy.


\section{Probability Distribution Functions: the IMF, SFH, CMF, CLF, A\_V distribution}
\label{intro:probability-distribution-functions-the-imf-sfh-cmf-clf-a-v-distribution}\label{intro:ssec-slugpdfs}
As mentioned above, SLUG regards the IMF, SFH, CMF, CLF, and extinction A\_V as probability distribution functions. These PDFs can be described by a very wide range of possible functional forms; see {\hyperref[pdfs:sec\string-pdfs]{\crossref{\DUrole{std,std-ref}{Probability Distribution Functions}}}} for details on the exact functional forms allowed, and on how they can be specified in the code. When SLUG runs a cluster simulation, it draws stars from the specified IMF in an attempt to produce a cluster of a user-specified total mass. There are a number of possible methods for performing such mass-limited sampling, and SLUG gives the user a wide menu of options; see {\hyperref[pdfs:sec\string-pdfs]{\crossref{\DUrole{std,std-ref}{Probability Distribution Functions}}}}. SLUG will also, upon user request, randomly draw a visual extinction A\_V to be applied to the light (and either the same or a different visual extinction can be applied to nebular light -- see {\hyperref[intro:ssec\string-nebula]{\crossref{\DUrole{std,std-ref}{Nebular Processing}}}}).

For a galaxy simulation, the procedure involves one extra step. In this case, SLUG assumes that some fraction \(f_c\) of the stars in the galaxy are born in star clusters, which, for the purposes of SLUG, means that they all share the same birth time. The remaining fraction \(1-f_c\) of stars are field stars. When a galaxy simulation is run, SLUG determines the total mass of stars \(M_*\) that should have formed since the start of the simulation (or since the last output, if more than one output is requested) from the star formation history, and then draws field stars and star clusters in an attempt to produce masses \((1-f_c)M_*\) and \(f_c M_*\). For the field stars, the stellar masses are drawn from the IMF, in a process completely analogous to the cluster case, and each star is given its own randomly-generated extinction. For star clusters, the masses of the clusters are drawn from the CMF, and each cluster is then populated from the IMF as in the cluster case. Again, each cluster gets its own extinction. For both the field stars and the star clusters, the time of their birth is drawn from the PDF describing the SFH.

Finally, star clusters can be disrupted independent of the fate of their parent stars. When each cluster is formed, it is assigned a lifetime drawn from the CLF. Once that time has passed, the cluster ceases to be entered in the lists of individual cluster spectra and photometry (see next section), although the individual stars continue to contribute to the integrated light of the galaxy.


\section{Spectra and Photometry}
\label{intro:ssec-spec-phot}\label{intro:spectra-and-photometry}
Once SLUG has drawn a population of stars, its final step is to compute the light they produce. SLUG does this in several steps. First, it computes the physical properties of all the stars present user-specified times using a set of stellar evolutionary tracks. Second, it uses these physical properties to compute the composite spectra produced by the stars, using a user-specified set of stellar atmosphere models. Formally, the quantity computed is the specific luminosity per unit wavelength \(L_\lambda\). Third, if nebular emission is enabled, the code calculates the spectrum \(L_{\lambda,\mathrm{neb}}\) that emerges after the starlight passes through the HII region aruond the star -- see {\hyperref[intro:ssec\string-nebula]{\crossref{\DUrole{std,std-ref}{Nebular Processing}}}}. Fourth, if extinction is enabled, SLUG computes the extincted stellar and nebula-processed spectra \(L_{\lambda,\mathrm{ex}}\) and \(L_{\lambda,\mathrm{neb,ex}}\) -- see {\hyperref[intro:ssec\string-extinction]{\crossref{\DUrole{std,std-ref}{Extinction}}}}. Fifth and finally, SLUG computes photometry for the stellar population by integrating all computed spectra over a set of specified photometric filters. Depending on the options specified by the user and the filter under consideration, the photometric value output will be one of the following:
\begin{itemize}
\item {} 
The frequency-averaged luminosity across the filter, defined as

\end{itemize}
\begin{equation*}
\begin{split}\langle L_\nu\rangle_R = \frac{\int L_\nu R_\nu \, d\ln\nu}{\int R_\nu (\nu/\nu_c)^\beta \, d\ln\nu},\end{split}
\end{equation*}
where \(L_\nu\) is the specific luminosity per unit frequency, \(R_\nu\) is the filter response function per photon at frequency \(\nu\), \(\nu_c\) is the central wavelength of the filter, and \(\beta\) is a constant that is defined by convention for each filter, and is either 0, 1, or 2; usually it is 0 for optical and UV filters.
\begin{itemize}
\item {} 
The wavelength-averaged luminosity across the filter, defined as

\end{itemize}
\begin{equation*}
\begin{split}\langle L_\lambda\rangle_R = \frac{\int L_\lambda R_\lambda \, d\ln\lambda}{\int R_\lambda (\lambda/\lambda_c)^{-\beta} \, d\ln\lambda},\end{split}
\end{equation*}
where \(L_\lambda\) is the specific luminosity per unit wavelength, \(R_\lambda\) is the filter response function per photon at wavelength \(\lambda\), and \(\lambda_c\) is the central wavelength of the filter.
\begin{itemize}
\item {} 
The AB magnitude, defined by

\end{itemize}
\begin{equation*}
\begin{split}M_{\rm AB} = -2.5 \log_{10} \left[\frac{\langle L_\nu\rangle_R}{4\pi\left(10\,\mathrm{pc}\right)^2}\right] - 48.6,\end{split}
\end{equation*}
where \(\langle L_\nu\rangle_R\) is in units of \(\mathrm{erg\,s}^{-1}\,\mathrm{Hz}^{-1}\).
\begin{itemize}
\item {} 
The ST magnitude, defined by

\end{itemize}
\begin{equation*}
\begin{split}M_{\rm ST} = -2.5 \log_{10} \left[\frac{\langle L_\lambda\rangle_R}{4\pi\left(10\,\mathrm{pc}\right)^2}\right] - 21.1,\end{split}
\end{equation*}
where \(\langle L_\lambda\rangle_R\) is in units of \(\mathrm{erg\, s}^{-1}\,\mathrm{Angstrom}^{-1}\).
\begin{itemize}
\item {} 
The Vega magnitude, defined by

\end{itemize}
\begin{equation*}
\begin{split}M_{\rm Vega} = M_{\rm AB} - M_{\rm AB}(\mbox{Vega}),\end{split}
\end{equation*}
where \(M_{\rm AB}(\mbox{Vega})\) is the AB magnitude of Vega. The latter quantity is computed on the fly, using a stored Kurucz model spectrum for Vega.
\begin{itemize}
\item {} 
The photon flux above some threshold \(\nu_0\), defined as

\end{itemize}
\begin{equation*}
\begin{split}Q(\nu_0) = \int_{\nu_0}^\infty \frac{L_\nu}{h\nu} \, d\nu.\end{split}
\end{equation*}\begin{itemize}
\item {} 
The bolometric luminosity,

\end{itemize}
\begin{equation*}
\begin{split}L_{\rm bol} = \int_0^\infty L_\nu \, d\nu.\end{split}
\end{equation*}
If nebular processing and/or extinction are enabled, photometric quantities are computed separately for each available version of the spectrum, \(L_\lambda\), \(L_{\lambda,\mathrm{neb}}\), \(L_{\lambda,\mathrm{ex}}\), and \(L_{\lambda,\mathrm{neb,ex}}\).

For a cluster simulation, this procedure is applied to the star cluster being simulated at a user-specified set of output times. For a galaxy simulation, the procedure is much the same, but it can be done both for all the stars in the galaxy taken as a whole, and individually for each star cluster that is still present (i.e., that has not been disrupted).


\section{Monte Carlo Simulation}
\label{intro:monte-carlo-simulation}
The steps described in the previous two section are those required for a single realization of the stellar population. However, the entire point of SLUG is to repeat this procedure many times in order to build up the statistics of the population light output. Thus the entire procedure can be repeated as many times as the user desires.


\section{Nebular Processing}
\label{intro:ssec-nebula}\label{intro:nebular-processing}
SLUG includes methods for post-processing the output starlight to compute the light that will emerge from the HII region around star clusters, and to further apply extinction to that light.

Nebular emission is computed by assuming that, for stars / star clusters younger than 10 Myr, all the ionizing photons are absorbed in a uniform-density, uniform-temperature HII region around each star cluster / star, and then computing the resulting emission at non-ionizing energies. The calculation assumes that the HII region is in photoionization equilibrium, and consists of hydrogen that is fully ionized and helium that is singly ionized. Under these assumptions the volume \(V\), electron density \(n_e\), and hydrogen density \(n_{\mathrm{H}}\) are related to the hydrogen ionizing luminosity \(Q(\mathrm{H}^0)\) via
\begin{equation*}
\begin{split}\phi Q(\mathrm{H}^0) = \alpha_{\mathrm{B}}(T) n_e n_{\mathrm{H}} V\end{split}
\end{equation*}
Here \(\phi\) is the fraction of ionizing photons that are absorbed by hydrogen within the observational aperture, and \(\alpha_{\mathrm{B}}(T)\) is the temperature-dependent case B recombination rate coefficient. SLUG approximates \(\alpha_{\mathrm{B}}(T)\) using the analytic approximation given by equation 14.6 of \href{http://adsabs.harvard.edu/abs/2011piim.book.....D}{Draine (2011, Physics of the Interstellar and Intergalactic Medium, Princeton University Press)}. The temperature used to compute \(\alpha_{\mathrm{B}}(T)\) can either be set by the user directly, or can be looked up automatically based on the age of the stellar population. The parameter \(\phi\) must be chosen by the user. It encompasses two distinct effects, both of which serve to reduce nebular emission. First, not all ionizing photons will be absorbed by H; some will be absorbed by dust, and will not yield nebular emission. At Solar metallicity, this effect sets an upper limit \(\phi\approx 0.73\) (see \href{http://adsabs.harvard.edu/abs/1997ApJ...476..144M}{McKee \& Williams (1997)}). Second, some of the ionizing photons may travel far from the stars before being absorbed that the nebular emission they produce is not captured within the observational aperture. The importance of this effect obviously depends on the details of the observation.

The relation above determines \(n_e n_{\mathrm{H}} V\), and from this SLUG computes the nebular emission including the following processes:
\begin{itemize}
\item {} 
\(\mathrm{H}^+\) and \(\mathrm{He}^+\) free-free emission

\item {} 
\(\mathrm{H}\) and \(\mathrm{He}\) bound-free emission

\item {} 
Hydrogen 2-photon emission

\item {} 
Hydrogen recombination lines from all lines with upper levels \(n_u \leq 25\)

\item {} 
Non-hydrogen line emission based on a tabulation (see below)

\end{itemize}

Formally, the luminosity per unit wavelength is computed as
\begin{equation*}
\begin{split}L_{\lambda,\mathrm{neb}} = \left[\gamma_{\mathrm{ff}}^{(\mathrm{H})} + \gamma_{\mathrm{bf}}^{(\mathrm{H})} + \gamma_{\mathrm{2p}}^{(\mathrm{H})} + \sum_{n,n' \leq 25, n<n'} \alpha_{nn'}^{\mathrm{eff,B,(H)}} E_{nn'}^{(\mathrm{H})} +  x_{\mathrm{He}} \gamma_{\mathrm{ff}}^{(\mathrm{He})} +  x_{\mathrm{He}} \gamma_{\mathrm{bf}}^{(\mathrm{He})} + \sum_i \gamma_{i,\mathrm{line}}^{(\mathrm{M})}\right] n_e n_{\mathrm{H}}{V}\end{split}
\end{equation*}
Here \(n_e n_{\mathrm{H}} V = \phi_{\mathrm{dust}} Q(\mathrm{H}^0)/ \alpha_{\mathrm{B}}(T)\) from photoionization equilibrium, \(E_{nn'}\) is the energy difference between hydrogen levels \(n\) and \(n'\), and the remaining terms and their sources appearing in this equation are:
\begin{itemize}
\item {} 
\(\gamma_{\mathrm{ff}}^{(\mathrm{H})}\) and \(\gamma_{\mathrm{ff}}^{(\mathrm{He})}\): HII and HeII free-free emission coefficients; these are computed from eqution 10.1 of \href{http://adsabs.harvard.edu/abs/2011piim.book.....D}{Draine (2011)}, using the analytic approximation to the Gaunt factor given by equation 10.8 of the same source

\item {} 
\(\gamma_{\mathrm{bf}}^{(\mathrm{H})}\) and \(\gamma_{\mathrm{bf}}^{(\mathrm{He})}\): HI and HeI bound-free emission coefficients; these are computed using the tabulation and interpolation method given in \href{http://adsabs.harvard.edu/abs/2006MNRAS.372.1875E}{Ercolano \& Storey (2006, MNRAS, 372, 1875)}

\item {} 
\(\alpha_{nn'}^{\mathrm{eff,B,(H)}}\) is the effective emission rate coefficient for the \(n\) to \(n'\) H recombination line, taken from the tabulation of \href{http://adsabs.harvard.edu/abs/1995MNRAS.272...41S}{Storey \& Hummer (1995, MNRAS, 272, 41)}

\item {} 
\(\gamma_{i,\mathrm{line}}^{(\mathrm{M})}\) is the emissivity for the brightest non-hydrogen lines, computed using a set of pre-tabulated values, following the procedure described in the \href{http://adsabs.harvard.edu/abs/2015MNRAS.452.1447K}{SLUG 2 method paper}

\item {} 
\(\gamma_{\mathrm{2p}}^{(\mathrm{H})}\): hydrogen two-photon emissivity, computed as

\end{itemize}
\begin{equation*}
\begin{split}\gamma_{\mathrm{2p}}^{(\mathrm{H})} = \frac{hc}{\lambda^3} I(\mathrm{H}^0) \alpha_{2s}^{\mathrm{eff,(H)}} \frac{1}{1 + \frac{n_{\mathrm{H}} q_{2s-2p,p} + (1+x_{\mathrm{He}}) n_{\mathrm{H}} q_{2s-2p,e}}{A_{2s-1s}}} P_\nu\end{split}
\end{equation*}
Here
\begin{itemize}
\item {} 
\(I(\mathrm{H}^0)\) is the hydrogen ionization potential

\item {} 
\(\alpha_{2s}^{\mathrm{eff,(H)}}\) is the effective recombination rate to the 2s state, taken from the tabulation of \href{http://adsabs.harvard.edu/abs/1995MNRAS.272...41S}{Storey \& Hummer (1995, MNRAS, 272, 41)}

\item {} 
\(q_{2s-2p,p}\) and \(q_{2s-2p,e}\) are the collisional rate coefficients for transitions from the 2s to the 2p state induced by collisions with protons and electrons, respectively, taken from \href{http://adsabs.harvard.edu/abs/1989agna.book.....O}{Osterbrock (1989, University Science Books, table 4.10)}

\item {} 
\(A_{2s-1s}\) is the Einstein coefficient for the hydrogen 2s-1s two-photon emission process, taken from \href{http://adsabs.harvard.edu/abs/2011piim.book.....D}{Draine (2011, section 14.2.4)}

\item {} 
\(P_\nu\) is the frequency distribution for two-photon emission, computed from the analytic approximation of \href{http://adsabs.harvard.edu/abs/1984A\%26A...138..495N}{Nussbaumer \& Schmutz (1984, A\&A, 138, 495)}

\end{itemize}


\section{Extinction}
\label{intro:extinction}\label{intro:ssec-extinction}
If extinction is enabled, SLUG applies extinction to the stellar spectra and, if nebular processing is enabled as well, to the spectrum that emerges from the nebula.

SLUG computes the extincted spectrum as
\begin{equation*}
\begin{split}L_{\lambda,\mathrm{ex}} = L_{\lambda} e^{-\tau_\lambda}\end{split}
\end{equation*}
where the optical depth \(\tau_\lambda = (\kappa_\lambda / \kappa_V) (A_V/1.086)\), \(A_V\) is the visual extinction in mag, the factor 1.086 is the conversion between magnitudes and the true dimensionless optical depth, \(\kappa_\lambda\) is a user-specified input extinction at wavelength \(\lambda\), and the V-band mean opacity is defined by
\begin{equation*}
\begin{split}\kappa_V = \frac{\int \kappa_\nu R_\nu(V) \, d\nu}{\int R_\nu(V) \, d\nu}\end{split}
\end{equation*}
where \(R_\nu(V)\) is the filter response function as frequency \(\nu\) for the Johnson V filter. The extinction curve \(\kappa_\lambda\) can be specified via a user-provided file, or the user may select from a set of pre-defined extinction curves; see {\hyperref[parameters:ssec\string-extinction\string-keywords]{\crossref{\DUrole{std,std-ref}{Extinction Keywords}}}} for details.

The computation for the extincted stellar plus nebular spectrum \(L_{\lambda,\mathrm{neb,ex}}\) is analogous. SLUG allows the nebular and stellar emission to undergo different amounts of extinction, consistent with observational results indicating that nebular light is usually more extincted than the stellar continuum (\href{http://adsabs.harvard.edu/abs/2000ApJ...533..682C}{Calzetti et al. (2000)}, \href{http://adsabs.harvard.edu/abs/2013ApJ...771...62K}{Kreckel et al. (2013)}). The total extincted stellar plus nebular spectrum is
\begin{equation*}
\begin{split}L_{\lambda,\mathrm{neb,ex}} = L_{\lambda} e^{-\tau_\lambda} + L_{\lambda,\mathrm{neb}} e^{-\tau_\lambda f_{\mathrm{neb-ex}}}\end{split}
\end{equation*}
where \(L_{\lambda}\) is the unextincted stellar spectrum, and \(f_{\mathrm{neb-ex}}\) is the ratio of nebular to stellar extinction -- typically about 2.1 based on observations, but left as a parameter to be set by the user.


\section{Chemical Yields}
\label{intro:chemical-yields}\label{intro:ssec-yields}
In addition to computing the light output by a stellar population, SLUG can also predict the yield of isotopes. At present SLUG includes yields for core collapse supernovae only. These are based on the yield tables provided by \href{http://adsabs.harvard.edu/abs/2016ApJ...821...38S}{Sukhbold et al. (2016)}, which provide a finely-spaced set of yields for progenitors of mass 9 - 120 \(M_\odot\). Yields from other stellar types will be added in later work.

SLUG operates by separately tracking the isotopes produced by stars winds and by their end-of-life explosions, as a function of progenitor mass. To obtain the yield of any star, SLUG interpolates on the wind and explosive yields using \href{http://adsabs.harvard.edu/abs/1990A\%26A...239..443S}{Steffen interpolation} to guarantee that no artifical extrema are produced. The code reports production of 302 distinct isotopes, including many stable species and select unstable ones; for unstable species, radioactive decay over the course of the simulation is properly handled.

At present there is a slight inconsistency in that the evolutionary tracks used by Sukhbold et al. do not precisely match those available for the purposes of omputing spectra and photometr. The death times of stars are computed using the tracks used output the photometric quantities, and thus are slightly inconsistent with the tracks used for the yields. This inconsistency also means that it is not possible to accurately place the wind yields in time; SLUG simply assumes that all yields due to winds are released when the stars end their lives, just as for the explosive yields.


\chapter{Compiling and Installing SLUG}
\label{compiling::doc}\label{compiling:compiling-and-installing-slug}

\section{Dependencies}
\label{compiling:dependencies}
The core SLUG program requires
\begin{itemize}
\item {} 
The \href{http://www.boost.org/}{Boost C++ libraries}

\item {} 
The \href{http://www.gnu.org/software/gsl/}{GNU scientific library} (version 2.x preferred, code can be compiled with version 1.x -- see below)

\end{itemize}

In addition, the following are required for some functionality, but not for the core code:
\begin{itemize}
\item {} 
The \href{http://heasarc.gsfc.nasa.gov/fitsio/fitsio.html}{cfitsio library} (required for FITS capabilities)

\item {} 
An implementation of \href{http://mpi-forum.org/}{MPI} (required for MPI support)

\end{itemize}

Compilation will be easiest if you install the required libraries such that the header files are included in your \code{CXX\_INCLUDE\_PATH} (for Boost) and \code{C\_INCLUDE\_PATH} (for GSL, cfitsio, and MPI) and the compiled object files are in your \code{LIBRARY\_PATH}. Alternately, you can manually specify the locations of these files by editing the Makefiles -- see below. The cfitsio library is optional, and is only required if you want the ability to write FITS output. To compile without it, use the flag \code{FITS=DISABLE\_FITS} when calling \code{make} (see below). The MPI libraries are required only for MPI capability, which is not enabled by default; see {\hyperref[library:ssec\string-mpi\string-support]{\crossref{\DUrole{std,std-ref}{Support for MPI Parallelism}}}} for an explanation of these capabilities and how to enable them. Note that SLUG uses some Boost libraries that must be built separately (see the Boost documentation on how to build and install Boost libraries).

In addition to the core dependencies, slugpy, the python helper library requires:
\begin{itemize}
\item {} 
\href{http://www.numpy.org/}{numpy}

\item {} 
\href{http://www.scipy.org/}{scipy}

\item {} 
\href{http://www.astropy.org/}{astropy} (optional, only required for FITS capabilities)

\end{itemize}

Finally, the cloudy coupling capability requires:
\begin{itemize}
\item {} 
\href{http://nublado.org}{cloudy}

\end{itemize}

This is only required performing cloudy runs, and is not required for any other part of SLUG.


\section{Compiling}
\label{compiling:ssec-compiling}\label{compiling:compiling}
If you have Boost in your \code{CXX\_INCLUDE\_PATH}, GSL in your \code{C\_INCLUDE\_PATH}, and (if you're using it) cfitsio in your \code{C\_INCLUDE\_PATH}, and the compiled libraries for each of these in your \code{LIBRARY\_PATH} environment variables, and your system is running either MacOSX or Linux, you should be able to compile simply by doing:

\begin{Verbatim}[commandchars=\\\{\}]
make
\end{Verbatim}

from the main \code{slug} directory.

To compile in debug mode, do:

\begin{Verbatim}[commandchars=\\\{\}]
make debug
\end{Verbatim}

instead.

To enable MPI support, do:

\begin{Verbatim}[commandchars=\\\{\}]
make MPI=ENABLE\PYGZus{}MPI
\end{Verbatim}

If you are compiling using GSL version 1.x or without cfitsio, you must specify these options when compiling. If you are using version 1.x of the GSL, do:

\begin{Verbatim}[commandchars=\\\{\}]
make GSLVERSION=1
\end{Verbatim}

To compile without FITS support, do:

\begin{Verbatim}[commandchars=\\\{\}]
make FITS=DISABLE\PYGZus{}FITS
\end{Verbatim}

Note that SLUG is written in C++11, and requires some C++11 features,
so it may not work with older C++ compilers. The following compiler
versions are known to work: gcc \textgreater{}= 4.8 (4.7 works on most but not all
platforms), clang/llvm \textgreater{}= 3.3, icc \textgreater{}= 14.0. Earlier versions may work
as well, but no guarantees.


\section{Machine-Specific Makefiles}
\label{compiling:machine-specific-makefiles}\label{compiling:ssec-machine-makefiles}
You can manually specify the compiler flags to be used for you machine
by creating a file named \code{Make.mach.MACHINE\_NAME} in the \code{src}
directory, and then doing:

\begin{Verbatim}[commandchars=\\\{\}]
make MACHINE=MACHINE\PYGZus{}NAME
\end{Verbatim}

An example machine-specific file, \code{src/Make.mach.ucsc-hyades} is
included in the repository. You can also override or reset any
compilation flag you want by editing the file
\code{src/Make.config.override}.


\chapter{Running a SLUG simulation}
\label{running::doc}\label{running:running-a-slug-simulation}

\section{Basic Serial Runs}
\label{running:basic-serial-runs}
Once SLUG is compiled, running a simulation is extremely simple. The first step, which is not required but makes life a lot simpler, is to set the environment variable \code{SLUG\_DIR} to the directory where you have installed SLUG. If you are using a \code{bash}-like shell, the syntax for this is:

\begin{Verbatim}[commandchars=\\\{\}]
export SLUG\PYGZus{}DIR = /path/to/slug
\end{Verbatim}

while for a \code{csh}-like shell, it is:

\begin{Verbatim}[commandchars=\\\{\}]
setenv SLUG\PYGZus{}DIR /path/to/slug
\end{Verbatim}

This is helpful because SLUG needs a lot of input data, and if you don't set this variable, you will have to manually specify where to find it.

Next, to run on a single processor, just do:

\begin{Verbatim}[commandchars=\\\{\}]
./bin/slug param/filename.param
\end{Verbatim}

where \code{filename.param} is the name of a parameter file, formatted as specified in {\hyperref[parameters:sec\string-parameters]{\crossref{\DUrole{std,std-ref}{Parameter Specification}}}}. The code will write a series of output files as described in {\hyperref[output:sec\string-output]{\crossref{\DUrole{std,std-ref}{Output Files and Format}}}}.


\section{Thread-Based Parallelism}
\label{running:thread-based-parallelism}
If you have more than one core at your disposal, you can also run SLUG in parallel using threads, via the command line:

\begin{Verbatim}[commandchars=\\\{\}]
python ./bin/slug.py param/filename.param
\end{Verbatim}

This called a python script that automatically divides up the Monte Carlo trials you have requested between the available processors, then consolidates the output so that it looks the same as if you had run a single-processor job. The python script allows fairly fine-grained control of the parallelism. It accepts the following command line arguments (not an exhaustive list -- do \code{python ./bin/slug.py -{-}help} for the full list):
\begin{itemize}
\item {} 
\code{-n NPROC, -{-}nproc NPROC}: this parameter specifies the number of simultaneous SLUG processes to run. It defaults to the number of cores present on the machine where the code is running.

\item {} 
\code{-b BATCHSIZE, -{-}batchsize BATCHSIZE}: this specifies how to many trials to do per SLUG process. It defaults to the total number of trials requested divided by the total number of processes, rounded up, so that only one SLUG process is run per processor. \emph{Rationale}: The default behavior is optimal from the standpoint of minimizing the overhead associated with reading data from disk, etc. However, if you are doing a very large number of runs that are going to require hours, days, or weeks to complete, and you probably want the code to checkpoint along the way. In that case it is probably wise to set this to a value smaller than the default in order to force output to be dumped periodically.

\item {} 
\code{-nc, -{-}noconsolidate}: by default the \code{slug.py} script will take all the outputs produced by the parallel runs and consolidate them into single output files, matching what would have been produced had the code been run in serial mode. If set, this flag suppresses that behavior, and instead leaves the output as a series of files whose root names match the model name given in the parameter file, plus the extension \code{\_pPPPPP\_nNNNNN}, where the digits \code{PPPPP} give the number of the processor that produces that file, and the digits \code{NNNNN} give the run number on that processor. \emph{Rationale}: normally consolidation is convenient. However, if the output is very large, this may produce undesirably bulky files. Furthermore, if one is doing a very large number of simulations over an extended period, and the \code{slug.py} script is going to be run multiple times (e.g., due to wall clock limits on a cluster), it may be preferable to leave the files unconsolidated until all runs have been completed.

\end{itemize}


\section{MPI-Based Parallelism}
\label{running:mpi-based-parallelism}
SLUG can also run in parallel on distributed-memory architectures using MPI. To use MPI, you must first compile the code with MPI support -- see {\hyperref[compiling:ssec\string-compiling]{\crossref{\DUrole{std,std-ref}{Compiling}}}}. Then to start an MPI-parallel computation, do:

\begin{Verbatim}[commandchars=\\\{\}]
mpirun \PYGZhy{}np N bin/slug param/filename.param
\end{Verbatim}

where \titleref{N} is the number of parallel processes to run. In this mode each MPI process will write its own output files, which will be named as \titleref{MODELNAME\_XXXX\_FILETYPE.EXT} where \titleref{MODELNAME} is the model name specified in the parameter file (see {\hyperref[parameters:sec\string-parameters]{\crossref{\DUrole{std,std-ref}{Parameter Specification}}}}), \titleref{XXXX} is the process number of the process that wrote the file, \titleref{FILETYPE} is the type of output file (see {\hyperref[output:sec\string-output]{\crossref{\DUrole{std,std-ref}{Output Files and Format}}}}), and \titleref{EXT} is the extension specifying the file format (see {\hyperref[output:sec\string-output]{\crossref{\DUrole{std,std-ref}{Output Files and Format}}}}).

If it is desirable to do so, the output files produced by an MPI run can be combined into a single output file using the \code{consolidate.py} script in the \code{tools} subdirectory.


\section{Checkpointing and Restarting}
\label{running:checkpointing-and-restarting}
When running a large number of trials, it is often desirable to checkpoint the calculation, i.e., to write intermediate outputs rather than waiting until the entire calculation is done to write. SLUG can checkpoint after a specified number of trials; this number is controlled by the \titleref{checkpoint\_interval} parameter (see {\hyperref[parameters:sec\string-parameters]{\crossref{\DUrole{std,std-ref}{Parameter Specification}}}}). Checkpoint files are are named as \titleref{MODELNAME\_chkYYYY\_FILETYPE.EXT} (or \titleref{MODELNAME\_XXXX\_chkYYYY\_FILETYPE.EXT} for MPI runs) where \titleref{YYYY} is the number of the checkpoint, starting at 0. Checkpoints are valid output files with some added information -- see {\hyperref[output:ssec\string-checkpoint\string-files]{\crossref{\DUrole{std,std-ref}{Checkpoint Files}}}} for details.

To restart a run from checkpoints, just give the command line option \titleref{--restart}, for example:

\begin{Verbatim}[commandchars=\\\{\}]
mpirun \PYGZhy{}np N bin/slug param/filename.param \PYGZhy{}\PYGZhy{}restart
\end{Verbatim}

SLUG will automatically search for checkpoint files (using the file names specified in \titleref{filename.param}), determine how many trials they contain, and resume the run to complete any remaining trials neede to reach the target number specified in the parameter file.

As with MPI runs, the output checkpoint files run can be combined into a single output file using the \code{consolidate.py} script in the \code{tools} subdirectory.


\chapter{Parameter Specification}
\label{parameters:sec-parameters}\label{parameters::doc}\label{parameters:parameter-specification}

\section{Automated Parameter File Generation}
\label{parameters:automated-parameter-file-generation}
The remainder of this section contains information on how parameter files are formatted, and exactly how parameter choices specify code behavior. However, as a convenience SLUG comes with a python script that provides a simple menu-driven interface to write parameter files automatically. The script can be started by doing:

\begin{Verbatim}[commandchars=\\\{\}]
python tools/write\PYGZus{}param.py
\end{Verbatim}

Once started, the script provides a series of menus that allow the user to set all the keywords specified below. The script can then write a validly-formatted parameter file based on the options chosen.


\section{File Format}
\label{parameters:file-format}
An example parameter file is included as \code{param/example.param} in the source tree. Parameter files for SLUG are generically formatted as a series of entries of the form:

\begin{Verbatim}[commandchars=\\\{\}]
keyword    value
\end{Verbatim}

Any line starting with \code{\#} is considered to be a comment and is ignored, and anything on a line after a \code{\#} is similarly treated as a comment and ignored. Some general rules on keywords are:
\begin{itemize}
\item {} 
Keywords may appear in any order.

\item {} 
Some keywords have default values, indicated in parenthesis in the list below. These keywords are optional and need not appear in the parameter file. All others are required.

\item {} 
Keywords and values are case-insensitive.

\item {} 
Unless explicitly stated otherwise, units for mass are always \(M_\odot\), units for time are always yr.

\item {} 
Any time a file or directory is specified, if it is given as a relative rather than absolute path, it is assumed to be relative to the environment variable \code{\$SLUG\_DIR}. If this environment variable is not set, it is assumed to be relative to the current working directory. EXCEPTION: the output directory (if it is specified as a relative rather than absolute path) is always assumed to be relative to the current working directory.

\end{itemize}

The keywords recognized by SLUG can be categorized as described in the remainder of this section.


\section{Basic Keywords}
\label{parameters:basic-keywords}\label{parameters:ssec-basic-keywords}
These specify basic data for the run.
\begin{itemize}
\item {} 
\code{model\_name} (default: \code{SLUG\_DEF}): name of the model. This will become the base filename for the output files.

\item {} 
\code{out\_dir} (default: current working direcory): name of the directory into which output should be written. If not specified, output is written into the directory from which the SLUG executable is called.

\item {} 
\code{verbosity} (default: \code{1}): level of verbosity when running, with 0 indicating no output, 1 indicating some output, and 2 indicating a great deal of output.

\end{itemize}


\section{Simulation Control Keywords}
\label{parameters:simulation-control-keywords}
These control the operation of the simulation.
\begin{itemize}
\item {} 
\code{sim\_type} (default: \code{galaxy}): set to \code{galaxy} to run a galaxy simulation (a composite stellar population), or to \code{cluster} to run a cluster simulation (a simple stellar population)

\item {} 
\code{n\_trials} (default: \code{1}): number of trials to run

\item {} 
\code{log\_time} (default: \code{0}): set to 1 for logarithmic time step, 0 for linear time steps

\item {} 
\code{time\_step}: size of the time step. If \code{log\_time} is set to 0, this is in yr. If \code{log\_time} is set to 1, this is in dex (i.e., a value of 0.2 indicates that every 5 time steps correspond to a factor of 10 increase in time). Alternately, if \code{time\_step} is set to any value that cannot be converted to a real number, then this is interpreted as giving the name of a PDF file, which must be formatted as described in {\hyperref[pdfs:sec\string-pdfs]{\crossref{\DUrole{std,std-ref}{Probability Distribution Functions}}}}. In this case one output time will be selected randomly for each trial from the specified PDF. This option is useful, for example, for generating a library of simulations that are randomly sampled in stellar population age. For the PDF option, the options \code{log\_time}, \code{start\_time} and \code{end\_time} will all be ignored, as the relevant parameters will be taken from the specified PDF file. This keyword may be omitted, and will be ignored, if \code{output\_times} is set.

\item {} 
\code{start\_time}: first output time. This may be omitted if \code{log\_time} is set to 0, in which case it defaults to a value equal to \code{time\_step}. It may also be omitted if \code{output\_times} is set.

\item {} 
\code{end\_time}: last output time, in yr. This may be omitted of \code{output\_times} is set. Note that not all the tracks include entries going out to times \textgreater{}1 Gyr, and the results will become inaccurate if the final time is larger than the tracks allow.

\item {} 
\code{output\_times}: an optional parameter giving an exact list of times in yr at which to write output. This must be specified as a comma-separated list, i.e., time0, time1, time2, ..., where all times are positive and are in strictly increasing order. There is no limit on the number of output times that may be included. This parameter may be omitted if \code{end\_time} and \code{time\_step} are set. If this parameter is set, it overrides \code{start\_time}, \code{end\_time}, and \code{time\_step}.

\item {} 
\code{sfr}: star formation rate. Only used if \code{sim\_type} is \code{galaxy}; for \code{cluster}, it will be ignored, and can be omitted. This parameter can be set in three ways. If the value given is a number, it will be interpreted as specifying a constant star formation rate. If it is the string \code{sfh}, the code will interpret this as a flag that a star formation history should be read from the file specified by the \code{sfh} keyword. If the parameter value is any other string that cannot be converted to a numerical value, it will be interpreted as the name of a PDF file (see {\hyperref[pdfs:sec\string-pdfs]{\crossref{\DUrole{std,std-ref}{Probability Distribution Functions}}}}); a (constant) value of the star formation rate for each trial will be drawn from this PDF.

\item {} 
\code{sfh}: name of star formation history file. This file is a PDF file, formatted as described in {\hyperref[pdfs:sec\string-pdfs]{\crossref{\DUrole{std,std-ref}{Probability Distribution Functions}}}}. This is ignored, and can be omitted, if \code{sim\_type} is \code{cluster}, or if \code{sfr} is not set to \code{sfh}.

\item {} 
\code{cluster\_mass}: mass of the star cluster for simulations with \code{sim\_type} set to \code{cluster}. This can be omitted, and will be ignored, if \code{sim\_type} is \code{galaxy}. This parameter can be set to either a positive number or to the string \code{cmf}. If it is set to a numerical value, that value will be used as the cluster mass, in \(M_\odot\) for each trial. If it is set to \code{cmf}, then a new cluster mass will be drawn from the CMF for each trial.

\item {} 
\code{redshift} (default: \code{0}): place the system at the specified redshift. The computed spectra and photometry will then be computed in the observed rather than the rest frame of the system.

\end{itemize}


\section{Output Control Keywords}
\label{parameters:output-control-keywords}
These control what quantities are computed and written to disk. Full a full description of the output files and how they are formatted, see {\hyperref[output:sec\string-output]{\crossref{\DUrole{std,std-ref}{Output Files and Format}}}}.
\begin{itemize}
\item {} 
\code{out\_cluster} (default: \code{1}): write out the physical properties of star clusters? Set to 1 for yes, 0 for no.

\item {} 
\code{out\_cluster\_phot} (default: \code{1}): write out the photometry of star clusters? Set to 1 for yes, 0 for no.

\item {} 
\code{out\_cluster\_spec} (default: \code{1}): write out the spectra of star clusters? Set to 1 for yes, 0 for no.

\item {} 
\code{out\_cluster\_yield} (default: \code{1}): write out the yield of star clusters? Set to 1 for yes, 0 for no.

\item {} 
\code{out\_integrated} (default: \code{1}): write out the integrated physical properties of the whole galaxy? Set to 1 for yes, 0 for no. This keyword is ignored if \code{sim\_type} is \code{cluster}.

\item {} 
\code{out\_integrated\_phot} (default: \code{1}): write out the integrated photometry of the entire galaxy? Set to 1 for yes, 0 for no. This keyword is ignored if \code{sim\_type} is \code{cluster}.

\item {} 
\code{out\_integrated\_spec} (default: \code{1}): write out the integrated spectra of the entire galaxy? Set to 1 for yes, 0 for no. This keyword is ignored if \code{sim\_type} is \code{cluster}.

\item {} 
\code{out\_integrated\_yield} (default: \code{1}): write out the integrated yield of the entire galaxy? Set to 1 for yes, 0 for no. This keyword is ignored if \code{sim\_type} is \code{cluster}.

\item {} 
\code{output\_mode} (default: \code{ascii}): set to \code{ascii}, \code{binary}, or \code{fits}. Selecting \code{ascii} causes the output to be written in ASCII text, which is human-readable, but produces much larger files. Selecting \code{binary} causes the output to be written in raw binary. Selecting \code{fits} causes the output to be written FITS format. This will be somewhat larger than raw binary output, but the resulting files will be portable between machines, which the raw binary files are not guaranteed to be. All three output modes can be read by the python library, though with varying speed -- ASCII output is slowest, FITS is intermediate, and binary is fastest.

\end{itemize}


\section{Stellar Model Keywords}
\label{parameters:stellar-model-keywords}\label{parameters:ssec-stellar-keywords}
These specify the physical models to be used for stellar evolution, atmospheres, the IMF, extinction, etc.
\begin{itemize}
\item {} \begin{description}
\item[{\code{imf} (default: \code{lib/imf/chabrier.imf}): name of the IMF descriptor file; this is a PDF file, formatted as described in {\hyperref[pdfs:sec\string-pdfs]{\crossref{\DUrole{std,std-ref}{Probability Distribution Functions}}}}. Note that SLUG ships with the following IMF files pre-defined (in the directory \code{lib/imf})}] \leavevmode\begin{itemize}
\item {} 
\code{chabrier.imf} (single-star IMF from \href{http://adsabs.harvard.edu/abs/2005ASSL..327...41C}{Chabrier, 2005, in ``The Initial Mass Function 50 Years Later'', eds. E. Corbelli, F. Palla, \& H. Zinnecker, Springer: Dordrecht, p. 41})

\item {} 
\code{chabrier03.imf} (single-star IMF from \href{http://adsabs.harvard.edu/abs/2003PASP..115..763C}{Chabrier, 2003, PASP, 115, 763-795})

\item {} 
\code{kroupa.imf} (IMF from \href{http://adsabs.harvard.edu/abs/2002Sci...295...82K}{Kroupa, 2002, Science, 295, 82-91})

\item {} 
\code{kroupa\_sb99.imf} (simplified version of the Kroupa, 2002 IMF used by default by \href{http://www.stsci.edu/science/starburst99/docs/default.htm}{starburst99})

\item {} 
\code{salpeter.imf} (single-component power law IMF from \href{http://adsabs.harvard.edu/abs/1955ApJ...121..161S}{Salpeter, 1955, ApJ, 121, 161})

\end{itemize}

\end{description}

\item {} 
\code{cmf} (default: \code{lib/cmf/slug\_default.cmf}): name of the CMF descriptor file; this is a PDF file, formatted as described in {\hyperref[pdfs:sec\string-pdfs]{\crossref{\DUrole{std,std-ref}{Probability Distribution Functions}}}}. The default selection is a power law \(dN/dM \propto M^{-2}\) from \(M = 10^2 - 10^7\;M_\odot\). This is ignored, and may be omitted, if \code{sim\_type} is set to \code{cluster} and \code{cluster\_mass} is set to a numerical value.

\item {} 
\code{clf} (default: \code{lib/clf/slug\_default.clf}): name of the CLF descriptor file; this is a PDF file, formatted as described in {\hyperref[pdfs:sec\string-pdfs]{\crossref{\DUrole{std,std-ref}{Probability Distribution Functions}}}}. The default gives a power law distribution of lifetimes \(t\) with \(dN/dt\propto t^{-1.9}\) from 1 Myr to 1 Gyr. Note that this corresponds to a cluster age distribution of slope -0.9. The SLUG source also ships with an alternative CLF file, \code{lib/clf/nodisrupt.clf}, which disables cluster disruption entirely (by setting the lifetime distribution to a \(\delta\) function at \(10^{300}\) yr).

\item {} \begin{description}
\item[{\code{tracks} (default: \code{lib/tracks/Z0140v00.txt}): stellar evolution tracks to use. The following tracks ship with SLUG (all in the directory \code{lib/tracks}):}] \leavevmode\begin{itemize}
\item {} 
\code{ZXXXXvYY.txt}: Geneva (2013) tracks; metallicities are Solar (\code{XXXX = 0140}) and 1/7 Solar (\code{XXXX = 0020}), and rotation rates are 0 (\code{YY = 00}) and 40\% of breakup (\code{YY = 40}).

\item {} 
\code{modcXXX.dat}: Geneva tracks with standard mass loss, for metallicities of \(2\times\) Solar (\code{040}), Solar (\code{020}), \(0.4\times\) Solar (\code{008}), \(0.2\times\) Solar (\code{004}), and \(0.05\times\) Solar (\code{001}).

\item {} 
\code{modeXXX.dat}: same as \code{modcXXX.dat}, but with higher mass loss rates.

\item {} 
\code{modpXXX.dat}: Padova tracks with thermally pulsing AGB stars; metallicities use the same scale as \code{modcXXX.dat} files (i.e., \code{020} is Solar).

\item {} 
\code{modsXXX.dat}: same as \code{modpXXX.dat}, but without thermally pulsing AGB stars

\end{itemize}

\end{description}

\item {} 
\code{atmospheres} (default: \code{lib/atmospheres}): directory where the stellar atmosphere library is located. Note that file names are hard-coded, so if you want to use different atmosphere models with a different format, you will have to write new source code to do so.

\item {} 
\code{yields} (default: \code{lib/yields}): directory where the stellar yield tables are located. Note that the file name and format is hardcoded, so if you want to use a different format, you will have to write source code to do so.

\item {} \begin{description}
\item[{\code{specsyn\_mode} (default: \code{sb99}): spectral synthesis mode. Allowed values are:}] \leavevmode\begin{itemize}
\item {} 
\code{planck}: treat all stars as black bodies

\item {} 
\code{Kurucz}: use Kurucz atmospheres, as compiled by \href{http://adsabs.harvard.edu/abs/1997A\%26AS..125..229L}{Lejeune et al. (1997, A\&AS, 125, 229)}, for all stars

\item {} 
\code{Kurucz+Hillier}: use Kurucz atmospheres for all stars except Wolf-Rayet stars; WR stars use Hillier model atmospheres (\href{http://adsabs.harvard.edu/abs/1998ApJ...496..407H}{Hillier \& Miller, 1998, ApJ, 496, 407})

\item {} 
\code{Kurucz+Pauldrach}: use Kurucz atmospheres for all stars except OB stars; OB stars use Pauldrach model atmospheres (\href{http://adsabs.harvard.edu/abs/2001A\%26A...375..161P}{Pauldrach et al., 2001, A\&A, 375, 161})

\item {} 
\code{SB99}: emulate the behavior of \code{starburst99}: use Pauldrach for OB stars, Hillier for WR stars, and Kurucz for all other stars

\end{itemize}

\end{description}

\item {} 
\code{clust\_frac} (default: \code{1.0}): fraction of stars formed in clusters

\item {} 
\code{min\_stoch\_mass} (default: \code{0.0}): minimum stellar mass to be treated stochastically. All stars with masses below this value are assumed to be sampled continuously from the IMF.

\item {} 
\code{metallicity}: metallicity of the stellar population, relative to solar. This may be omitted if \code{tracks} is set to one of the default sets of tracks that ships with SLUG, as the metallicities for these tracks are hardwired in. This keyword is provided to allow users to supply their own tracks.

\item {} 
\code{WR\_mass}: minimum starting mass that stars must have in order to pass through a Wolf-Rayet phase. This can be omitted if \code{tracks} is set to one of the default sets of tracks that ships with SLUG, as the WR cutoff masses for these tracks are hardwired in. This keyword is provided to allow users to supply their own tracks.

\end{itemize}


\section{Extinction Keywords}
\label{parameters:extinction-keywords}\label{parameters:ssec-extinction-keywords}\begin{itemize}
\item {} 
\code{A\_V} (default: no extinction): extinction distribution. This parameter has three possible behaviors. If the parameter \code{A\_V} is omitted entirely, then the code will not compute extinction-corrected spectra or photometry at all; only unextincted values will be reported. If this parameter is specified as a real number, it will be interepreted as specifying a uniform extinction value \(A_V\), in mag, and this extinction will be applied to all predicted light output. Finally, if this parameter is a string that cannot be converted to a real number, it will be interpreted as the name of a PDF file, formatted as described in {\hyperref[pdfs:sec\string-pdfs]{\crossref{\DUrole{std,std-ref}{Probability Distribution Functions}}}}, specifying the probability distribution of \(A_V\) values, in mag.

\item {} \begin{description}
\item[{\code{extinction\_curve} (default: \code{lib/extinct/SB\_ATT\_SLUG.dat}) file specifying the extinction curve; the file format is two columns of numbers in ASCII, the first giving the wavelength in Angstrom and the second giving the exintction \(\kappa_\nu\) at that wavelength / frequency in \(\mathrm{cm}^2\). Note that the absolute normalization of the exitnction curve is unimportant; only the wavelength-dependence matters (see {\hyperref[intro:ssec\string-spec\string-phot]{\crossref{\DUrole{std,std-ref}{Spectra and Photometry}}}}). SLUG ships with the following extinction curves (all in \code{lib/extinct}):}] \leavevmode\begin{itemize}
\item {} 
\code{LMC\_EXT\_SLUG.dat} : LMC extinction curve; optical-UV from \href{http://adsabs.harvard.edu/abs/1999PASP..111...63F}{Fitzpatrick, E. L., 1999, PASP, 111, 63}, IR from \href{http://adsabs.harvard.edu/abs/1984A\%26A...134..284L}{Landini, M., et al., 1984, A\&A, 134, 284}; parts combined by D. Calzetti

\item {} 
\code{MW\_EXT\_SLUG.dat} : MW extinction curve; optical-UV from \href{http://adsabs.harvard.edu/abs/1999PASP..111...63F}{Fitzpatrick, E. L., 1999 PASP, 111, 63}, IR from \href{http://adsabs.harvard.edu/abs/1984A\%26A...134..284L}{Landini, M., et al., 1984, A\&A, 134, 284}; parts combined by D. Calzetti

\item {} 
\code{SB\_ATT\_SLUG.dat} : ``starburst'' extinction curve from \href{http://adsabs.harvard.edu/abs/2000ApJ...533..682C}{Calzetti, D., et al., 2000, ApJ, 533, 682}

\item {} 
\code{SMC\_EXT\_SLUG.dat} : SMC extinction curve from \href{http://adsabs.harvard.edu/abs/1985A\%26A...149..330B}{Bouchet, P., et al., 1985, A\&A, 149, 330}

\item {} 
\code{MW\_draine\_RV3.1.dat} : MW extinction curve for reddening \(R_V = 3.1\), taken from the model of \href{http://adsabs.harvard.edu/abs/2003ARA\%26A..41..241D}{Draine, 2003, ARA\&A, 41, 241}, and retrieved from B. Draine's \href{https://www.astro.princeton.edu/~draine/dust/dustmix.html}{personal web page}

\end{itemize}

\end{description}

\item {} 
\code{nebular\_extinction\_factor} (default: 1.0): nebular extinction excess factor. This parameter specifies the ratio of the extinction applied to the nebular light to that applied to the starlight, i.e., it gives \(f_{\mathrm{neb,ex}} = A_{V,\mathrm{neb}} / A_{V,*}\), as defined in {\hyperref[intro:ssec\string-extinction]{\crossref{\DUrole{std,std-ref}{Extinction}}}}. As with \code{A\_V}, this parameter can be set either to a real number, in which case this ratio is treated as constant and equal to the input number, or to the name of a PDF file that specified the distribution of this ratio, formatted as described in {\hyperref[pdfs:sec\string-pdfs]{\crossref{\DUrole{std,std-ref}{Probability Distribution Functions}}}}. If this keyword is omitted entirely, the nebular and stellar extinctions are set equal to one another.

\end{itemize}


\section{Nebular Keywords}
\label{parameters:ssec-nebular-keywords}\label{parameters:nebular-keywords}\begin{itemize}
\item {} 
\code{compute\_nebular} (default: \code{1}): compute the spectrum that results after starlight is processed through the nebula surrounding each cluster or star? Set to 1 for yes, 0 for no.

\item {} 
\code{atomic\_data} (default: \code{lib/atomic/}): directory where the atomic data used for nebular emission calculations is located

\item {} 
\code{nebular\_no\_metals} (default: 0): if set to 1, metal lines are not used when computing nebular emission

\item {} 
\code{nebular\_den} (default: \code{1e2}): hydrogen number density in \(\mathrm{cm}^{-3}\) to use in nebular emission computations

\item {} 
\code{nebular\_temp} (default: \code{-1}): gas kinetic temperature in K to use in nebular emission computations; if set to non-positive value, the temperature will be determined via the lookup table of cloudy runs for fully sampled IMFs

\item {} 
\code{nebular\_logU} (default: \code{-3}): log of dimensionless volume-weighted ionization parameter to assume when computing metal line emission and HII region temperatures from the tabulated cloudy data. At present the allowed values are -3, -2.5, and -2.

\item {} 
\code{nebular\_phi} (default: \code{0.73}): fraction of ionizing photons absorbed by H atoms rather than being absorbed by dust grains or rescaping; the default value of \code{0.73}, taken from \href{http://adsabs.harvard.edu/abs/1997ApJ...476..144M}{McKee \& Williams (1997, ApJ, 476, 144)} means that 73\% of ionizing photons are absorbed by H

\end{itemize}


\section{Photometric Filter Keywords}
\label{parameters:ssec-phot-keywords}\label{parameters:photometric-filter-keywords}
These describe the photometry to be computed. Note that none of these keywords have any effect unless \code{out\_integrated\_phot} or \code{out\_cluster\_phot} is set to 1.
\begin{itemize}
\item {} \begin{description}
\item[{\code{phot\_bands}: photometric bands for which photometry is to be computed. The values listed here can be comma- or whitespace-separated. For a list of available photometric filters, see the file \code{lib/filters/FILTER\_LIST}. In addition to these filters, SLUG always allows four special ``bands'':}] \leavevmode\begin{itemize}
\item {} 
\code{QH0}: the \(\mathrm{H}^0\) ionizing luminosity, in photons/sec

\item {} 
\code{QHe0}: the \(\mathrm{He}^0\) ionizing luminosity, in photons/sec

\item {} 
\code{QHe1}: the \(\mathrm{He}^+\) ionizing luminosity, in photons/sec

\item {} 
\code{Lbol}: the bolometric luminosity, in \(L_\odot\)

\end{itemize}

\end{description}

\item {} 
\code{filters} (default: \code{lib/filters}): directory containing photometric filter data

\item {} \begin{description}
\item[{\code{phot\_mode} (default: \code{L\_nu}): photometric system to be used when writing photometric outputs. Full definitions of the quantities computed for each of the choices listed below are given in {\hyperref[intro:ssec\string-spec\string-phot]{\crossref{\DUrole{std,std-ref}{Spectra and Photometry}}}}. Note that these values are ignored for the four special bands \code{QH0}, \code{QHe0}, \code{QHe1}, and \code{Lbol}. These four bands are always written out in the units specified above. Allowed values are:}] \leavevmode\begin{itemize}
\item {} 
\code{L\_nu}: report frequency-averaged luminosity in the band, in units of erg/s/Hz

\item {} 
\code{L\_lambda}: report wavelength-averaged luminosity in the band, in units of erg/s/Angstrom

\item {} 
\code{AB}: report AB magnitude

\item {} 
\code{STMAG}: report ST magnitude

\item {} 
\code{VEGA}: report Vega magnitude

\end{itemize}

\end{description}

\end{itemize}


\chapter{Probability Distribution Functions}
\label{pdfs:probability-distribution-functions}\label{pdfs::doc}\label{pdfs:sec-pdfs}
The SLUG code regards the IMF, the CMF, the CLF, the SFH, and the extinction \(A_V\) as probability distribution functions -- see {\hyperref[intro:ssec\string-slugpdfs]{\crossref{\DUrole{std,std-ref}{Probability Distribution Functions: the IMF, SFH, CMF, CLF, A\_V distribution}}}}. The code provides a generic file format through which PDFs can be specified. Examples can be found in the \code{lib/imf}, \code{lib/cmf}, \code{lib/clf}, and \code{lib/sfh} directories of the SLUG distribution.

PDFs in SLUG are generically written as functions
\begin{equation*}
\begin{split}\frac{dp}{dx} = n_1 f_1(x; x_{1,a}, x_{1,b}) + n_2 f_2(x; x_{2,a}, x_{2,b}) + n_3 f_3(x; x_{3,a}, x_{3,b}) + \cdots,\end{split}
\end{equation*}
where \(f_i(x; x_{i,a}, x_{i,b})\) is non-zero only for \(x \in [x_{i,a}, x_{i,b}]\). The functions \(f_i\) are simple continuous functional forms, which we refer to as \emph{segments}. Functions in this form can be specified in SLUG in two ways.


\section{Basic Mode}
\label{pdfs:basic-mode}
The most common way of specifying a PDF is in basic mode. Basic mode describes a PDF that has the properties that
\begin{enumerate}
\item {} 
the segments are contiguous with one another, i.e., \(x_{i,b} = x_{i+1,a}\)

\item {} 
\(n_i f_i(x_{i,b}; x_{i,a}, x_{i,b}) = n_{i+1} f_{i+1}(x_{i+1,a}; x_{i+1,a}, x_{i+1,b})\)

\item {} 
the overall PDF is normalized such that \(\int (dp/dx)\, dx = 1\)

\end{enumerate}

Given these constraints, the PDF can be specified fully simply by giving the \(x\) values that define the edges of the segments and the functional forms \(f\) of each segment; the normalizations can be computed from the constraint equations. Note that SFH PDFs cannot be described using basic mode, because they are not normalized to unity. Specifying a non-constant SFH requires advanced mode.

An example of a basic mode PDF file is as follows:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}
\PYGZsh{} This is an IMF definition file for SLUG v2.
\PYG{g+gh}{\PYGZsh{} This file defines the Chabrier (2005) IMF}
\PYG{g+gh}{\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}}

\PYGZsh{} Breakpoints: mass values where the functional form changes
\PYGZsh{} The first and last breakpoint will define the minimum and
\PYGZsh{} maximum mass
breakpoints 0.08 1 120

\PYGZsh{} Definitions of segments between the breakpoints

\PYGZsh{} This segment is a lognormal with a mean of log\PYGZus{}10 (0.2 Msun)
\PYGZsh{} and dispersion 0.55; the dispersion is in log base 10, not
\PYGZsh{} log base e
segment
type lognormal
mean 0.2
disp 0.55

\PYGZsh{} This segment is a powerlaw of slope \PYGZhy{}2.35
segment
type powerlaw
slope \PYGZhy{}2.35
\end{Verbatim}

This example represents a \href{http://adsabs.harvard.edu/abs/2005ASSL..327...41C}{Chabrier (2005)} IMF from \(0.08 - 120\) \(M_\odot\), which is of the functional form
\begin{equation*}
\begin{split}\frac{dp}{dm} \propto \left\{\begin{array}{ll} \exp[-\log(m/m_0)^2/(2\sigma^2)] (m/m_b)^{-1} , & m < m_b \\ \exp[-\log(m_b/m_0)^2/(2\sigma^2)] (m/m_b)^{-2.35}, & m \geq m_b \end{array} \right.,\end{split}
\end{equation*}
where \(m_0 = 0.2\) \(M_\odot\), \(\sigma = 0.55\), and \(m_b = 1\) \(M_\odot\).

Formally, the format of a basic mode file is as follows. Any line beginning with \code{\#} is a comment and is ignored. The first non-empty, non-comment line in a basic mode PDF file must be of the form:

\begin{Verbatim}[commandchars=\\\{\}]
breakpoints x1 x2 x3 ...
\end{Verbatim}

where \code{x1}, \code{x2}, \code{x3}, \code{...} are a non-decreasing series of real numbers. These represent the breakpoints that define the edges of the segment, in units of \(M_\odot\). In the example given above, the breakpoints are are \(0.08\), \(1\), and \(120\), indicating that the first segment goes from \(0.08 - 1\) \(M_\odot\), and the second from \(1 - 120\) \(M_\odot\).

After the \code{breakpoints} line, there must be a series of entries of the form:

\begin{Verbatim}[commandchars=\\\{\}]
segment
type TYPE
key1 VAL1
\PYG{g+gh}{key2 VAL2}
\PYG{g+gh}{...}
\end{Verbatim}

where \code{TYPE} specifies what functional form describes the segment, and \code{key1 VAL1}, \code{key2 VAL2}, etc. are a series of (key, value) pairs the define the free parameters for that segment. In the example above, the first segment is described as having a \code{lognormal} functional form, and the keywords \code{mean} and \code{disp} specify that the lognormal has a mean of 0.2 \(M_\odot\) and a dispersion of 0.55 in \(\log_{10}\). The second segment is of type \code{powerlaw}, and it has a slope of \(-2.35\). The full list of allowed segment types and the keywords that must be specified with them are listed in the {\hyperref[pdfs:tab\string-segtypes]{\crossref{\DUrole{std,std-ref}{Segment Types}}}} Table. Keywords and segment types are case-insensitive. Where more than one keyword is required, the order is arbitrary.

The total number of segments must be equal to one less than the number of breakpoints, so that each segment is described. Note that it is not necessary to specify a normalization for each segment, as the segments will be normalized relative to one another automatically so as to guarantee that the overall function is continuous.


\begin{threeparttable}
\capstart\caption{Segment Types}\label{pdfs:tab-segtypes}\label{pdfs:id1}
\noindent\begin{tabulary}{\linewidth}{|L|L|L|L|L|L|}
\hline
\textsf{\relax 
Name
} & \textsf{\relax 
Functional form
} & \textsf{\relax 
Keyword
} & \textsf{\relax 
Meaning
} & \textsf{\relax 
Keyword
} & \textsf{\relax 
Meaning
}\\
\hline
\code{delta}
 & 
\(\delta(x-x_a)\)
 &  &  &  & \\
\hline
\code{exponential}
 & 
\(\exp(-x/x_*)\)
 & 
\code{scale}
 & 
Scale length, \(x_*\)
 &  & \\
\hline
\code{lognormal}
 & 
\(x^{-1} \exp\{-[\log_{10}(x/x_0)]^2/2\sigma^2\}\)
 & 
\code{mean}
 & 
Mean, \(x_0\)
 & 
\code{disp}
 & 
Dispersion in \(\log_{10}\), \(\sigma\)
\\
\hline
\code{normal}
 & 
\(\exp[-(x-x_0)^2/2\sigma^2]\)
 & 
\code{mean}
 & 
Mean, \(x_0\)
 & 
\code{disp}
 & 
Dispersion, \(\sigma\)
\\
\hline
\code{powerlaw}
 & 
\(x^p\)
 & 
\code{slope}
 & 
Slope, \(p\)
 &  & \\
\hline
\code{schechter}
 & 
\(x^p \exp(-x/x_*)\)
 & 
\code{slope}
 & 
Slope, \(p\)
 & 
\code{xstar}
 & 
Cutoff, \(x_*\)
\\
\hline\end{tabulary}

\end{threeparttable}



\section{Variable Mode}
\label{pdfs:variable-mode}
Variable Mode works as an extension to Basic Mode. Instead of assigning a value to a parameter, you can define a PDF and then draw values for the parameter from it.

Formally, the format of a Variable Mode PDF file follows that of a Basic Mode PDF file, but with one addition. To specify a parameter as variable, the entry must be of the form:

\begin{Verbatim}[commandchars=\\\{\}]
key1 V path/to/pdf.vpar
\end{Verbatim}

with the \code{V} instructing the code that the parameter is variable. The \code{.vpar} files are formatted as if they are standard Basic Mode PDF files. Variable Mode is an extension of Basic Mode, and it is not supported in Advanced Mode PDF files.

Any number of parameters belonging to a PDF can be made to vary, and the distributions from which their values are drawn can be constructed from any combination of the PDF segment types specified for Basic Mode.

An example of a Variable Mode PDF file for the IMF is as follows:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}
\PYGZsh{} This is an IMF definition file for SLUG v2.
\PYG{g+gh}{\PYGZsh{} This file defines a power law PDF with variable slope}
\PYG{g+gh}{\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}}

\PYGZsh{} Breakpoints: mass values where the functional form changes
\PYGZsh{} The first and last breakpoint will define the minimum and
\PYGZsh{} maximum mass
breakpoints 0.08 120

\PYGZsh{} Definitions of segments between the breakpoints

\PYGZsh{} This segment is a powerlaw with slopes drawn from slope\PYGZus{}pdf
segment
type powerlaw
slope V lib/imf/slope\PYGZus{}pdf.vpar
\end{Verbatim}

An example of a parameter's PDF file is as follows:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}
\PYGZsh{} This is a parameter definition file for SLUG v2.
\PYG{g+gh}{\PYGZsh{} lib/imf/slope\PYGZus{}pdf.vpar}
\PYG{g+gh}{\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}}

\PYGZsh{} Breakpoints
breakpoints \PYGZhy{}3.0 \PYGZhy{}1.0

\PYGZsh{} Draw parameters from a normal distribution
segment
type normal
mean \PYGZhy{}2.35
disp 0.1
\end{Verbatim}

The above examples correspond to a powerlaw IMF with a slope varying between -3.0 and -1.0, with the value drawn from a normal distribution.

While the Variable Mode implementation is very general, it is currently only enabled for the IMF. The new parameter values are drawn at the start of each galaxy or cluster realisation.


\section{Advanced Mode}
\label{pdfs:advanced-mode}
In advanced mode, one has complete freedom to set all the parameters describing the PDF: the endpoints of each segment \(x_{i,a}\) and \(x_{i,b}\), the normalization of each segment \(n_i\), and the functional forms of each segment \(f_i\). This can be used to defined PDFs that are non-continuous, or that are overlapping; the latter option can be used to construct segments with nearly arbitrary functional forms, by constructing a Taylor series approximation to the desired functional form and then using a series of overlapping \code{powerlaw} segments to implement that series.

An example of an advanced mode PDF file is as follows:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}
\PYGZsh{} This is a SFH definition file for SLUG v2.
\PYGZsh{} This defines a SF history consisting of a series of
\PYGZsh{} exponentially\PYGZhy{}decaying bursts with a period of 100 Myr and
\PYGZsh{} a decay timescale of 10 Myr, with an amplitude chosen to
\PYG{g+gh}{\PYGZsh{} give a mean SFR of 10\PYGZca{}\PYGZhy{}3 Msun/yr.}
\PYG{g+gh}{\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}\PYGZsh{}}

\PYGZsh{} Declare that this is an advanced mode file
advanced

\PYGZsh{} First exponential burst
segment
type exponential
min      0.0
max      1.0e8         \PYGZsh{} Go to 100 Myr
weight   1.0e5         \PYGZsh{} Form 10\PYGZca{}5 Msun of stars over 100 Myr
scale    1.0e7         \PYGZsh{} Decay time 10 Myr

\PYGZsh{} Next 4 bursts
segment
type exponential
min      1.0e8
max      2.0e8
weight   1.0e5
scale    1.0e7

segment
type exponential
min      2.0e8
max      3.0e8
weight   1.0e5
scale    1.0e7

segment
type exponential
min      3.0e8
max      4.0e8
weight   1.0e5
scale    1.0e7

segment
type exponential
min      4.0e8
max      5.0e8
weight   1.0e5
scale    1.0e7
\end{Verbatim}

This represents a star formation history that is a series of exponential bursts, separated by 100 Myr, with decay times of 10 Myr. Formally, this SFH follows the functional form
\begin{equation*}
\begin{split}\dot{M}_* = n e^{-(t\,\mathrm{mod}\, P)/t_{\rm dec}},\end{split}
\end{equation*}
where \(P = 100\) Myr is the period and \(t_{\rm dec} = 10\) Myr is the decay time, from times \(0-500\) Myr. The normalization constant \(n\) is set by the condition that \((1/P) \int_0^P \dot{M}_* \,dt = 0.001\) \(M_\odot\;\mathrm{yr}^{-1}\), i.e., that the mean SFR averaged over a single burst period is 0.001 \(M_\odot\;\mathrm{yr}^{-1}\).

Formally, the format of an advanced mode file is as follows. First, all advanced mode files must start with the line:

\begin{Verbatim}[commandchars=\\\{\}]
advanced
\end{Verbatim}

to declare that the file is in advanced mode. After that, there must be a series of entries of the form:

\begin{Verbatim}[commandchars=\\\{\}]
segment
type TYPE
min MIN
max MAX
weight WEIGHT
key1 VAL1
\PYG{g+gh}{key2 VAL2}
\PYG{g+gh}{...}
\end{Verbatim}

The \code{type} keyword is exactly the same as in basic mode, as are the segment-specific parameter keywords \code{key1}, \code{key2}, \(\ldots\). The same functional forms, listed in the {\hyperref[pdfs:tab\string-segtypes]{\crossref{\DUrole{std,std-ref}{Segment Types}}}} Table, are available as in basic mode. The additional keywords that must be supplied in advanced mode are \code{min}, \code{max}, and \code{weight}. The \code{min} and \code{max} keywords give the upper and lower limits \(x_{i,a}\) and \(x_{i,b}\) for the segment; the probability is zero outside these limits. The keyword \code{weight} specifies the integral under the segment, i.e., the weight \(w_i\) given for segment \(i\) is used to set the normalization \(n_i\) via the equation
\begin{equation*}
\begin{split}w_i = n_i \int_{x_{i,a}}^{x_{i,b}} f_i(x) \, dx.\end{split}
\end{equation*}
In the case of a star formation history, as in the example above, the weight \(w_i\) of a segment is simply the total mass of stars formed in that segment. In the example given above, the first segment declaration sets up a PDF that with a minimum at 0 Myr, a maximum at 100 Myr, following an exponential functional form with a decay time of \(10^7\) yr. During this time, a total mass of \(10^5\) \(M_\odot\) of stars is formed.

Note that, for the IMF, CMF, and CLF, the absolute values of the weights to not matter, only their relative values. On the other hand, for the SFH, the absolute weight does matter.


\section{Sampling Methods}
\label{pdfs:sampling-methods}\label{pdfs:sampling-metod-label}
A final option allowed in both basic and advanced mode is a specification of the sampling method. The sampling method is a description of how to draw a population of objects from the PDF, when the population is specified as having a total sum \(M_{\rm target}\) (usually but not necessarily a total mass) rather than a total number of members \(N\); there are a number of ways to do this, which do not necessarily yield identical distributions, even for the same underlying PDF. To specify a sampling method, simply add the line:

\begin{Verbatim}[commandchars=\\\{\}]
method METHOD
\end{Verbatim}

to the PDF file. This line can appear anywhere except inside a \code{segment} specification, or before the \code{breakpoints} or \code{advanced} line that begins the file. The following values are allowed for \code{METHOD} (case-insensitive, as always):
\begin{itemize}
\item {} 
\code{stop\_nearest}: this is the default option: draw until the total mass of the population exceeds \(M_{\rm target}\). Either keep or exclude the final star drawn depending on which choice brings the total mass closer to the target value.

\item {} 
\code{stop\_before}: same as \code{stop\_nearest}, but the final object drawn is always excluded.

\item {} 
\code{stop\_after}: same as \code{stop\_nearest}, but the final object drawn is always kept.

\item {} 
\code{stop\_50}: same as \code{stop\_nearest}, but keep or exclude the final object with 50\% probability regardless of which choice gets closer to the target.

\item {} 
\code{number}: draw exactly \(N = M_{\rm target}/\langle M\rangle\) object, where \(\langle M\rangle\) is the expectation value for a single draw.

\item {} 
\code{poisson}: draw exactly \(N\) objects, where the value of \(N\) is chosen from a Poisson distribution with expectation value \(\langle N \rangle = M_{\rm target}/\langle M\rangle\)

\item {} 
\code{sorted\_sampling}: this method was introduced by \href{http://adsabs.harvard.edu/abs/2006MNRAS.365.1333W}{Weidner \& Kroupa (2006, MNRAS. 365, 1333)}, and proceeds in steps. One first draws exactly \(N= M_{\rm target}/\langle M\rangle\) as in the \code{number} method. If the resulting total mass \(M_{\rm pop}\) is less than \(M_{\rm target}\), the procedure is repeated recursively using a target mass \(M_{\rm target} - M_{\rm pop}\) until \(M_{\rm pop} > M_{\rm target}\). Finally, one sorts the resulting stellar list from least to most massive, and then keeps or removes the final, most massive star using a \code{stop\_nearest} policy.

\end{itemize}

See the file \code{lib/imf/wk06.imf} for an example of a PDF file with a \code{method} specification.


\chapter{Output Files and Format}
\label{output:output-files-and-format}\label{output::doc}\label{output:sec-output}
SLUG can produce 7 output files, though the actual number produced depends on the setting for the \code{out\_*} keywords in the parameter file. (Additional output files can be produced by {\hyperref[cloudy:sec\string-cloudy\string-slug]{\crossref{\DUrole{std,std-ref}{cloudy\_slug: An Automated Interface to cloudy}}}}, and are documented in that section rather than here.)

The only file that is always produced is the summary file, which is named \code{MODEL\_NAME\_summary.txt}, where \code{MODEL\_NAME} is the value given by the \code{model\_name} keyword in the parameter file. This file contains some basic summary information for the run, and is always formatted as ASCII text regardless of the output format requested.

The other eight output files all have names of the form \code{MODEL\_NAME\_xxx.ext}, where the extension \code{.ext} is one of \code{.txt}, \code{.bin}, or \code{.fits} depending on the \code{output\_mode} specified in the parameter file, and \code{xxx} is \code{integrated\_prop}, \code{integrated\_spec}, \code{integrated\_phot}, \code{integrated\_yield}, \code{cluster\_prop}, \code{cluster\_spec}, \code{cluster\_phot}, or \code{cluster\_yield}. The production of these output files is controlled by the parameters \code{out\_integrated}, \code{out\_integrated\_spec}, \code{out\_integrated\_phot}, \code{out\_integrated\_yield}, \code{out\_cluster}, \code{out\_cluster\_spec}, \code{out\_cluster\_phot}, and \code{out\_cluster\_yield} in the parameter file.

The easiest way to read these output files is with {\hyperref[slugpy:sec\string-slugpy]{\crossref{\DUrole{std,std-ref}{slugpy -- The Python Helper Library}}}}, which can parse them and store the information in python structures. However, for users who wish to write their own parsers or examine the data directly, the format is documented below. The following conventions are used throughout, unless noted otherwise:
\begin{itemize}
\item {} 
Masses are in \(M_\odot\)

\item {} 
Times in year

\item {} 
Wavelengths are in Angstrom

\item {} 
Specific luminosities are in erg/s/Angstrom

\item {} 
For \code{binary} outputs, variable types refer to C++ types

\end{itemize}


\section{The \texttt{integrated\_prop} File}
\label{output:the-integrated-prop-file}
This file contains data on the bulk physical properties of the galaxy as a whole. It consists of a series of entries containing the following fields:
\begin{itemize}
\item {} 
\code{Trial}: which trial these data are from

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{TargetMass}: target mass of stars in the galaxy up that time, if the IMF and SFH were perfectly sampled

\item {} 
\code{ActualMass}: actual mass of stars produced in the galaxy up to that time; generally not exactly equal to \code{TargetMass} due to finite sampling of the IMF and SFH

\item {} 
\code{LiveMass}:  current mass of all stars in the galaxy, accounting for the effects of stellar evolution (mass loss, supernovae); does not include the mass of stellar remnants (black holes, neutron stars, white dwarfs)

\item {} 
\code{StellarMass}: same as \code{LiveMass}, but including the mass of stellar remnants; at present, remnant mass calculation is hard-coded to use the initial-final mass relation of \href{http://adsabs.harvard.edu/abs/2009A\%26A...507.1409K}{Kruijssen (2009, A\&A, 507, 1409)}

\item {} 
\code{ClusterMass}: current mass of all stars in the galaxy that are presently in clusters; same as \code{LiveMass}, but only including the stars in clusters

\item {} 
\code{NumClusters}: number of non-disrupted clusters present in the galaxy at this time

\item {} 
\code{NumDisClust}: number of disrupted clusters present in the galaxy at this time

\item {} 
\code{NumFldStars}: number of field stars present in the galaxy at this time; this count only includes those stars being treated stochastically (see the parameter \code{min\_stoch\_mass} in {\hyperref[parameters:ssec\string-stellar\string-keywords]{\crossref{\DUrole{std,std-ref}{Stellar Model Keywords}}}})

\item {} 
\code{VPx}: values drawn for variable parameter \code{x} (0,1,2 etc...); present only if SLUG was run with a variable mode IMF

\end{itemize}

If \code{output\_mode} is \code{ascii}, these data are output in a series of columns, with different trials separated by lines of dashes. If \code{output\_mode} is \code{fits}, the data are stored as a FITS binary table extension, with one column for each of the variables above, plus an additional column giving the trial number for that entry. Both the ASCII- and FITS-formatted output should be fairly self-documenting.

For \code{binary} output, the file consists of a series of records containing the following variables
\begin{itemize}
\item {} 
\code{Trial} (\code{unsigned int})

\item {} 
\code{Time} (\code{double})

\item {} 
\code{TargetMass} (\code{double})

\item {} 
\code{ActualMass} (\code{double})

\item {} 
\code{LiveMass} (\code{double})

\item {} 
\code{StellarMass} (\code{double})

\item {} 
\code{ClusterMass} (\code{double})

\item {} 
\code{NumClusters} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long})

\item {} 
\code{NumDisClust} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long})

\item {} 
\code{NumFldStars} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long})

\item {} 
\code{VPx} (\code{double}); present only if \code{nvps} is greater than 0, with one entry present for each variable parameter \code{x}

\end{itemize}

The first record in the binary file indicates the number of variable parameters used.
\begin{itemize}
\item {} 
\code{nvps} (\code{integer}): the number of variable parameters for the IMF.

\end{itemize}

There is one record of this form for each output time, with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial.


\section{The \texttt{integrated\_spec} File}
\label{output:the-integrated-spec-file}\label{output:ssec-int-spec-file}
This file contains data on the spectra of the entire galaxy, and consists of a series of entries containing the following fields:
\begin{itemize}
\item {} 
\code{Trial}: which trial these data are from

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{Wavelength}: observed frame wavelength at which the stellar spectrum is evaluated

\item {} 
\code{L\_lambda}: specific luminosity at the specified wavelength, before extinction or nebular effects are applied

\item {} 
\code{Wavelength\_neb}: observed frame wavelength at which the stellar plus nebular spectrum is evaluated (present only if SLUG was run with nebular emission enabled)

\item {} 
\code{L\_lambda\_neb}: specific luminosity at the specified wavelength, after the light has been processed through the nebula (only present if SLUG was run with nebular emission enabled)

\item {} 
\code{Wavelength\_ex}: observed frame wavelength at which the extincted stellar spectrum is evaluated (present only if SLUG was run with extinction enabled)

\item {} 
\code{L\_lambda\_ex}: specific luminosity at the specified wavelength after extinction is applied, but without the effects of the nebula (only present if SLUG was run with extinction enabled)

\item {} 
\code{Wavelength\_neb\_ex}: observed frame wavelength at which the extincted stellar plus nebular spectrum is evaluated (present only if SLUG was run with nebular processing and  extinction enabled)

\item {} 
\code{L\_lambda\_neb\_ex}: specific luminosity at the specified wavelength, after the light is first processed by the nebular and then subjected to dust extinction (only present if SLUG was run with both extinction and nebular emission enabled)

\end{itemize}

If \code{output\_mode} is \code{ascii}, these data are output in a series of columns, with different trials separated by lines of dashes. In \code{ascii} mode, only a single \code{Wavelength} column is present, and for those wavelengths that are not included in one of the grids, some entries may be blank.

If \code{output\_mode} is \code{fits}, the output FITS file has two binary table extensions. The first table contains a field \code{Wavelength} listing the wavelengths at which the stellar spectra are given; if nebular emission was enabled in the SLUG calculation, there is also a field \code{Wavelength\_neb} giving the nebular wavelength grid, and if extinction was enabled the table has a field \code{Wavelength\_ex} listing the wavelengths at which the extincted spectrum is computed. If both nebular emission and extinction were included, the field \code{Wavelength\_neb\_ex} exists as well, giving the wavelength grid for that spectrum. The second table has three fields, \code{Trial}, \code{Time}, and \code{L\_lambda} giving the trial number, time, and stellar spectrum. It may also contain fields \code{L\_lambda\_neb}, \code{L\_lambda\_ex}, and \code{L\_lambda\_neb\_ex} giving the stellar plus nebular spectrum, extincted stellar spectrum, and extincted stellar plus nebular spectrum. Both the ASCII- and FITS-formatted output should be fairly self-documenting.

For binary output, the file is formatted as follows. The file starts with
\begin{itemize}
\item {} 
\code{Nebular} (\code{byte}): a single byte, with a value of 0 indicating that nebular processing was not enabled for this run, and a value of 1 indicating that it was enabled

\item {} 
\code{Extinct} (\code{byte}): a single byte, with a value of 0 indicating that extinction was not enabled for this run, and a value of 1 indicating that it was enabled

\item {} 
\code{NWavelength} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the stellar spectra

\item {} 
\code{Wavelength} (\code{NWavelength} entries of type \code{double})

\item {} 
\code{NWavelength\_neb} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the stellar plus nebular spectra; only present if \code{Nebular} is 1

\item {} 
\code{Wavelength\_neb} (\code{NWavelength\_neb} entries of type \code{double})

\item {} 
\code{NWavelength\_ex} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the extincted spectra; only present if \code{Extinct} is 1

\item {} 
\code{Wavelength\_ex} (\code{NWavelength\_ex} entries of type \code{double}); only present if \code{Extinct} is 1

\item {} 
\code{NWavelength\_neb\_ex} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the extincted nebular plus stellar spectra; only present if \code{Nebular} and \code{Extinct} are both 1

\item {} 
\code{Wavelength\_ex} (\code{NWavelength\_neb\_ex} entries of type \code{double}); only present if \code{Nebular} and \code{Extinct} are both 1

\end{itemize}

and then contains a series of records in the format
\begin{itemize}
\item {} 
\code{Trial} (\code{unsigned int})

\item {} 
\code{Time} (\code{double})

\item {} 
\code{L\_lambda} (\code{NWavelength} entries of type \code{double})

\item {} 
\code{L\_lambda\_neb} (\code{NWavelength\_neb} entries of type \code{double}); only present if \code{Nebular} is 1

\item {} 
\code{L\_lambda\_ex} (\code{NWavelength\_ex} entries of type \code{double}); only present if \code{Extinct} is 1

\item {} 
\code{L\_lambda\_neb\_ex} (\code{NWavelength\_neb\_ex} entries of type \code{double}); only present if \code{Nebular} and \code{Extinct} are both 1

\end{itemize}

There is one such record for each output time, with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial.


\section{The \texttt{integrated\_phot} File}
\label{output:the-integrated-phot-file}\label{output:ssec-int-phot-file}
This file contains data on the photometric properties of the entire galaxy, and consists of a series of entries containing the following fields:
\begin{itemize}
\item {} 
\code{Trial}: which trial these data are from

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{PhotFilter1}: photometric value through filter 1, where filters follow the order in which they are specified by the \code{phot\_bands} keyword; units depend on the value of \code{phot\_mode} (see {\hyperref[parameters:ssec\string-phot\string-keywords]{\crossref{\DUrole{std,std-ref}{Photometric Filter Keywords}}}})

\item {} 
\code{PhotFilter2}

\item {} 
\code{PhotFilter3}

\item {} 
\code{...}

\item {} 
\code{PhotFilter1\_neb}: photometric value through filter 1 for the spectrum after nebular processing, in the same units as \code{PhotFilter1}; only present if SLUG was run with nebular processing enabled

\item {} 
\code{PhotFilter2\_neb}

\item {} 
\code{PhotFilter3\_neb}

\item {} 
\code{...}

\item {} 
\code{PhotFilter1\_ex}: photometric value through filter 1 for the extincted spectrum, in the same units as \code{PhotFilter1}; only present if SLUG was run with extinction enabled

\item {} 
\code{PhotFilter2\_ex}

\item {} 
\code{PhotFilter3\_ex}

\item {} 
\code{...}

\item {} 
\code{PhotFilter1\_neb\_ex}: photometric value through filter 1 for the spectrum after nebular processing and extinction, in the same units as \code{PhotFilter1}; only present if SLUG was run with both nebular processing and extinction enabled

\item {} 
\code{PhotFilter2\_neb\_ex}

\item {} 
\code{PhotFilter3\_neb\_ex}

\item {} 
\code{...}

\end{itemize}

If \code{output\_mode} is \code{ascii}, these data are output in a series of
columns, with different trials separated by lines of dashes. The
columns for photometry of the extincted spectrum are present only if
extinction was enabled when SLUG was run. Entries for some filters may
be left blank. If so, this indicates that the photon response function
provided for that filter extends beyond the wavelength range covered
by the provided extinction curve. Since the extincted spectrum cannot
be computed over the full range of the filter in this case, photometry
for that filter cannot be computed either.

If \code{output\_mode} is \code{fits}, the data are stored as a series of
columns in a binary table extension to the FITS file; the filter names
and units are included in the header information for the columns. If
SLUG was run with nebular emission enabled, for each filter \code{FILTERNAME}
there is a corresponding column \code{FILTERNAME\_neb} giving the photometric
value for the nebular-processed spectrum. Similarly, the columns
\code{FILTERNAME\_ex} and \code{FILTERNAME\_neb\_ex} give the photometric values
for the extincted stellar and stellar + nebular spectra, respectively.
Some of the extincted values may be \code{NaN}; this
indicates that the photon response function provided for that filter
extends beyond the wavelength range covered by the provided extinction
curve. In addition to the time and photometric filter values, the FITS
file contains a column specifying the trial number for that
entry. Both the ASCII- and FITS-formatted output should be fairly
self-documenting.

For binary output, the file is formatted as follows. The file starts with
\begin{itemize}
\item {} 
\code{NFilter} (stored as \code{ASCII text}): number of filters used

\item {} 
\code{FilterName} \code{FilterUnit} (\code{NFilter} entries stored as \code{ASCII
text}): the name and units for each filter are listed in ASCII, one
filter-unit pair per line

\item {} 
\code{Nebular} (\code{byte}): a single byte, with a value of 0 indicating
that nebular processing was not enabled for this run, and a value of 1
indicating that it was enabled

\item {} 
\code{Extinct} (\code{byte}): a single byte, with a value of 0 indicating
that extinction was not enabled for this run, and a value of 1
indicating that it was enabled

\end{itemize}

This is followed by a series of entries of the form
\begin{itemize}
\item {} 
\code{Trial} (\code{unsigned int})

\item {} 
\code{Time} (\code{double})

\item {} 
\code{PhotFilter} (\code{NFilter} entries of type \code{double})

\item {} 
\code{PhotFilter\_neb} (\code{NFilter} entries of type \code{double}); only present if \code{Nebular} is 1.

\item {} 
\code{PhotFilter\_ex} (\code{NFilter} entries of type \code{double}); only present if \code{Extinct} is 1. Note that some values may be \code{NaN} if photometry could not be computed for that filter (see above).

\item {} 
\code{PhotFilter\_neb\_ex} (\code{NFilter} entries of type \code{double}); only present if \code{Nebular} and \code{Extinct} are both 1. Note that some values may be \code{NaN} if photometry could not be computed for that filter (see above).

\end{itemize}

There is one such record for each output time, with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial.


\section{The \texttt{integrated\_yield} File}
\label{output:the-integrated-yield-file}\label{output:ssec-int-yield-file}
This file contains data on the integrated chemical yield of the entire
galaxy, and consists of a series of entries containing the following
fields:
\begin{itemize}
\item {} 
\code{Trial}: which trial these data are from

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{Name}: name (i.e., atomic symbol) of an isotope being produced

\item {} 
\code{Z}: atomic number of an isotope being produced

\item {} 
\code{A}: mass number of an isotope being produced

\item {} 
\code{Yield}: mass of a particular isotope produced up to the specified time; for unstable isotopes, this includes the effects of radioactive decay, so yield can decrease with time under some circumstances

\end{itemize}

If \code{output\_mode} is \code{ascii}, these data are output in a series of
columns, with different trials separated by lines of dashes. If
\code{output\_mode} is \code{fits}, the data are stored as a series of
columns in a binary table extension to the FITS file; the \code{Name},
\code{Z}, and \code{A} fields are placed in the first binary table
extension, and are the same for every output. The \code{Time} and
\code{Yields} fields are in the second binary table extension. In
addition to these two fields, the second binary table contains a
column specifying the trial number for each entry.

For binary output, the file is formatted as follows. It starts with
\begin{itemize}
\item {} 
\code{NIso} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned
long long}): number of isotopes in the output

\end{itemize}

This is followed by \code{NIso} entries of the form
\begin{itemize}
\item {} 
\code{Name} (\code{char{[}4{]}}): isotope name (i.e., element symbol)

\item {} 
\code{Z} (\code{unsigned int})

\item {} 
\code{A} (\code{unsigned int})

\end{itemize}

The remainder of the file contains records of the from
\begin{itemize}
\item {} 
\code{Trial} (\code{unsigned int})

\item {} 
\code{Time} (\code{double})

\item {} 
\code{Yield} (\code{double{[}NIso{]}})

\end{itemize}

There is one such record for each output time, with different trials
ordered sequentially, so that all the times for one trial are output
before the first time for the next trial.


\section{The \texttt{cluster\_prop} File}
\label{output:the-cluster-prop-file}
This file contains data on the bulk physical properties of the non-disrupted star clusters in the galaxy, with one entry per cluster per time at which that cluster exists. Each entry contains the following fields
\begin{itemize}
\item {} 
\code{UniqueID}: a unique identifier number for each cluster that is preserved across times and output files

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{FormTime}: time at which that cluster formed

\item {} 
\code{Lifetime}: amount of time from birth to when the cluster will disrupt

\item {} 
\code{TargetMass}: target mass of stars in the cluster, if the IMF were perfectly sampled

\item {} 
\code{BirthMass}: actual mass of stars present in the cluster at formation

\item {} 
\code{LiveMass}: current mass of all stars in the cluster, accounting for the effects of stellar evolution (mass loss, supernovae); does not include the mass of stellar remnants (black holes, neutron stars, white dwarfs)

\item {} 
\code{StellarMass}: same as \code{LiveMass}, but including the mass of stellar remnants; at present, remnant mass calculation is hard-coded to use the initial-final mass relation of \href{http://adsabs.harvard.edu/abs/2009A\%26A...507.1409K}{Kruijssen (2009, A\&A, 507, 1409)}

\item {} 
\code{NumStar}: number of living stars in the cluster at this time; this count only includes those stars being treated stochastically (see the parameter \code{min\_stoch\_mass} in {\hyperref[parameters:ssec\string-stellar\string-keywords]{\crossref{\DUrole{std,std-ref}{Stellar Model Keywords}}}})

\item {} 
\code{MaxStarMass}: mass of most massive star still living in the cluster; this only includes those stars being treated stochastically (see the parameter \code{min\_stoch\_mass} in {\hyperref[parameters:ssec\string-stellar\string-keywords]{\crossref{\DUrole{std,std-ref}{Stellar Model Keywords}}}})

\item {} 
\code{A\_V}: visual extinction for that cluster, in mag; present only if SLUG was run with extinction enabled

\item {} 
\code{VPx}: values drawn for variable parameter \code{x} (0,1,2 etc...); present only if SLUG was run with a variable mode IMF

\end{itemize}

If \code{output\_mode} is \code{ascii}, these data are output in a series of columns, with different trials separated by lines of dashes. If \code{output\_mode} is \code{fits}, the data are stored as a FITS binary table extension, with one column for each of the variables above, plus an additional column giving the trial number for that entry. Both the ASCII- and FITS-formatted output should be fairly self-documenting.

For \code{binary} output, the first entry in the file is a header containing
\begin{itemize}
\item {} 
\code{Extinct} (\code{byte}): a single byte, with a value of 0 indicating that extinction was not enabled for this run, and a value of 1 indicating that it was enabled

\item {} 
\code{nvps} (\code{integer}): the number of variable parameters for the IMF.

\end{itemize}

Thereafter, the file consists of a series of records, one for each output time, with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial. Each record consists of a header containing
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{NCluster} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): number of non-disrupted clusters present at this time

\end{itemize}

This is followed by \code{NCluster} entries of the following form:
\begin{itemize}
\item {} 
\code{UniqueID} (\code{unsigned long})

\item {} 
\code{FormationTime} (\code{double})

\item {} 
\code{Lifetime} (\code{double})

\item {} 
\code{TargetMass} (\code{double})

\item {} 
\code{BirthMass} (\code{double})

\item {} 
\code{LiveMass} (\code{double})

\item {} 
\code{StellarMass} (\code{double})

\item {} 
\code{NumStar} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long})

\item {} 
\code{MaxStarMass} (\code{double})

\item {} 
\code{A\_V} (\code{double}); present only if \code{Extinct} is 1

\item {} 
\code{VPx} (\code{double}); present only if \code{nvps} is greater than 0, with one entry present for each variable parameter \code{x}

\end{itemize}


\section{The \texttt{cluster\_spec} File}
\label{output:the-cluster-spec-file}
This file contains the spectra of the individual clusters, and each entry contains the following fields:
\begin{itemize}
\item {} 
\code{UniqueID}: a unique identifier number for each cluster that is preserved across times and output files

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{Wavelength}: observed frame wavelength at which the stellar spectrum is evaluated

\item {} 
\code{L\_lambda}: specific luminosity at the specified wavelength, before extinction or nebular effects are applied

\item {} 
\code{Wavelength\_neb}: observed frame wavelength at which the stellar plus nebular spectrum is evaluated (present only if SLUG was run with nebular emission enabled)

\item {} 
\code{L\_lambda\_neb}: specific luminosity at the specified wavelength, after the light has been processed through the nebula (only present if SLUG was run with nebular emission enabled)

\item {} 
\code{Wavelength\_ex}: observed frame wavelength at which the extincted stellar spectrum is evaluated (present only if SLUG was run with extinction enabled)

\item {} 
\code{L\_lambda\_ex}: specific luminosity at the specified wavelength after extinction is applied, but without the effects of the nebula (only present if SLUG was run with extinction enabled)

\item {} 
\code{Wavelength\_neb\_ex}: observed frame wavelength at which the extincted stellar plus nebular spectrum is evaluated (present only if SLUG was run with nebular processing and  extinction enabled)

\item {} 
\code{L\_lambda\_neb\_ex}: specific luminosity at the specified wavelength, after the light is first processed by the nebular and then subjected to dust extinction (only present if SLUG was run with both extinction and nebular emission enabled)

\end{itemize}

If \code{output\_mode} is \code{ascii}, these data are output in a series of columns, with different trials separated by lines of dashes. The columns \code{L\_lambda\_neb}, \code{L\_lambda\_ex}, and \code{L\_lambda\_neb\_ex} are present only if SLUG was run with the appropriate options enabled. Some entries in these fields may be empty; see {\hyperref[output:ssec\string-int\string-spec\string-file]{\crossref{\DUrole{std,std-ref}{The integrated\_spec File}}}}.

If \code{output\_mode} is \code{fits}, the output FITS file has two binary table extensions. The first table contains a field listing the wavelengths at which the spectra are given, in the same format as for {\hyperref[output:ssec\string-int\string-spec\string-file]{\crossref{\DUrole{std,std-ref}{The integrated\_spec File}}}}. The second table has always contains the fields \code{UniqueId}, \code{Time}, \code{Trial}, and \code{L\_lambda} giving the cluster unique ID, time, trial number, and stellar spectrum. Depending on whether nebular processing and/or extinction were enabled when SLUG was run, it may also contain the fields \code{L\_lambda\_neb}, \code{L\_lambda\_ex}, and \code{L\_lambda\_neb\_ex} giving the nebular-processed, extincted, and nebular-processed plus extincted spectra. Both the ASCII- and FITS-formatted output should be fairly self-documenting.

Output in \code{binary} mode is formatted as follows.  The file starts with
\begin{itemize}
\item {} 
\code{Nebular} (\code{byte}): a single byte, with a value of 0 indicating that nebular processing was not enabled for this run, and a value of 1 indicating that it was enabled

\item {} 
\code{Extinct} (\code{byte}): a single byte, with a value of 0 indicating that extinction was not enabled for this run, and a value of 1 indicating that it was enabled

\item {} 
\code{NWavelength} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the stellar spectra

\item {} 
\code{Wavelength} (\code{NWavelength} entries of type \code{double})

\item {} 
\code{NWavelength\_neb} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the stellar plus nebular spectra; only present if \code{Nebular} is 1

\item {} 
\code{Wavelength\_neb} (\code{NWavelength\_neb} entries of type \code{double})

\item {} 
\code{NWavelength\_ex} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the extincted spectra; only present if \code{Extinct} is 1

\item {} 
\code{Wavelength\_ex} (\code{NWavelength\_ex} entries of type \code{double}); only present if \code{Extinct} is 1

\item {} 
\code{NWavelength\_neb\_ex} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): the number of wavelength entries in the extincted nebular plus stellar spectra; only present if \code{Nebular} and \code{Extinct} are both 1

\item {} 
\code{Wavelength\_ex} (\code{NWavelength\_neb\_ex} entries of type \code{double}); only present if \code{Nebular} and \code{Extinct} are both 1

\end{itemize}

and then contains a series of records, one for each output time, with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial. Each record consists of a header containing
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{NCluster} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): number of non-disrupted clusters present at this time

\end{itemize}

This is followed by \code{NCluster} entries of the following form:
\begin{itemize}
\item {} 
\code{UniqueID} (\code{unsigned long})

\item {} 
\code{L\_lambda} (\code{NWavelength} entries of type \code{double})

\item {} 
\code{L\_lambda\_neb} (\code{NWavelength\_neb} entries of type \code{double}); only present if \code{Nebular} is 1

\item {} 
\code{L\_lambda\_ex} (\code{NWavelength\_ex} entries of type \code{double}); only present if \code{Extinct} is 1

\item {} 
\code{L\_lambda\_neb\_ex} (\code{NWavelength\_neb\_ex} entries of type \code{double}); only present if \code{Nebular} and \code{Extinct} are both 1

\end{itemize}


\section{The \texttt{cluster\_phot} File}
\label{output:the-cluster-phot-file}\label{output:ssec-cluster-phot-file}
This file contains the photometric values for the individual clusters. Each entry contains the following fields:
\begin{itemize}
\item {} 
\code{UniqueID}: a unique identifier number for each cluster that is preserved across times and output files

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{PhotFilter1}: photometric value through filter 1, where filters follow the order in which they are specified by the \code{phot\_bands} keyword; units depend on the value of \code{phot\_mode} (see {\hyperref[parameters:ssec\string-phot\string-keywords]{\crossref{\DUrole{std,std-ref}{Photometric Filter Keywords}}}})

\item {} 
\code{PhotFilter2}

\item {} 
\code{PhotFilter3}

\item {} 
\code{...}

\item {} 
\code{PhotFilter1\_neb}: photometric value through filter 1 for the spectrum after nebular processing, in the same units as \code{PhotFilter1}; only present if SLUG was run with nebular processing enabled

\item {} 
\code{PhotFilter2\_neb}

\item {} 
\code{PhotFilter3\_neb}

\item {} 
\code{...}

\item {} 
\code{PhotFilter1\_ex}: photometric value through filter 1 for the extincted spectrum, in the same units as \code{PhotFilter1}; only present if SLUG was run with extinction enabled

\item {} 
\code{PhotFilter2\_ex}

\item {} 
\code{PhotFilter3\_ex}

\item {} 
\code{...}

\item {} 
\code{PhotFilter1\_neb\_ex}: photometric value through filter 1 for the spectrum after nebular processing and extinction, in the same units as \code{PhotFilter1}; only present if SLUG was run with both nebular processing and extinction enabled

\item {} 
\code{PhotFilter2\_neb\_ex}

\item {} 
\code{PhotFilter3\_neb\_ex}

\item {} 
\code{...}

\end{itemize}

If \code{output\_mode} is \code{ascii}, these data are output in a series of columns, with different trials separated by lines of dashes. Some of the extincted photometry columns may be blank; see {\hyperref[output:ssec\string-int\string-phot\string-file]{\crossref{\DUrole{std,std-ref}{The integrated\_phot File}}}}.

If \code{output\_mode} is \code{fits}, the data are stored as a series of
columns in a binary table extension to the FITS file; the filter names
and units are included in the header information for the columns. If
SLUG was run with nebular emission enabled, for each filter \code{FILTERNAME}
there is a corresponding column \code{FILTERNAME\_neb} giving the photometric
value for the nebular-processed spectrum. Similarly, the columns
\code{FILTERNAME\_ex} and \code{FILTERNAME\_neb\_ex} give the photometric values
for the extincted stellar and stellar + nebular spectra, respectively.
Some of the extincted values may be \code{NaN}; this
indicates that the photon response function provided for that filter
extends beyond the wavelength range covered by the provided extinction
curve. In addition to the time and photometric filter values, the FITS
file contains a column specifying the trial number for that
entry. Both the ASCII- and FITS-formatted output should be fairly
self-documenting.

In \code{binary} output mode, the binary data file starts with
\begin{itemize}
\item {} 
\code{NFilter} (stored as \code{ASCII text}): number of filters used

\item {} 
\code{FilterName} \code{FilterUnit} (\code{NFilter} entries stored as \code{ASCII text}): the name and units for each filter are listed in ASCII, one filter-unit pair per line

\item {} 
\code{Nebular} (\code{byte}): a single byte, with a value of 0 indicating that nebular processing was not enabled for this run, and a value of 1 indicating that it was enabled

\item {} 
\code{Extinct} (\code{byte}): a single byte, with a value of 0 indicating that extinction was not enabled for this run, and a value of 1 indicating that it was enabled

\end{itemize}

and then contains a series of records, one for each output time , with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial. Each record consists of a header containing
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{NCluster} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): number of non-disrupted clusters present at this time

\end{itemize}

This is followed by \code{NCluster} entries of the following form:
\begin{itemize}
\item {} 
\code{UniqueID} (\code{unsigned long})

\item {} 
\code{PhotFilter} (\code{NFilter} entries of type \code{double})

\item {} 
\code{PhotFilter\_neb} (\code{NFilter} entries of type \code{double}); only present if \code{Nebular} is 1.

\item {} 
\code{PhotFilter\_ex} (\code{NFilter} entries of type \code{double}); only present if \code{Extinct} is 1. Note that some values may be \code{NaN} if photometry could not be computed for that filter (see above).

\item {} 
\code{PhotFilter\_neb\_ex} (\code{NFilter} entries of type \code{double}); only present if \code{Nebular} and \code{Extinct} are both 1. Note that some values may be \code{NaN} if photometry could not be computed for that filter (see above).

\end{itemize}


\section{The \texttt{cluster\_yield} File}
\label{output:ssec-cluster-yield-file}\label{output:the-cluster-yield-file}
This file contains data on the chemical yield of individual star
clusters, and consists of a series of entries containing the following
fields:
\begin{itemize}
\item {} 
\code{UniqueID}: a unique identifier number for each cluster that is
preserved across times and output files

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{Name}: name (i.e., atomic symbol) of an isotope being produced

\item {} 
\code{Z}: atomic number of an isotope being produced

\item {} 
\code{A}: mass number of an isotope being produced

\item {} 
\code{Yield}: mass of a particular isotope produced up to the specified
time; for unstable isotopes, this includes the effects of
radioactive decay, so yield can decrease with time under some
circumstances

\end{itemize}

If \code{output\_mode} is \code{ascii}, these data are output in a series of
columns, with different trials separated by lines of dashes. If
\code{output\_mode} is \code{fits}, the data are stored as a series of
columns in a binary table extension to the FITS file; the \code{Name},
\code{Z}, and \code{A} fields are placed in the first binary table
extension, and are the same for every output. The \code{Time} and
\code{Yields} fields are in the second binary table extension. In
addition to these two fields, the second binary table contains a
column specifying the trial number for each entry.

For binary output, the file is formatted as follows. It starts with
\begin{itemize}
\item {} 
\code{NIso} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned
long long}): number of isotopes in the output

\end{itemize}

This is followed by \code{NIso} entries of the form
\begin{itemize}
\item {} 
\code{Name} (\code{char{[}4{]}}): isotope name (i.e., element symbol)

\item {} 
\code{Z} (\code{unsigned int})

\item {} 
\code{A} (\code{unsigned int})

\end{itemize}

Thereafter, the file consists of a series of records, one for each output time, with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial. Each record consists of a header containing
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{NCluster} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): number of non-disrupted clusters present at this time

\end{itemize}

This is followed by \code{NCluster} entries of the following form:
\begin{itemize}
\item {} 
\code{UniqueID} (\code{unsigned long})

\item {} 
\code{Yield} (\code{double{[}NIso{]}})

\end{itemize}

There is one such record for each output time, with different trials
ordered sequentially, so that all the times for one trial are output
before the first time for the next trial.


\section{Checkpoint Files}
\label{output:ssec-checkpoint-files}\label{output:checkpoint-files}
Checkpoint files are identical to regular output files, except that
they begin with a statement of the number of trials they contain. For
ASCII files, this is indicated by a first line of the form \code{N\_Trials =
N}. For binary files, the file begins with an unsigned integer that
gives the number of trials. For FITS files, the file has a keyword
\code{N\_Trials} in the first binary table HDU that gives the number of
trials.

The slugpy library can read checkpoint files as well as regular output
files (see {\hyperref[slugpy:sec\string-slugpy]{\crossref{\DUrole{std,std-ref}{slugpy -- The Python Helper Library}}}}).


\chapter{Filters and Filter Data}
\label{filters:sec-filters}\label{filters::doc}\label{filters:filters-and-filter-data}
SLUG comes with a fairly extensive list of filters, adapted from the list maintained by Charlie Conroy as part of \href{https://code.google.com/p/fsps/}{fsps}. However, users may wish to add additional filters, and so the format of the filter list is documented here for convenience.

Filter data is stored in two ASCII text files, \code{FILTER\_LIST} and \code{allfilters.dat}, which are stored in the \code{lib/filters} directory. The \code{FILTER\_LIST} file is an index listing the available filters. In consists of five whitespace-separated columns. The first column is just an numerical index. The second is the name of the filter; this is the name that should be entered in the \code{phot\_bands} keyword (see {\hyperref[parameters:ssec\string-phot\string-keywords]{\crossref{\DUrole{std,std-ref}{Photometric Filter Keywords}}}}) to request photometry in that filter. The third and fourth columns the value of \(\beta\) and \(\lambda_c\) (the central wavelength) for that filter -- see {\hyperref[intro:ssec\string-spec\string-phot]{\crossref{\DUrole{std,std-ref}{Spectra and Photometry}}}} for definitions. Anything after the fourth column is regarded as a comment, and can be used freely for a description of that filter.

The \code{allfilters.dat} file contains the filter responses. The file contains a series of entires for different filters, each delineated by a header line that begins with \code{\#}. The order in which filters appear in this file matches that in which they appear in the \code{FILTER\_LIST}. After the header line, are a series of lines each containing two numbers. The first is the wavelength in Angstrom, and the second is the filter response function at that wavelength.


\chapter{slugpy -- The Python Helper Library}
\label{slugpy:slugpy-the-python-helper-library}\label{slugpy:sec-slugpy}\label{slugpy::doc}

\section{Installing slugpy}
\label{slugpy:installing-slugpy}
SLUG comes with the python module slugpy, which contains an extensive set of routines for reading, writing, and manipulating SLUG outputs. You can install slugpy one of two ways.
\begin{enumerate}
\item {} 
Using \code{Make}. If you compile the main slug code by doing \code{Make} in the main slug directory, the c slugpy extensions will be build automatically. Once that is done, you will be able to use slugpy just by importing it, provided that the slugpy directory is in your python import path.

\item {} 
Using \code{setup.py}. The slug distribution comes with a \code{setup.py} script that follows the standard python package convensions. Just do:

\begin{Verbatim}[commandchars=\\\{\}]
python setup.py build
\end{Verbatim}

\end{enumerate}

to build the c extensions in place, which will let you import slugpy from the directory where it is located. Alternately, do:

\begin{Verbatim}[commandchars=\\\{\}]
python setup.py install
\end{Verbatim}

to install as a site package. Installing as a site package often requires root permissions, in which case you should do:

\begin{Verbatim}[commandchars=\\\{\}]
sudo python setup.py install
\end{Verbatim}

or:

\begin{Verbatim}[commandchars=\\\{\}]
python setup.py install \PYGZhy{}\PYGZhy{}user
\end{Verbatim}

instead.

A note on compiling slugpy with setup.py: the slugpy c extensions require the \href{http://www.gnu.org/software/gsl/}{GNU Scientific Library}; to build or install slugpy, the appropriate headers must be in your default include path, and the appropriate libraries in your default link path. If they are not, you can tell setup where they are located by creating a file named \code{setup.cfg} in the slug2 directory, which contains the lines:

\begin{Verbatim}[commandchars=\\\{\}]
[build\PYGZus{}ext]
include\PYGZus{}dirs=/PATH/TO/GSL/HEADER
library\PYGZus{}dirs=/PATH/TO/GSL/LIBRARIES
\end{Verbatim}

Then you should be able to build and install slugpy with \code{setup.py}.


\section{Basic Usage}
\label{slugpy:basic-usage}
The most common task is to read a set of SLUG outputs into memory so that they can be processed. To read the data from a SLUG run using slugpy, one can simply do the following:

\begin{Verbatim}[commandchars=\\\{\}]
from slugpy import *
idata = read\PYGZus{}integrated(\PYGZsq{}SLUG\PYGZus{}MODEL\PYGZus{}NAME\PYGZsq{})
cdata = read\PYGZus{}cluster(\PYGZsq{}SLUG\PYGZus{}MODEL\PYGZus{}NAME\PYGZsq{})
\end{Verbatim}

The \code{read\_integrated} function reads all the integrated-light data (i.e., the data stored in the \code{\_integrated\_*} files -- see {\hyperref[output:sec\string-output]{\crossref{\DUrole{std,std-ref}{Output Files and Format}}}}) for a SLUG output whose name is given as the argument. This is the base name specified by the \code{model\_name} keyword (see {\hyperref[parameters:ssec\string-basic\string-keywords]{\crossref{\DUrole{std,std-ref}{Basic Keywords}}}}), without any extensions; the slugpy library will automatically determine which outputs are available and in what format, and read the appropriate files. It returns a \code{namedtuple} containing all the output data available for that simulation. Note that some of these fields will only be present if the cloudy-slug interface (see {\hyperref[cloudy:sec\string-cloudy\string-slug]{\crossref{\DUrole{std,std-ref}{cloudy\_slug: An Automated Interface to cloudy}}}}) was used to process the SLUG output through cloudy to predict nebular emission, and some will be present only if extinction was enabled when SLUG was run. The fields returned are as follows:
\begin{itemize}
\item {} 
time: output times

\item {} 
target\_mass: target stellar mass at each time

\item {} 
actual\_mass: actual stellar mass at each time

\item {} 
live\_mass: mass of currently-alive stars

\item {} 
cluster\_mass: mass of living stars in non-disrupted clusters

\item {} 
num\_clusters: number of non-disrupted clusters

\item {} 
num\_dis\_clusters: number of disrupted clusters

\item {} 
num\_fld\_stars: number of still-living stars that formed in the field

\item {} 
wl: wavelengths of output stellar spectra (in Angstrom)

\item {} 
spec: integrated spectrum of all stars, expressed as a specific luminosity (erg/s/Angstrom)

\item {} 
filter\_names: list of photometric filter names

\item {} 
filter\_units: list of units for photometric outputs

\item {} 
filter\_wl\_eff: effective wavelength for each photometric filter

\item {} 
filter\_wl: list of wavelengths for each filter at which the response function is given (in Angstrom)

\item {} 
filter\_response: photon response function for each filter at each wavelength (dimensionless)

\item {} 
filter\_beta: index \(\beta\) used to set the normalization for each filter -- see {\hyperref[intro:ssec\string-spec\string-phot]{\crossref{\DUrole{std,std-ref}{Spectra and Photometry}}}}

\item {} 
filter\_wl\_c: pivot wavelength used to set the normalization for each filter for which \(\beta \neq 0\) -- see {\hyperref[intro:ssec\string-spec\string-phot]{\crossref{\DUrole{std,std-ref}{Spectra and Photometry}}}}

\item {} 
phot: photometry of the stars in each filter

\item {} 
isotope\_name: element symbols for the isotopes whose yields are reported

\item {} 
isotope\_Z: atomic numbers for the isotopes whose yields are reported

\item {} 
isotope\_A: atomic numbers for the isotopes whose yields are reported

\item {} 
yld: yield of each isotope at each time

\end{itemize}

The following fields are present only if SLUG was run with nebular processing enabled:
\begin{itemize}
\item {} 
wl\_neb: same as wl, but for the spectrum that emerges after the starlight has passed through the nebulae around the emitting clusters and field stars. The nebular grid is finer than the stellar grid, because it contains wavelength extra entries around prominent lines so that the lines are resolved on the grid

\item {} 
spec\_neb: same as spec, but for the nebular-processed spectrum

\item {} 
phot\_neb: same as phot, but for the nebular-processed spectrum

\end{itemize}

The following fields are present only if SLUG was run with extinction enabled:
\begin{itemize}
\item {} 
wl\_ex: wavelengths of output stellar spectra after extinction has been applied(in Angstrom). Note that wl\_ex will generally cover a smaller wavelength range than wl, because the extinction curve used may not cover the full wavelength range of the stellar spectra. Extincted spectra are computed only over the range covered by the extinction curve.

\item {} 
spec\_ex: same as spec, but for the extincted spectrum

\item {} 
phot\_ex: same as phot, but for the extincted spectrum. Note that some values may be \code{NaN}. This indicates that photometry of the extincted spectrum could not be computed for that filter, because the filter response curve extends to wavelengths outside the range covered by the extinction curve.

\end{itemize}

The following fields are present only if SLUG was run with both nebular processing and extinction enabled:
\begin{itemize}
\item {} 
wl\_neb\_ex: same as wl\_neb, but for the extincted, nebular-processed spectrum. Will be limited to the same wavelength range as wl\_ex.

\item {} 
spec\_neb\_ex: same as spec\_neb, but with extinction applied

\item {} 
phot\_neb\_ex: same as phot\_neb, but wtih extinction applied. Note that some values may be \code{NaN}. This indicates that photometry of the extincted spectrum could not be computed for that filter, because the filter response curve extends to wavelengths outside the range covered by the extinction curve.

\end{itemize}

The following fields are present only for runs that have been processed through the cloudy\_slug interface (see {\hyperref[cloudy:sec\string-cloudy\string-slug]{\crossref{\DUrole{std,std-ref}{cloudy\_slug: An Automated Interface to cloudy}}}}):
\begin{itemize}
\item {} 
cloudy\_wl: wavelengths of the output nebular spectra (in Angstrom)

\item {} 
cloudy\_inc: incident stellar radiation field, expressed as a specific luminosity (erg/s/Angstrom) -- should be the same as spec, but binned onto cloudy's wavelength grid; provided mainly as a bug-checking diagnostic

\item {} 
cloudy\_trans: the transmitted stellar radiation field computed by cloudy, expressed as a specific luminosity (erg/s/Angstrom) -- this is the radiation field of the stars after it has passed through the HII region, and is what one would see in an observational aperture centered on the stars with negligible contribution from the nebula

\item {} 
cloudy\_emit: the emitted nebular radiation field computed by cloudy, expressed as a specific luminosity (erg/s/Angstrom) -- this is the radiation emitted by the nebula excluding the stars, and is what one would see in an observational aperture that included the nebula but masked out the stars

\item {} 
cloudy\_trans\_emit: the sum of the transmitted stellar and emitted nebular radiation, expressed as a specific luminosity (erg/s/Angstrom) -- this is what one would see in an observational aperture covering the both the stars and the nebula

\item {} 
cloudy\_linelabel: list of emitting species for the line luminosities computed by cloudy, following cloudy's 4-letter notation

\item {} 
cloudy\_linewl: wavelengths of all the lines computed by cloudy (in Angstrom)

\item {} 
cloudy\_linelum: luminosities of the lines computed by cloudy (in erg/s)

\item {} 
cloudy\_filter\_names, cloudy\_filter\_units, cloudy\_filter\_wl\_eff, cloudy\_filter\_wl, cloudy\_filter\_response, cloudy\_filter\_beta, cloudy\_filter\_wl\_c: exactly the same as the corresponding fields without the cloudy prefix, but for the photometric filters applied to the cloudy output

\item {} 
cloudy\_phot\_trans, cloudy\_phot\_emit, and cloudy\_phot\_trans\_emit: photometry of the transmitted, emitted, and transmitted+emitted radiation field provided by cloudy\_trans, cloudy\_emit, and cloudy\_trans\_emit

\end{itemize}

For the above fields, quantities that are different for each trial and each time are stored as numpy arrays with a shape (N\_times, N\_trials) for scalar quantities (e.g., actual\_mass), or a shape (N, N\_times, N\_trials) for quantities that are vectors of length N (e.g., the spectrum).

The \code{read\_cluster} function is analogous, except that instead of reading the whole-galaxy data, it reads data on the individual star clusters, as stored in the \code{\_cluster\_*} output files. It returns the following fields:
\begin{itemize}
\item {} 
id: a unique identifier number for each cluster; this is guaranteed to be unique across both times and trials, so that if two clusters in the list have the same id number, that means that the data given are for the same cluster at two different times in its evolution

\item {} 
trial: the trial number in which that cluster appeared

\item {} 
time: the time at which the data for that cluster are computed

\item {} 
form\_time: the time at which that cluster formed

\item {} 
lifetime: the between when the cluster formed and when it will disrupt

\item {} 
target\_mass: the target stellar mass of the cluster

\item {} 
actual\_mass: the actual stellar mass of the cluter

\item {} 
live\_mass: the mass of all still-living stars in the cluster

\item {} 
num\_star: the number of stars in the cluster

\item {} 
max\_star\_mass: the mass of the single most massive still-living star in the cluster

\item {} 
A\_V: the visual extinction for this cluster, in mag; present only if SLUG was run with extinction enabled

\item {} 
All the remaining fields are identical to those listed above for integrated quantities, starting with wl

\end{itemize}

For all these fields, scalar quantities that are different for each cluster (e.g., actual\_mass) will be stored as arrays of shape (N\_cluster); vector quantities that are different for each cluster (e.g., spec) will be stored as arrays of shape (N\_cluster, N).

The following fields are present only if SLUG was run with a Variable Mode IMF:
\begin{itemize}
\item {} 
VPx: The value drawn for variable parameter x (0,1,2...) in each trial. The parameters are numbered in the order they are defined in the IMF definition file.

\end{itemize}

These fields are present in both the cluster and integrated outputs if a simulation has been run using the variable mode IMF.


\section{Full Documentation of slugpy}
\label{slugpy:module-slugpy}\label{slugpy:full-documentation-of-slugpy}\index{slugpy (module)}\index{combine\_cluster() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.combine_cluster}\pysiglinewithargsret{\code{slugpy.}\bfcode{combine\_cluster}}{\emph{data}}{}
Function to combine cluster data from multiple SLUG2 runs,
treating each input run as a separate set of trials. Trial and
cluster unique ID numbers are altered as necessary to avoid
duplication between the merged data sets.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}list\_like{]}
A list containing the cluster data for each run, as
returned by read\_cluster

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{combined\_data}] \leavevmode{[}namedtuple{]}
The combined data, in the same format as each object in data

\end{description}

\end{description}

\end{fulllineitems}

\index{combine\_integrated() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.combine_integrated}\pysiglinewithargsret{\code{slugpy.}\bfcode{combine\_integrated}}{\emph{data}}{}
Function to combine integrated data from multiple SLUG2 runs,
treating each input run as a separate set of trials.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}list\_like{]}
A list containing the integrated data for each run, as
returned by read\_integrated

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{combined\_data}] \leavevmode{[}namedtuple{]}
The combined data, in the same format as each object in data

\end{description}

\end{description}

\end{fulllineitems}

\index{compute\_photometry() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.compute_photometry}\pysiglinewithargsret{\code{slugpy.}\bfcode{compute\_photometry}}{\emph{wl}, \emph{spec}, \emph{filtername}, \emph{photsystem='L\_nu'}, \emph{filter\_wl=None}, \emph{filter\_response=None}, \emph{filter\_beta=None}, \emph{filter\_wl\_c=None}, \emph{filter\_dir=None}}{}
This function takes an input spectrum and a set of response
functions for photometric filters, and returns the photometry
through those filters.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{wl}] \leavevmode{[}array{]}
Wavelength of input spectrum in Angstrom

\item[{spec}] \leavevmode{[}array{]}
Specific luminosity per unit wavelength for input spectrum, in
erg/s/A

\item[{filtername}] \leavevmode{[}string or iterable of strings{]}
Name or list of names of the filters to be used. Filter names
can also include the special filters Lbol, QH0, QHe0, and QHe1;
the values returned for these will be the bolometric luminosity
(in erg/s) and the photon luminosities (in photons/s) in the H,
He, and He+ ionizing-continua, respectively.

\item[{photsystem}] \leavevmode{[}string{]}
The photometric system to use for the output. Allowable values
are `L\_nu', `L\_lambda', `AB', `STMAG', and `Vega',
corresponding to the options defined in the SLUG code.

\item[{filter\_wl}] \leavevmode{[}array or iterable of arrays{]}
Array giving the wavelengths in Angstrom at which the filter is
response function is given. If this object is an iterable of
arrays rather than a single array, it is assumed to represent
the wavelengths for a set of filters. If this is set,
no data is read from disk. Default behavior is to read the
filter information from disk.

\item[{filter\_response}] \leavevmode{[}array or iterable of arrays{]}
Array giving the filter response function at each wavelenght
and for each filter in filter\_wl. Must be set if filter\_wl is
set, ignored otherwise.

\item[{filter\_beta}] \leavevmode{[}iterable{]}
Array-like object containing the index beta for each
filter. Must be set if filter\_wl is set, ignored otherwise.

\item[{filter\_wl\_c}] \leavevmode{[}iterable{]}
Array-like object containing the pivot wavelength for each
filter. Must be set if filter\_wl is set, ignored otherwise.

\item[{filter\_dir}] \leavevmode{[}string{]}
Directory where the filter data files can be found. If left as
None, filters will be looked for in the \$SLUG\_DIR/lib/filters
directory. This parameter is used only if filtername is not
None.

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{phot}] \leavevmode{[}array{]}
Photometric values in the requested filters. Units depend on
the choice of photometric system:
L\_nu --\textgreater{} erg/s/Hz;
L\_lambda --\textgreater{} erg/s/A;
AB --\textgreater{} absolute AB magnitude;
STMAG --\textgreater{} absolute ST magnitude;
Vega --\textgreater{} absolute Vega magnitude;

\end{description}

\end{description}

\end{fulllineitems}

\index{photometry\_convert() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.photometry_convert}\pysiglinewithargsret{\code{slugpy.}\bfcode{photometry\_convert}}{\emph{photsystem}, \emph{phot}, \emph{units}, \emph{wl\_cen=None}, \emph{filter\_last=False}, \emph{filter\_names=None}, \emph{filter\_dir=None}}{}
Function to convert photometric data between photometric systems.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{photsystem}] \leavevmode{[}string{]}
The photometric system to which to convert. Allowable values
are `L\_nu', `L\_lambda', `AB', `STMAG', and `Vega',
corresponding to the options defined in the SLUG code. If this
is set and the conversion requested involves a conversion from
a wavelength-based system to a frequency-based one, wl\_cen must
not be None.

\item[{phot}] \leavevmode{[}array{]}
array of photometric data; if the array has more than one
dimension, the first dimension is assumed to represent the
different photometric filters (unless filter\_last is True,
in which case the last dimension is represents the array of
filters)

\item[{units}] \leavevmode{[}iterable of strings{]}
iterable listing the units of the input photometric data. On
return, strings will be changed to the units of the new system.

\item[{wl\_cen}] \leavevmode{[}array{]}
central wavelengths of the filters, in Angstrom; can be left as
None if the requested conversion doesn't require going between
wavelength- and frequency-based systems.

\item[{filter\_last}] \leavevmode{[}bool{]}
If the input data have more than one dimension, by default it
is assumed that the first dimension contains values for the
different photometric filters. If this keyword is set to True,
it will instead be assumed that the last dimension contains the
values for the different filters.

\item[{filter\_names}] \leavevmode{[}iterable of strings{]}
Names of all filters, used to read the filter response
functions from disk; only needed for conversions to and from
Vega magnitudes, and ignored otherwise

\item[{filter\_dir}] \leavevmode{[}string{]}
Directory where the filter data files can be found. If left as
None, filters will be looked for in the \$SLUG\_DIR/lib/filters
directory. This parameter is used only for conversions to
and from Vega magnitudes.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\item[{Raises}] \leavevmode
ValueError, if wl\_cen is None but the requested conversion
requires going between wavelength- and frequency-based systems

\end{description}

\end{fulllineitems}

\index{read\_cluster() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_cluster}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_cluster}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{nofilterdata=False}, \emph{photsystem=None}, \emph{verbose=False}, \emph{read\_filters=None}, \emph{read\_nebular=None}, \emph{read\_extinct=None}, \emph{read\_info=None}, \emph{no\_stellar\_mass=False}}{}
Function to read all cluster data for a SLUG2 run.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{nofilterdata}] \leavevmode{[}bool{]}
If True, the routine does not attempt to read the filter
response data from the standard location

\item[{photsystem}] \leavevmode{[}None or string{]}
If photsystem is None, the data will be returned in the same
photometric system in which they were read. Alternately, if it
is a string, the data will be converted to the specified
photometric system. Allowable values are `L\_nu', `L\_lambda',
`AB', `STMAG', and `Vega', corresponding to the options defined
in the SLUG code. If this is set and the conversion requested
involves a conversion from a wavelength-based system to a
frequency-based one, nofilterdata must be False so that the
central wavelength of the photometric filters is available.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_filters}] \leavevmode{[}None \textbar{} string \textbar{} listlike containing strings{]}
If this is None, photometric data on all filters is
read. Otherwise only filters whose name(s) match the input
filter names ar read.

\item[{read\_nebular}] \leavevmode{[}None \textbar{} bool{]}
If True, only photometric data with the nebular contribution
is read; if False, only data without it is read. Default
behavior is to read all data.

\item[{read\_extinct}] \leavevmode{[}None \textbar{} bool{]}
If True, only photometric data with extinction applied is
read; if False, only data without it is read. Default
behavior is to read all data.

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `prop\_name',
`phot\_name', `spec\_name', `cloudyspec\_name', `cloudylines\_name'
and `format', giving the names of the files read and the format
they were in; `format' will be one of `ascii', `binary', or
`fits'. If one of the files is not present, the corresponding
\_name key will be omitted from the dict.

\item[{no\_stellar\_mass}] \leavevmode{[}bool{]}
Prior to 7/15, output files did not contain the stellar\_mass
field; this can be detected automatically for ASCII and FITS
formats, but not for binary format; if True, this specifies
that the binary file being read does not contain a
stellar\_mass field; it has no effect for ASCII or FITS files

\item[{no\_neb\_extinct}] \leavevmode{[}bool{]}
Prior to 2/17, SLUG did not support differential nebular
extinction, and thus there was no output field for it; this
is detected and handled automatically for ASCII and FITS
files; for binary outputs, this flag must be set for pre
2/17 output files to be read correctly

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:

(Always present)
\begin{description}
\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
time at which cluster's properties are being evaluated

\item[{A\_V}] \leavevmode{[}array{]}
A\_V value for each cluster, in mag (present only if SLUG was
run with extinction enabled)

\item[{A\_Vneb}] \leavevmode{[}array{]}
value of A\_V applied to the nebular light for each cluster
(present only if SLUG was run with both nebular emission and
extinction enabled)

\end{description}

(Present if the run being read contains a cluster\_prop file)
\begin{description}
\item[{form\_time}] \leavevmode{[}array{]}
time when cluster formed

\item[{lifetime}] \leavevmode{[}array{]}
time at which cluster will disrupt

\item[{target\_mass}] \leavevmode{[}array{]}
target cluster mass

\item[{actual\_mass}] \leavevmode{[}array{]}
actual mass at formation

\item[{live\_mass}] \leavevmode{[}array{]}
mass of currently living stars

\item[{stellar\_mass}] \leavevmode{[}array{]}
mass of all stars, living and stellar remnants

\item[{num\_star}] \leavevmode{[}array, dtype ulonglong{]}
number of living stars in cluster being treated stochastically

\item[{max\_star\_mass}] \leavevmode{[}array{]}
mass of most massive living star in cluster

\item[{vpn\_tuple}] \leavevmode{[}tuple{]}
tuple containing arrays for any variable parameters we have
(eg: VP0, VP1,VP2...) in the IMF. Each element of the tuple
is an array. Present only if variable parameters were
enables when SLUG was run.

\end{description}

(Present if the run being read contains a cluster\_spec file)
\begin{description}
\item[{wl}] \leavevmode{[}array{]}
wavelength, in Angstrom

\item[{spec}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of each cluster at each wavelength, in erg/s/A

\item[{wl\_neb}] \leavevmode{[}array{]}
wavelength for the nebular spectrum, in Angstrom (present
only if SLUG was run with nebular emission enabled)

\item[{spec\_neb}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity at each wavelength and each time for each
trial, including emission and absorption by the HII region,
in erg/s/A (present only if SLUG was run with nebular
emission enabled)

\item[{wl\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum, in Angstrom (present
only if SLUG was run with extinction enabled)

\item[{spec\_ex}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial after extinction has been applied, in
erg/s/A (present only if SLUG was run with extinction
enabled)

\item[{wl\_neb\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum with nebular emission,
in Angstrom (present only if SLUG was run with both nebular
emission and extinction enabled)

\item[{spec\_neb\_ex}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial including emission and absorption by the
HII region, after extinction has been applied, in erg/s/A
(present only if SLUG was run with nebular emission and
extinction both enabled)

\end{description}

(Present if the run being read contains a cluster\_phot file)
\begin{description}
\item[{filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{filter\_wl\_cen}] \leavevmode{[}list{]}
central wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1; omitted if
nofilterdata is True

\item[{filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1; omitted
if nofilterdata is True

\item[{phot}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value in each filter for each cluster; units are as
indicated in the units field

\end{description}

If extinction is enabled, phot\_ex will contain photometry  
after extinction has been applied.

(Present if the run being read contains a cluster\_yield file)
\begin{description}
\item[{isotope\_name}] \leavevmode{[}array of strings{]}
Atomic symbols of the isotopes included in the yield table

\item[{isotope\_Z}] \leavevmode{[}array of int{]}
Atomic numbers of the isotopes included in the yield table

\item[{isotope\_A}] \leavevmode{[}array of int{]}
Atomic mass number of the isotopes included in the yield table

\item[{yld}] \leavevmode{[}array{]}
Yield of each isotope, defined as the instantaneous amount
produced up to that time; for unstable isotopes, this
includes the effects of decay since production

\end{description}

(Present if the run being read contains a cluster\_cloudyspec file)
\begin{description}
\item[{cloudy\_wl}] \leavevmode{[}array{]}
wavelength, in Angstrom

\item[{cloudy\_inc}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of the cluster's stellar radiation field at
each wavelength, in erg/s/A

\item[{cloudy\_trans}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of the stellar radiation field after it has
passed through the HII region, at each wavelength, in erg/s/A

\item[{cloudy\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of the radiation field emitted by the HII
region, at each wavelength, in erg/s/A

\item[{cloudy\_trans\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
the sum of the emitted and transmitted fields; this is what
would be seen by an observer looking at both the star cluster
and its nebula

\end{description}

(Present if the run being read contains a cluster\_cloudylines file)
\begin{description}
\item[{cloudy\_linelabel}] \leavevmode{[}array, dtype='S4', shape (N\_lines){]}
labels for the lines, following cloudy's 4 character line label
notation

\item[{cloudy\_linewl}] \leavevmode{[}array, shape (N\_lines){]}
rest wavelength for each line, in Angstrom

\item[{cloudy\_linelum}] \leavevmode{[}array, shape (N\_cluster, N\_lines){]}
luminosity of each line at each time for each trial, in erg/s

\end{description}

(Present if the run being read contains a cluster\_cloudyphot file)
\begin{description}
\item[{cloudy\_filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{cloudy\_filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{cloudy\_filter\_wl\_eff}] \leavevmode{[}list{]}
effective wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{cloudy\_filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1; omitted if
nofilterdata is True

\item[{cloudy\_filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1; omitted
if nofilterdata is True

\item[{cloudy\_filter\_beta}] \leavevmode{[}list{]}
powerlaw index beta for each filter; used to normalize the
photometry

\item[{cloudy\_filter\_wl\_c}] \leavevmode{[}list{]}
pivot wavelength for each filter; used to normalize the photometry

\item[{cloudy\_phot\_trans}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value for each cluster in each filter for the
transmitted light (i.e., the starlight remaining after it has
passed through the HII region); units are as indicated in
the units field

\item[{cloudy\_phot\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value for each cluster in each filter for the
emitted light (i.e., the diffuse light emitted by the HII
region); units are as indicated in the units field

\item[{cloudy\_phot\_trans\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value in each filter for each cluster for the
transmitted plus emitted light (i.e., the light coming
directly from the stars after absorption by the HII region,
plus the diffuse light emitted by the HII region); units are as
indicated in the units field

\end{description}

(Present if the run being read contains a cluster\_cloudyparams file)
\begin{description}
\item[{cloudy\_hden}] \leavevmode{[}array{]}
number density of H nuclei at the inner edge of the ionized
region simulated by cloudy

\item[{cloudy\_r0}] \leavevmode{[}array{]}
inner radius of the ionized region simulated by cloudy

\item[{cloudy\_rS}] \leavevmode{[}array{]}
outer radius of the ionized region simulated by cloudy (approximate!)

\item[{cloudy\_QH0}] \leavevmode{[}array{]}
ionizing luminosity used in the cloudy computation

\item[{cloudy\_covFac}] \leavevmode{[}array{]}
covering factor assumed in the cloudy computation; only a
fraction covFac of the ionizing photons are assumed to
produce emission within the HII region, while the remainder
are assumed to escape

\item[{cloudy\_U}] \leavevmode{[}array{]}
volume-averaged ionization parameter of the HII region
simulated by cloudy; note that this value is approximate,
not exact, and the approximation can be very poor if
radiation pressure effects are significant

\item[{cloudy\_Omega}] \leavevmode{[}array{]}
Yeh \& Matzner (2012) wind parameter for the HII region
simulated by cloudy; as with U, this value is approximate,
and the approximation is valid only if radiation pressure
effects are small

\end{description}

\item[{Raises}] \leavevmode
IOError, if no photometry file can be opened
ValueError, if photsystem is set to an unknown values

\end{description}

\end{fulllineitems}

\index{read\_cluster\_phot() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_cluster_phot}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_cluster\_phot}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{nofilterdata=False}, \emph{photsystem=None}, \emph{verbose=False}, \emph{read\_info=None}, \emph{filters\_only=False}, \emph{read\_filters=None}, \emph{read\_nebular=None}, \emph{read\_extinct=None}, \emph{phot\_only=False}}{}
Function to read a SLUG2 cluster\_phot file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\item[{nofilterdata}] \leavevmode{[}bool{]}
If True, the routine does not attempt to read the filter
response data from the standard location

\item[{photsystem}] \leavevmode{[}None or string{]}
If photsystem is None, the data will be returned in the same
photometric system in which they were read. Alternately, if it
is a string, the data will be converted to the specified
photometric system. Allowable values are `L\_nu', `L\_lambda',
`AB', `STMAG', and `Vega', corresponding to the options defined
in the SLUG code. If this is set and the conversion requested
involves a conversion from a wavelength-based system to a
frequency-based one, nofilterdata must be False so that the
central wavelength of the photometric filters is available.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\item[{filters\_only}] \leavevmode{[}bool{]}
If True, the code only reads the data on the filters, not
any of the actual photometry. If combined with nofilterdata,
this can be used to return the list of available filters
and nothing else.

\item[{read\_filters}] \leavevmode{[}None \textbar{} string \textbar{} listlike containing strings{]}
If this is None, data on all filters is read. Otherwise only
filters whose name(s) match the input filter names ar
read.

\item[{read\_nebular}] \leavevmode{[}None \textbar{} bool{]}
If True, only data with the nebular contribution is read; if
False, only data without it is read. Default behavior is to
read all data.

\item[{read\_extinct}] \leavevmode{[}None \textbar{} bool{]}
If True, only data with extinction applied is read; if
False, only data without it is read. Default behavior is to
read all data.

\item[{phot\_only}] \leavevmode{[}bool{]}
If true, id, trial, time, and filter information are not
read, only photometry

\end{description}

\item[{Returns}] \leavevmode
A namedtuple, which can contain the following fields depending
on the input options, and depending on which fields are present
in the file being read:
\begin{description}
\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
times at which cluster spectra are output, in yr

\item[{filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{filter\_wl\_eff}] \leavevmode{[}list{]}
effective wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1

\item[{filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1

\item[{filter\_beta}] \leavevmode{[}list{]}
powerlaw index beta for each filter; used to normalize the
photometry

\item[{filter\_wl\_c}] \leavevmode{[}list{]}
pivot wavelength for each filter; used to normalize the photometry

\item[{phot}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value in each filter for each cluster; units are as
indicated in the units field

\item[{phot\_neb}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
same as phot, but for the light after it has passed through
the HII region

\item[{phot\_ex}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
same as phot, but after extinction has been applied

\item[{phot\_neb\_ex}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
same as phot, but for the light after it has passed through
the HII region and then had extinction applied

\end{description}

\item[{Raises}] \leavevmode
IOError, if no photometry file can be opened
ValueError, if photsystem is set to an unknown value

\end{description}

\end{fulllineitems}

\index{read\_cluster\_prop() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_cluster_prop}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_cluster\_prop}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}, \emph{no\_stellar\_mass=False}, \emph{no\_neb\_extinct=False}}{}
Function to read a SLUG2 cluster\_prop file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\item[{no\_stellar\_mass}] \leavevmode{[}bool{]}
Prior to 7/15, output files did not contain the stellar\_mass
field; this can be detected automatically for ASCII and FITS
formats, but not for binary format; if True, this specifies
that the binary file being read does not contain a
stellar\_mass field; it has no effect for ASCII or FITS files

\item[{no\_neb\_extinct}] \leavevmode{[}bool{]}
Prior to 2/17, SLUG did not support differential nebular
extinction, and thus there was no output field for it; this
is detected and handled automatically for ASCII and FITS
files; for binary outputs, this flag must be set for pre
2/17 output files to be read correctly

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
time at which cluster's properties are being evaluated

\item[{form\_time}] \leavevmode{[}array{]}
time when cluster formed

\item[{lifetime}] \leavevmode{[}array{]}
time at which cluster will disrupt

\item[{target\_mass}] \leavevmode{[}array{]}
target cluster mass

\item[{actual\_mass}] \leavevmode{[}array{]}
actual mass at formation

\item[{live\_mass}] \leavevmode{[}array{]}
mass of currently living stars

\item[{stellar\_mass}] \leavevmode{[}array{]}
mass of all stars, living and stellar remnants

\item[{num\_star}] \leavevmode{[}array, dtype ulonglong{]}
number of living stars in cluster being treated stochastically

\item[{max\_star\_mass}] \leavevmode{[}array{]}
mass of most massive living star in cluster

\item[{A\_V}] \leavevmode{[}array{]}
A\_V value for each cluster, in mag (present only if SLUG was
run with extinction enabled)

\item[{A\_Vneb}] \leavevmode{[}array{]}
value of A\_V applied to the nebular light for each cluster
(present only if SLUG was run with both nebular emission and
extinction enabled)

\item[{vpn\_tuple}] \leavevmode{[}tuple{]}
tuple containing arrays for any variable parameters we have
(eg: VP0, VP1,VP2...) in the IMF. Each element of the tuple
is an array. Present only if variable parameters were
enables when SLUG was run.

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_cluster\_spec() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_cluster_spec}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_cluster\_spec}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 cluster\_spec file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
times at which cluster spectra are output, in yr

\item[{wl}] \leavevmode{[}array{]}
wavelength, in Angstrom

\item[{spec}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of each cluster at each wavelength, in erg/s/A

\item[{wl\_neb}] \leavevmode{[}array{]}
wavelength for the nebular spectrum, in Angstrom (present
only if SLUG was run with nebular emission enabled)

\item[{spec\_neb}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity at each wavelength and each time for each
trial, including emission and absorption by the HII region,
in erg/s/A (present only if SLUG was run with nebular
emission enabled)

\item[{wl\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum, in Angstrom (present
only if SLUG was run with extinction enabled)

\item[{spec\_ex}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial after extinction has been applied, in
erg/s/A (present only if SLUG was run with extinction
enabled)

\item[{wl\_neb\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum with nebular emission,
in Angstrom (present only if SLUG was run with both nebular
emission and extinction enabled)

\item[{spec\_neb\_ex}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial including emission and absorption by the
HII region, after extinction has been applied, in erg/s/A
(present only if SLUG was run with nebular emission and
extinction both enabled)

\end{description}

\item[{Raises}] \leavevmode
IOError, if no spectrum file can be opened

\end{description}

\end{fulllineitems}

\index{read\_cluster\_yield() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_cluster_yield}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_cluster\_yield}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 cluster\_spec file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{isotope\_name}] \leavevmode{[}array of strings, shape (N\_iso){]}
Atomic symbols of the isotopes included in the yield table

\item[{isotope\_Z}] \leavevmode{[}array of int, shape (N\_iso){]}
Atomic numbers of the isotopes included in the yield table

\item[{isotope\_A}] \leavevmode{[}array of int, shape (N\_iso){]}
Atomic mass number of the isotopes included in the yield table

\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
times at which cluster spectra are output, in yr

\item[{yld}] \leavevmode{[}array, shape (N\_cluster, N\_iso){]}
Yield of each isotope, defined as the instantaneous amount
produced up to that time; for unstable isotopes, this
includes the effects of decay since production

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_filter() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_filter}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_filter}}{\emph{filtername}, \emph{filter\_dir=None}}{}
Function to read a filter or set of filters for SLUG2. By default
this function searches the SLUG\_DIR/lib/filter directory, followed
by the current working directory. This can be overridden by the
filter\_dir keyword.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{filtername}] \leavevmode{[}string or iterable containing strings{]}
Name or names of filters to be read; for the special filters
Lbol, QH0, QHe0, and QHe1, the return value will be None

\item[{filter\_dir}] \leavevmode{[}string{]}
Directory where the filter data files can be found

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{wl\_eff}] \leavevmode{[}float or array{]}
Central wavelength of the filter, defined by 
wl\_eff = exp(int R ln lambda dln lambda / int R dln lambda)

\item[{wl}] \leavevmode{[}array or list of arrays{]}
Wavelength table for each filter, in Ang

\item[{response}] \leavevmode{[}array or list of arrays{]}
Response function per photon for each filter

\item[{beta}] \leavevmode{[}float or array{]}
Index beta for the filter

\item[{wl\_c}] \leavevmode{[}float or array{]}
Pivot wavelength for the filter; used when beta != 0 to
normalize the photometry

\end{description}

\item[{Raises}] \leavevmode
IOError, if the filter data files cannot be opened, or if the
requested filter cannot be found

\end{description}

\end{fulllineitems}

\index{read\_integrated() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_integrated}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_integrated}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{nofilterdata=False}, \emph{photsystem=None}, \emph{verbose=False}, \emph{read\_info=None}, \emph{no\_stellar\_mass=False}}{}
Function to read all integrated light data for a SLUG2 run.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{nofilterdata}] \leavevmode{[}bool{]}
If True, the routine does not attempt to read the filter
response data from the standard location

\item[{photsystem}] \leavevmode{[}None or string{]}
If photsystem is None, the data will be returned in the same
photometric system in which they were read. Alternately, if it
is a string, the data will be converted to the specified
photometric system. Allowable values are `L\_nu', `L\_lambda',
`AB', `STMAG', and `Vega', corresponding to the options defined
in the SLUG code. If this is set and the conversion requested
involves a conversion from a wavelength-based system to a
frequency-based one, nofilterdata must be False so that the
central wavelength of the photometric filters is available.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `prop\_name',
`phot\_name', `spec\_name', `cloudyspec\_name', `cloudylines\_name'
and `format', giving the names of the files read and the format
they were in; `format' will be one of `ascii', `binary', or
`fits'. If one of the files is not present, the corresponding
\_name key will be omitted from the dict.

\item[{no\_stellar\_mass}] \leavevmode{[}bool{]}
Prior to 7/15, output files did not contain the stellar\_mass
field; this can be detected automatically for ASCII and FITS
formats, but not for binary format; if True, this specifies
that the binary file being read does not contain a
stellar\_mass field; it has no effect for ASCII or FITS files

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:

(Always present)
\begin{description}
\item[{time: array}] \leavevmode
Times at which data are output

\end{description}

(Only present if an integrated\_prop file is found)
\begin{description}
\item[{target\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Target stellar mass at each time in each trial

\item[{actual\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Actual mass of stars created up to each time in each trial

\item[{live\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Mass of currently-alive stars at each time in each trial

\item[{stellar\_mass}] \leavevmode{[}array{]}
mass of all stars, living and stellar remnants

\item[{cluster\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Mass of living stars in non-disrupted clusters at each time in
each trial

\item[{num\_clusters}] \leavevmode{[}array, shape (N\_times, N\_trials), dtype ulonglong{]}
Number of non-disrupted clusters present at each time in each
trial

\item[{num\_dis\_clusters}] \leavevmode{[}array, shape (N\_times, N\_trials), dtype ulonglong{]}
Number of disrupted clusters present at each time in each trial

\item[{num\_fld\_stars}] \leavevmode{[}array, shape (N\_times, N\_trials), dtype ulonglong{]}
Number of living field stars (excluding those in disrupted 
clusters and those being treated non-stochastically) present at
each time in each trial

\end{description}

(Only present if an integrated\_spec file is found)
\begin{description}
\item[{wl}] \leavevmode{[}array{]}
wavelengths, in Angstrom

\item[{spec}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength and each time for each
trial, in erg/s/A

\item[{wl\_neb}] \leavevmode{[}array{]}
wavelength for the nebular spectrum, in Angstrom (present
only if SLUG was run with nebular emission enabled)

\item[{spec\_neb}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength and each time for each
trial, including emission and absorption by the HII region,
in erg/s/A (present only if SLUG was run with nebular
emission enabled)

\item[{wl\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum, in Angstrom (present
only if SLUG was run with extinction enabled)

\item[{spec\_ex}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial after extinction has been applied, in
erg/s/A (present only if SLUG was run with extinction
enabled)

\item[{wl\_neb\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum with nebular emission,
in Angstrom (present only if SLUG was run with both nebular
emission and extinction enabled)

\item[{spec\_neb\_ex}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial including emission and absorption by the
HII region, after extinction has been applied, in erg/s/A
(present only if SLUG was run with nebular emission and
extinction both enabled)

\end{description}

(Only present if an integrated\_phot file is found)
\begin{description}
\item[{filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{filter\_wl\_cen}] \leavevmode{[}list{]}
central wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1; omitted if
nofilterdata is True

\item[{filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1; omitted
if nofilterdata is True

\item[{phot}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial;
units are as indicated in the units field

\end{description}

If extinction is enabled, phot\_ex will contain photometry  
after extinction has been applied.

(Only present if an integrate\_yield file is found)
\begin{description}
\item[{isotope\_name}] \leavevmode{[}array of strings{]}
Atomic symbols of the isotopes included in the yield table

\item[{isotope\_Z}] \leavevmode{[}array of int{]}
Atomic numbers of the isotopes included in the yield table

\item[{isotope\_A}] \leavevmode{[}array of int{]}
Atomic mass number of the isotopes included in the yield table

\item[{yld}] \leavevmode{[}array{]}
Yield of each isotope, defined as the instantaneous amount
produced up to that time; for unstable isotopes, this
includes the effects of decay since production

\end{description}

(Only present if an integrated\_cloudyspec file is found)
\begin{description}
\item[{cloudy\_wl}] \leavevmode{[}array{]}
wavelength, in Angstrom

\item[{cloudy\_inc}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity of the stellar radiation field at each
wavelength and each time for each trial, in erg/s/A

\item[{cloudy\_trans}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity of the stellar radiation field after it has
passed through the HII region, at each wavelength and each time
for each trial, in erg/s/A

\item[{cloudy\_emit}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity of the radiation field emitted by the HII
region, at each wavelength and each time for each trial, in
erg/s/A

\item[{cloudy\_trans\_emit}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
the sum of emitted and transmitted; this is what would be seen
by an observer looking at both the star cluster and its nebula

\end{description}

(Only present if an integrated\_cloudylines file is found)
\begin{description}
\item[{cloudy\_linelabel}] \leavevmode{[}array, dtype='S4', shape (N\_lines){]}
labels for the lines, following cloudy's 4 character line label
notation

\item[{cloudy\_linewl}] \leavevmode{[}array, shape (N\_lines){]}
rest wavelength for each line, in Angstrom

\item[{cloudy\_linelum}] \leavevmode{[}array, shape (N\_lines, N\_times, N\_trials){]}
luminosity of each line at each time for each trial, in erg/s

\end{description}

(Only present if an integrated\_cloudyphot file is found)
\begin{description}
\item[{cloudy\_filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{cloudy\_filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{cloudy\_filter\_wl\_eff}] \leavevmode{[}list{]}
effective wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{cloudy\_filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1; omitted if
nofilterdata is True

\item[{cloudy\_filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1; omitted
if nofilterdata is True

\item[{cloudy\_filter\_beta}] \leavevmode{[}list{]}
powerlaw index beta for each filter; used to normalize the
photometry

\item[{cloudy\_filter\_wl\_c}] \leavevmode{[}list{]}
pivot wavelength for each filter; used to normalize the photometry

\item[{cloudy\_phot\_trans}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial for
the transmitted light (i.e., the starlight remaining after it
has passed through the HII region); units are as indicated in
the units field

\item[{cloudy\_phot\_emit}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial for
the emitted light (i.e., the diffuse light emitted by the HII
region); units are as indicated in the units field

\item[{cloudy\_phot\_trans\_emit}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial for
the transmitted plus emitted light (i.e., the light coming
directly from the stars after absorption by the HII region,
plus the diffuse light emitted by the HII region); units are as
indicated in the units field

\end{description}

(Only present if an integrated\_cloudyparams file is found)
\begin{description}
\item[{cloudy\_hden}] \leavevmode{[}array{]}
number density of H nuclei at the inner edge of the ionized
region simulated by cloudy

\item[{cloudy\_r0}] \leavevmode{[}array{]}
inner radius of the ionized region simulated by cloudy

\item[{cloudy\_rS}] \leavevmode{[}array{]}
outer radius of the ionized region simulated by cloudy (approximate!)

\item[{cloudy\_QH0}] \leavevmode{[}array{]}
ionizing luminosity used in the cloudy computation

\item[{cloudy\_covFac}] \leavevmode{[}array{]}
covering factor assumed in the cloudy computation; only a
fraction covFac of the ionizing photons are assumed to
produce emission within the HII region, while the remainder
are assumed to escape

\item[{cloudy\_U}] \leavevmode{[}array{]}
volume-averaged ionization parameter of the HII region
simulated by cloudy; note that this value is approximate,
not exact, and the approximation can be very poor if
radiation pressure effects are significant

\item[{cloudy\_Omega}] \leavevmode{[}array{]}
Yeh \& Matzner (2012) wind parameter for the HII region
simulated by cloudy; as with U, this value is approximate,
and the approximation is valid only if radiation pressure
effects are small

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_integrated\_phot() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_integrated_phot}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_integrated\_phot}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{nofilterdata=False}, \emph{photsystem=None}, \emph{verbose=False}, \emph{read\_info=None}, \emph{filters\_only=False}, \emph{read\_filters=None}, \emph{read\_nebular=None}, \emph{read\_extinct=None}}{}
Function to read a SLUG2 integrated\_phot file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\item[{nofilterdata}] \leavevmode{[}bool{]}
If True, the routine does not attempt to read the filter
response data from the standard location

\item[{photsystem}] \leavevmode{[}None or string{]}
If photsystem is None, the data will be returned in the same
photometric system in which they were read. Alternately, if it
is a string, the data will be converted to the specified
photometric system. Allowable values are `L\_nu', `L\_lambda',
`AB', `STMAG', and `Vega', corresponding to the options defined
in the SLUG code. If this is set and the conversion requested
involves a conversion from a wavelength-based system to a
frequency-based one, nofilterdata must be False so that the
central wavelength of the photometric filters is available.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\item[{filters\_only}] \leavevmode{[}bool{]}
If True, the code only reads the data on the filters, not
any of the actual photometry. If combined with nofilterdata,
this can be used to return the list of available filters
and nothing else.

\item[{read\_filters}] \leavevmode{[}None \textbar{} string \textbar{} listlike containing strings{]}
If this is None, data on all filters is read. Otherwise only
filters whose name(s) match the input filter names ar
read.

\item[{read\_nebular}] \leavevmode{[}None \textbar{} bool{]}
If True, only data with the nebular contribution is read; if
False, only data without it is read. Default behavior is to
read all data.

\item[{read\_extinct}] \leavevmode{[}None \textbar{} bool{]}
If True, only data with extinction applied is read; if
False, only data without it is read. Default behavior is to
read all data.

\end{description}

\item[{Returns}] \leavevmode
A namedtuple , which can contain the following fields depending
on the input options, and depending on which fields are present
in the file being read:
\begin{description}
\item[{time}] \leavevmode{[}array, shape (N\_times) or shape (N\_trials){]}
Times at which data are output; shape is either N\_times (if
the run was done with fixed output times) or N\_trials (if
the run was done with random output times)

\item[{filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{filter\_wl\_eff}] \leavevmode{[}list{]}
effective wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1

\item[{filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1

\item[{filter\_beta}] \leavevmode{[}list{]}
powerlaw index beta for each filter; used to normalize the
photometry

\item[{filter\_wl\_c}] \leavevmode{[}list{]}
pivot wavelength for each filter; used to normalize the photometry

\item[{phot}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial;
units are as indicated in the units field

\item[{phot\_neb}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
same as phot, but for the light after it has passed through
the HII region

\item[{phot\_ex}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
same as phot, but after extinction has been applied

\item[{phot\_neb\_ex}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
same as phot, but for the light after it has passed through
the HII region and then had extinction applied

\end{description}

\item[{Raises}] \leavevmode
IOError, if no photometry file can be opened
ValueError, if photsystem is set to an unknown value

\end{description}

\end{fulllineitems}

\index{read\_integrated\_prop() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_integrated_prop}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_integrated\_prop}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}, \emph{no\_stellar\_mass=False}}{}
Function to read a SLUG2 integrated\_prop file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\item[{no\_stellar\_mass}] \leavevmode{[}bool{]}
Prior to 7/15, output files did not contain the stellar\_mass
field; this can be detected automatically for ASCII and FITS
formats, but not for binary format; if True, this specifies
that the binary file being read does not contain a
stellar\_mass field; it has no effect for ASCII or FITS files

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{time}] \leavevmode{[}array, shape (N\_times) or shape (N\_trials){]}
Times at which data are output; shape is either N\_times (if
the run was done with fixed output times) or N\_trials (if
the run was done with random output times)

\item[{target\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Target stellar mass at each time

\item[{actual\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Actual mass of stars created up to each time in each trial

\item[{live\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Mass of currently-alive stars at each time in each trial

\item[{stellar\_mass}] \leavevmode{[}array{]}
mass of all stars, living and stellar remnants

\item[{cluster\_mass}] \leavevmode{[}array, shape (N\_times, N\_trials){]}
Stellar mass in non-disrupted clusters at each time in each
trial

\item[{num\_clusters}] \leavevmode{[}array, shape (N\_times, N\_trials), dtype ulonglong{]}
Number of non-disrupted clusters present at each time in each
trial

\item[{num\_dis\_clusters}] \leavevmode{[}array, shape (N\_times, N\_trials), dtype ulonglong{]}
Number of disrupted clusters present at each time in each trial

\item[{num\_fld\_stars}] \leavevmode{[}array, shape (N\_times, N\_trials), dtype ulonglong{]}
Number of living field stars (excluding those in disrupted 
clusters and those being treated non-stochastically) present at
each time in each trial

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_integrated\_spec() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_integrated_spec}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_integrated\_spec}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 integrated\_spec file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{time}] \leavevmode{[}array, shape (N\_times) or shape (N\_trials){]}
Times at which data are output; shape is either N\_times (if
the run was done with fixed output times) or N\_trials (if
the run was done with random output times)

\item[{wl}] \leavevmode{[}array{]}
wavelength, in Angstrom

\item[{spec}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength and each time for each
trial, in erg/s/A

\item[{wl\_neb}] \leavevmode{[}array{]}
wavelength for the nebular spectrum, in Angstrom (present
only if SLUG was run with nebular emission enabled)

\item[{spec\_neb}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength and each time for each
trial, including emission and absorption by the HII region,
in erg/s/A (present only if SLUG was run with nebular
emission enabled)

\item[{wl\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum, in Angstrom (present
only if SLUG was run with extinction enabled)

\item[{spec\_ex}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial after extinction has been applied, in
erg/s/A (present only if SLUG was run with extinction
enabled)

\item[{wl\_neb\_ex}] \leavevmode{[}array{]}
wavelength for the extincted spectrum with nebular emission,
in Angstrom (present only if SLUG was run with both nebular
emission and extinction enabled)

\item[{spec\_neb\_ex}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity at each wavelength in wl\_ex and each
time for each trial including emission and absorption by the
HII region, after extinction has been applied, in erg/s/A
(present only if SLUG was run with nebular emission and
extinction both enabled)

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_integrated\_yield() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_integrated_yield}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_integrated\_yield}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{read\_info=None}, \emph{verbose=False}}{}
Function to read a SLUG2 integrated\_yield file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{time}] \leavevmode{[}array, shape (N\_times) or shape (N\_trials){]}
Times at which data are output; shape is either N\_times (if
the run was done with fixed output times) or N\_trials (if
the run was done with random output times)

\item[{isotope\_name}] \leavevmode{[}array of strings, shape (N\_iso){]}
Atomic symbols of the isotopes included in the yield table

\item[{isotope\_Z}] \leavevmode{[}array of int, shape (N\_iso){]}
Atomic numbers of the isotopes included in the yield table

\item[{isotope\_A}] \leavevmode{[}array of int, shape (N\_iso){]}
Atomic mass number of the isotopes included in the yield table

\item[{yld}] \leavevmode{[}array, shape (N\_iso, N\_times) or (N\_iso, N\_trials){]}
Yield of each isotope, defined as the instantaneous amount
produced up to that time; for unstable isotopes, this
includes the effects of decay since production

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_summary() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.read_summary}\pysiglinewithargsret{\code{slugpy.}\bfcode{read\_summary}}{\emph{model\_name}, \emph{output\_dir=None}}{}
Function to open a SLUG output summary file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{summary}] \leavevmode{[}dict{]}
A dict containing all the keywords stored in the output file

\end{description}

\item[{Raises}] \leavevmode
IOError, if a summary file for the specified model cannot be found

\end{description}

\end{fulllineitems}

\index{slug\_open() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.slug_open}\pysiglinewithargsret{\code{slugpy.}\bfcode{slug\_open}}{\emph{filename}, \emph{output\_dir=None}, \emph{fmt=None}}{}
Function to open a SLUG2 output file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{filename}] \leavevmode{[}string{]}
Name of the file to open, without any extension. The following
extensions are tried, in order: .txt, .bin, .fits

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the
SLUG\_DIR/output directory if the SLUG\_DIR environment variable
is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{fp}] \leavevmode{[}file or astropy.io.fits.hdu.hdulist.HDUList{]}
A file object pointing the file that has been opened

\item[{fname}] \leavevmode{[}string{]}
Name of the file that was opened

\end{description}

\item[{Raises}] \leavevmode
IOError, if a file of the specified name cannot be found

\end{description}

\end{fulllineitems}

\index{slug\_pdf (class in slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.slug_pdf}\pysiglinewithargsret{\strong{class }\code{slugpy.}\bfcode{slug\_pdf}}{\emph{pdffile=None}}{}
A class that implements the SLUG PDF drawing method. This class
contains a method to parse slug-formatted PDF files and then draw
values from the PDFs they specify. This class is thread-safe, in
the sense that if multiple slug\_pdf instances are instantiated in
different threads, the random streams they generate will not be
identical.
\index{draw() (slugpy.slug\_pdf method)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.slug_pdf.draw}\pysiglinewithargsret{\bfcode{draw}}{\emph{*d}}{}
Draw from the PDF
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{d0, d1, ..., dn}] \leavevmode{[}int, optional{]}
Dimensions of the returned array; if left unspecified, a
single python float is returned

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{x}] \leavevmode{[}float or array{]}
One or more numbers drawn from the PDF

\end{description}

\end{description}

\end{fulllineitems}


\end{fulllineitems}

\index{write\_cluster() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.write_cluster}\pysiglinewithargsret{\code{slugpy.}\bfcode{write\_cluster}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Function to write a set of output cluster files in SLUG2 format,
starting from a cluster data set as returned by read\_cluster.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Cluster data to be written, in the namedtuple format returned
by read\_cluster

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the output file; `txt' and `ascii' produce ASCII
text files, `bin' or `binary' produce binary files, and
`fits' or `fits2' product FITS files; `fits2' uses an
ordering that allows for more efficient querying of outputs
too large to fit in memory

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}

\index{write\_integrated() (in module slugpy)}

\begin{fulllineitems}
\phantomsection\label{slugpy:slugpy.write_integrated}\pysiglinewithargsret{\code{slugpy.}\bfcode{write\_integrated}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Function to write a set of output integrated files in SLUG2 format,
starting from an integrated data set as returned by
read\_integrated.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Integrated data to be written, in the namedtuple format returned
by read\_integrated

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}



\chapter{cloudy\_slug: An Automated Interface to cloudy}
\label{cloudy:cloudy-slug-an-automated-interface-to-cloudy}\label{cloudy::doc}\label{cloudy:sec-cloudy-slug}
SLUG stochastically generates stellar spectra, and it includes an
approximate computation of the nebular lines produced when those
photons interact with the interstellar medium. However, this
approximation ignores a number of potentially important effects, and
does not properly account for the stochastic nature of the stellar
spectra. To perform a much more accurate calculation, SLUG includes an
automated interface to \href{http://nublado.org/}{cloudy} (\href{http://adsabs.harvard.edu/abs/2013RMxAA..49..137F}{Ferland et
al., 2013, RMxAA, 49, 137}). This can be
used to post-process the output of a SLUG run in order to compute
nebular emission.


\section{cloudy\_slug Basics}
\label{cloudy:cloudy-slug-basics}
The basic steps (described in greater detail below) are as follows:
\begin{enumerate}
\item {} 
Get cloudy installed and compiled, following the directions on the
\href{http://nublado.org/}{cloudy website}.

\item {} 
Set the environment variable \code{\$CLOUDY\_DIR} to the directory where
the cloudy executable \code{cloudy.exe} is located.  If you are using
a \code{bash}-like shell, the syntax for this is:

\begin{Verbatim}[commandchars=\\\{\}]
export CLOUDY\PYGZus{}DIR = /path/to/cloudy
\end{Verbatim}

while for a \code{csh}-like shell, it is:

\begin{Verbatim}[commandchars=\\\{\}]
setenv CLOUDY\PYGZus{}DIR /path/to/cloudy
\end{Verbatim}

\item {} 
If you desire, edit the cloudy input template
\code{cloudy\_slug/cloudy.in\_template} and the line list
\code{cloudy\_slug/LineList\_HII.dat}. There are the template input files
that will be used for all the cloudy runs, and their syntax follows
the standard cloudy syntax. They control things like the density and
element abundances in the nebula -- see {\hyperref[cloudy:ssec\string-cloudy\string-template]{\crossref{\DUrole{std,std-ref}{The cloudy\_slug Input Template}}}}
for more details.

\item {} 
Perform the desired SLUG simulation. The SLUG simulation outputs
must include spectra and photometry, and one of the photometric
bands output must be \code{QH0} (see
{\hyperref[parameters:ssec\string-phot\string-keywords]{\crossref{\DUrole{std,std-ref}{Photometric Filter Keywords}}}}). Depending on whether one is running in
integrated or cluster mode (see
{\hyperref[cloudy:sssec\string-cloudy\string-integrated\string-cluster]{\crossref{\DUrole{std,std-ref}{Integrated versus Cluster Spectra}}}}), either integrated specta
and photometry or cluster spectra and photometry are
required.

\item {} 
Invoke the cloudy\_slug interface script via:

\begin{Verbatim}[commandchars=\\\{\}]
python cloudy\PYGZus{}slug/cloudy\PYGZus{}slug.py SLUG\PYGZus{}MODEL\PYGZus{}NAME
\end{Verbatim}

where \code{SLUG\_MODEL\_NAME} is the name of the SLUG run to be
processed. See {\hyperref[cloudy:ssec\string-cloudy\string-model]{\crossref{\DUrole{std,std-ref}{The cloudy\_slug Physical Model}}}} for more information on
the underlying physical model assumed in the calculation, and
{\hyperref[cloudy:ssec\string-cloudy\string-slug\string-options]{\crossref{\DUrole{std,std-ref}{The cloudy\_slug Interface Script}}}} for more details on the python
script and its options.

\item {} 
The output will be stored as a series of additional output files of
with names of the form SLUG\_MODEL\_NAME\_*cloudy*.ext, where the
extension is .txt, .bin, or .fits, depending on the format in which
the orignal SLUG output was stored. These files can be processed
automatically by the slugpy helper routines (see
{\hyperref[slugpy:sec\string-slugpy]{\crossref{\DUrole{std,std-ref}{slugpy -- The Python Helper Library}}}}). See {\hyperref[cloudy:ssec\string-cloudy\string-output]{\crossref{\DUrole{std,std-ref}{Full Description of cloudy\_slug Output}}}} for a description
of the outputs.

\end{enumerate}

Note that some care is required in selecting the conditions passed to
cloudy to ensure that the results are physically sensible. Users are
strongly encouraged to read {\hyperref[cloudy:ssec\string-cloudy\string-model]{\crossref{\DUrole{std,std-ref}{The cloudy\_slug Physical Model}}}} to understand
exactly what physical assumptions are being made, and to ensure that
they are reasonable.


\section{The cloudy\_slug Physical Model}
\label{cloudy:ssec-cloudy-model}\label{cloudy:the-cloudy-slug-physical-model}
The cloudy\_slug code computes emission from a spherical HII region
surrounding a stellar population. The stellar population comes from
SLUG, and the emission calculation is performed with cloudy. Combining
the two requires some physical assumptions and inputs, which are
explained in this section.


\subsection{Integrated versus Cluster Spectra}
\label{cloudy:sssec-cloudy-integrated-cluster}\label{cloudy:integrated-versus-cluster-spectra}
SLUG outputs both integrated spectra for all the stars in a galaxy,
and spectra for individual clusters. Both the integrated spectra and
the individual cluster spectra can be processed by cloudy. However, it
is important to understand the implicit physical assumptions that one
is making while doing so. If one has a galaxy where all stars are in
clusters (i.e., cluster formation fraction is unity and there is no
cluster disruption), then the integrated starlight spectrum is just
the sum of the individual cluster spectra. For nebular emission,
however, this is not the case: nebular emission does not, in general,
add linearly.

For this reason, if one processes the integrated spectrum through
cloudy, the implicit physical assumption is that the entire galaxy is
a single giant HII region being ionized by the starlight of all the
clusters present. If one processes the individual cluster spectra
instead, the implicit physical picture is that there is no overlap
whatsoever between the HII regions surrounding different star
clusters. Reality almost certainly lies somewhere between these two
extremes, but it is important to understand physically what assumption
one is making by adopting one or the other. We refer to processing the
integrated spectrum as integrated mode, and to processing the
individual cluster spectra as cluster mode. Note that cluster mode can
be very computationally intensive if there are many clusters present,
and that in cluster mode there is no processing of nebular emission
produced by field stars.

In either mode, the spectrum that is used to compute the nebular
emission will be the \emph{unextincted, non-redshifted} spectrum computed
by SLUG.


\subsection{Nebular Properties}
\label{cloudy:nebular-properties}\label{cloudy:sssec-cloudy-nebular-properties}
Computing the nebular emission requires specifying the physical
properties of the interstellar gas into which the
photons propagate. Codes like cloudy require that the HII region be
described by an inner radius \(r_0\) and a number density
\(n_{0}\) of hydrogen nuclei at that radius. One option for
cloudy\_slug is that these parameters can be set in the cloudy inputs
as they would be for a normal cloudy run. However, these parameters
are not necessarily the most convenient or descriptive ones with which
to describe HII regions. For this reason, cloudy\_slug allows users to
specify HII region properties in a number of other more convneient
ways.

The basic assumptions made in cloudy\_slug's parameterization is
that the HII region is isobaric and isothermal, at all points hydrogen
is fully ionized and helium is singly ionized, and that radiation
pressure is negligible. The HII region occupies a spherical shell
bounded by an inner radius \(r_0\) and an outer radius
\(r_1\). The inner radius is set by the presence of a bubble of
shocked stellar wind material at a temperature \(\sim 10^6\) K,
which is assumed to be optically thick to ionizing photons. The
outer radius is set by the location where all the ionizing photons
have been absorbed.

Under these assumptions, the inner density \(n_0\) is simply the
(uniform) density \(n_{\mathrm{II}}\) throughout the ionized
region, and the ionizing photon luminosity passing through a shell of
material at a distance \(r\) from the stars is
\begin{equation*}
\begin{split}Q(r) = Q(\mathrm{H}^0)
\left[1 - \left(\frac{r}{r_S}\right)^3 +
\left(\frac{r_0}{r_S}\right)^3\right],\end{split}
\end{equation*}
where \(Q(\mathrm{H}^0)\) is the hydrogen-ionizing luminosity of
the source and \(r_S\) is the Stromgren radius, given by
\begin{equation*}
\begin{split}r_S = \left(\frac{3 Q(\mathrm{H}^0)}{4\pi
\alpha_B f_e n_{\mathrm{II}}^2}\right)^{1/3}.\end{split}
\end{equation*}
Here \(\alpha_B\) is the case B recombination coefficient and
\(f_e\) is the abundance of electrons per H nucleus. For the
purposes of cloudy\_slug, we take these two quantities to have
the fixed values \(\alpha_B = 2.59\times
10^{-13}\;\mathrm{cm}^3\;\mathrm{s}^{-1}\), appropriate for a
temperature of \(10^4\) K, and \(f_e = 1.1\), appropriate for
a region where He is singly ionized.

From this setup one can define some useful dimensionless numbers. One
is the wind parameter \(\Omega\) introduced by \href{http://adsabs.harvard.edu/abs/2012ApJ...757..108Y}{Yeh \&
Matnzer (2012, ApJ, 757, 108)}, which under the
simple assumptions made in cloudy\_slug is given by
\begin{equation*}
\begin{split}\Omega = \frac{r_0^3}{r_1^3-r_0^3}\end{split}
\end{equation*}
i.e., it is just the ratio of the volume occupied by the wind gas to
that occupied by the photoionized gas. The value of \(\Omega\)
determines whether winds are important (\(\Omega \gg 1\)) or
unimportant (\(\Omega \ll 1\)) for the dynamics of the HII
region. The second dimensionless parameter is the volume-averaged
ionization parameter
\begin{equation*}
\begin{split}\mathcal{U} = \frac{3}{4\pi (r_1^3-r_0^3)} \int_{r_0}^{r_1}
\left(\frac{Q(r)}{4\pi r^2 c f_i n_{\mathrm{II}}}\right)
4\pi r^2 \, dr.\end{split}
\end{equation*}
Here \(f_i\) is the number of free ions per H nucleus, and is
equal to \(f_i = 1.1\) under the assumption that He is singly
ionized. The quantity in parentheses is the ratio of the ionizing
photon to ion number densities at radius \(r\). The value of
\(\mathcal{U}\) is, together with \(n_{\mathrm{II}}\), the
most important factor in determining the output spectrum. A third
useful dimensionless parameter is the ionization parameter at the
inner radius,
\begin{equation*}
\begin{split}\mathcal{U}_0 = \frac{Q(\mathrm{H}^0)}
{4\pi r_0^2 f_i n_{\mathrm{II}} c}.\end{split}
\end{equation*}
The various quantities are not unrelated. It is straightforward to
show that they are constrained by the following relationships:
\begin{align*}\!\begin{aligned}
r_0 & = \Omega^{1/3} r_S \\\\
r_1 & = \left(1 + \Omega\right)^{1/3} r_S \\\\
\mathcal{U} & = \left[\frac{81 \alpha_B^2 n_{\mathrm{II}}
Q(\mathrm{H}^0)}{256 \pi c^3 f_e}\right]^{1/3}
\left[\left(1 + \Omega\right)^{4/3}
- \Omega^{1/3} \left(\frac{4}{3}+\Omega\right)\right] \\\\
& = \left[\frac{81 \alpha_B Q(\mathrm{H}^0)}
{64 \pi c^2 f_e r_S}\right]^{1/2}
\left[\left(1 + \Omega\right)^{4/3}
- \Omega^{1/3} \left(\frac{4}{3}+\Omega\right)\right]\\
\mathcal{U}_0 &= \left[
\frac{\alpha_B^2 n_{\mathrm{II}} Q(\mathrm{H}^0)}
{36 \pi c^3 f_e}\right]^{1/3} \frac{1}{\Omega^{2/3}}\\
&= \frac{4}{9}\Omega^{-2/3} \left[(1+\Omega)^{4/3} -
\Omega^{1/3}\left(\frac{4}{3}+\Omega\right)\right]^{-1}
\mathcal{U}\\
\end{aligned}\end{align*}
These relations may be used to compute any four of the quantities
\(n_{\mathrm{II}}\), \(r_0\), \(r_1\), \(\mathcal{U}\),
\(\mathcal{U}_0\) and \(\Omega\) given the other two.
{\hyperref[slugpy:sec\string-slugpy]{\crossref{\DUrole{std,std-ref}{slugpy -- The Python Helper Library}}}} provides a class \code{hiiregparam} that can be used
to perform such a computation.

Given this background, cloudy\_slug allows the user to specify the
physical properties of the HII region by setting any two of the
following six quantities:
\begin{enumerate}
\item {} 
The photoionized gas density \(n_{\mathrm{II}}\).

\item {} 
The inner radius \(r_0\).

\item {} 
The outer radius \(r_1\).

\item {} 
The volume-averaged ionization parameter \(\mathcal{U}\).

\item {} 
The inner radius ionization parameter \(\mathcal{U}_0\).

\item {} 
The wind parameter \(\Omega\).

\end{enumerate}

The two quantities chosen can be specified exactly, or can be drawn
from a specified PDF. One final option, which is only available in
cluster mode, is to obtain the required quantities from a dynamical
model -- see {\hyperref[cloudy:sssec\string-cloudy\string-dynamical\string-cluster\string-mode]{\crossref{\DUrole{std,std-ref}{Dynamical Mode}}}}.

A few caveats are in order at this point.
\begin{enumerate}
\item {} 
Not all combinations of values are realizable. In addition to the
obvious constraints (e.g., \(r_1 > r_0\)), there are some
subtle ones. For example, for any given ionizing luminosity
\(Q(\mathrm{H}^0)\) and density \(n_{\mathrm{II}}\), the
value of \(\mathcal{U}\) is bounded from above. Increasing the
wind parameter \(\Omega\) can allow arbitrarily small values of
\(\mathcal{U}\), but not arbitrarily large ones. If the user
requests a physically impossible combination of parameters,
cloudy\_slug will correct the parameters to the allowed range and
run, while issuing a warning.

\item {} 
Even for parameters that are not physically impossible, the results
may not be sensible, and may cause cloudy to crash in extreme
cases. For example, if one sets \(\Omega = 0\) and
\(\mathcal{U} = 10^{-4}\), then for an ionizing lumnosity of
\(Q(\mathrm{H}^0) = 10^{50}\) photons/s (typical for a cluster
of \(\sim 10^4M_\odot\)), the corresponding density is
\(n_{\mathrm{II}} \approx 10^{-5}\mbox{ cm}^{-3}\)! As this
density the gas will be fully ionized by cosmic rays and the
extragalactic background, and it makes no sense to think of it as
an HII region. Caution is required.

\item {} 
The parameter combinations \((r_0,\mathcal{U})\) and
\((r_1,\mathcal{U}_0)\) not allowed
because they do not define a unique solution for the other
parameters (the resulting equations have multiple physically-valid
solutions).

\item {} 
The relations given above are only valid if radiation pressure is
not dynamically significant. If it is, then there are no known
analytic relations between the various quantities. The cloudy\_slug
code will still run, and will use the relations above, but the
actual HII region properties may be markedly different from those
requested. In cases where radiation pressure is important, it is
generally advisable to save the HII region physical conditions
output by cloudy to compute quatities from them directly. The
cloudy\_slug script will issue a warning if radiation pressure is
expected to be significant for the HII region being computed. As a rule
of thumb, radiation pressure is significant if

\end{enumerate}
\begin{equation*}
\begin{split}\zeta \equiv \frac{r_{\mathrm{ch}}}{r_1} > 1\end{split}
\end{equation*}
where \(r_{\mathrm{ch}}\) is the
characteristic radius defined by \href{http://adsabs.harvard.edu/abs/2009ApJ...703.1352K}{Krumholz \& Matzner (2009, ApJ,
703, 1352)} as
\begin{equation*}
\begin{split}r_{\mathrm{ch}} & = \frac{\alpha_B}{12\pi\phi}
\left(\frac{\epsilon_0}{2 f_e k_B T_{\mathrm{II}}}\right)^2
f_{\mathrm{trap}}^2 \frac{\psi^2 Q(\mathrm{H}^0)}{c^2}\end{split}
\end{equation*}
Here  \(\phi = 0.73\) is the fraction of ionizing photons absorbed
by hydrogen atoms rather than dust, \(\epsilon_0 =
13.6\;\mathrm{eV}\) is the hydrogen ionization potential,
\(T_{\mathrm{II}} = 10^4\;\mathrm{K}\) is the temperature inside
the HII region, \(f_{\mathrm{trap}} = 2\) is the trapping factor
that accounts for stellar wind and trapped infrared radiation
pressure, and \(\psi = 3.2\) is the mean photon energy in Rydberg for
a fully sampled IMF at zero age.


\subsection{Dynamical Mode}
\label{cloudy:dynamical-mode}\label{cloudy:sssec-cloudy-dynamical-cluster-mode}
In cluster mode, cloudy\_slug allows an additional option to derive the
physical properties of the HII region. They can be computed from a
dynamical model of HII region expansion, taken from \href{http://adsabs.harvard.edu/abs/2009ApJ...703.1352K}{Krumholz \&
Matzner (2009, ApJ, 703, 1352)}.  In this model,
the radius of an HII region can be computed as a function of the
ionizing luminosity \(Q(\mathrm{H}^0)\), ambient hydrogen number
density \(n_{\mathrm{H}}\), and star cluster age \(t\) as
\begin{align*}\!\begin{aligned}
r_1 & = r_{\mathrm{ch}}
\left(x_{\mathrm{rad}}^{7/2} +
x_{\mathrm{gas}}^{7/2}\right)^{2/7} \\\\
x_{\mathrm{rad}} &= (2\tau^2)^{1/4} \\\\
x_{\mathrm{gas}} &= (49\tau^2/36)^{2/7} \\\\
\tau &= t/t_{\mathrm{ch}} \\\\
t_{\mathrm{ch}} & = \left(\frac{4\pi \mu m_{\mathrm{H}}
n_{\mathrm{H}} c r_{\mathrm{ch}}^4}{3 f_{\mathrm{trap}}
Q(\mathrm{H}^0) \psi \epsilon_0}\right)^{1/2}\\
\end{aligned}\end{align*}
Definitions of various quantities appearing in these equations are
given above. The quantity \(\mu = 1.4\) is the mean
mass per hydrogen nucleus for gas of the standard cosmic
composition.

We refer to this method of computing HII region properties as
dynamical mode. In this mode, a user can specify the properties of the
nebula in terms of an ambient density \(n_{\mathrm{H}}\) and
a wind parameter \(\Omega\). All other quantities are
derived from these two and from the ionizing luminosity
\(Q(\mathrm{H}^0)\) and age \(t\) of each cluster. Dynamical
mode can only be used in combination with cluster mode, not integrated
mode, because composite stellar populations do not have well-defined
ages.


\section{The cloudy\_slug Input Template}
\label{cloudy:the-cloudy-slug-input-template}\label{cloudy:ssec-cloudy-template}
The cloudy\_slug interface operates by reading SLUG output spectra and
using them as inputs to a cloudy calculation. However, cloudy
obviously requires many input parameters beyond simply the spectrum of
the input radiation field. These parameters are normally provided by
an input file whose format is as described in the \href{http://nublado.org}{cloudy documentation}. The cloudy\_slug interface works by reading a
\emph{template} input file that specifies all these parameter, and which
will be used as a basis for the final cloudy input files that will
contain the SLUG spectra.

In general the template input file looks just like an ordinary cloudy
input file, subject to the following restrictions:
\begin{enumerate}
\item {} 
The input file \emph{must not} contain any commands that specify the
luminosity, intensity, or the spectral shape. These will be
inserted automatically by the cloudy\_slug script.

\item {} 
The input file \emph{may} contain a radius command specifying the inner
radius of the HII region. If it does not, then the user must
specify the radius in another way, by setting 2 of the 6 inputs
described in {\hyperref[cloudy:sssec\string-cloudy\string-nebular\string-properties]{\crossref{\DUrole{std,std-ref}{Nebular Properties}}}} (for
simulations not done in dynamic mode) or by setting an ambient
density and wind parameters in
{\hyperref[cloudy:sssec\string-cloudy\string-dynamical\string-cluster\string-mode]{\crossref{\DUrole{std,std-ref}{Dynamical Mode}}}}. If the user does
set these quantities, any radius command in the template file will be
ignored, and a warning message will be issued if one is
found. Finally, note that cluster\_slug will only compute derived
parameters correctly from a radius in the template file if the
radius is specified in cloudy's default format, by giving a log of
the radius in cm; the keywords ``linear'' and ``parsecs'' are not
currently supported.

\item {} 
The input file \emph{may} contain a hydrogen density command specifying
the starting hydrogen density. The rules for this are the same as
for the radius command.

\item {} 
Any outputs to be written (specified using the \code{save} or
\code{punch} keywords) must give file names containing the string
\code{\$OUTPUT\_FILENAME}. This string will be replaced by the
cloudy\_slug script to generate a unique file name for each cloudy
run, and to read back these outputs for post-processing.

\item {} 
The cloudy\_slug output will contain output spectra only if the
cloudy input file contains a \code{save last continuum} command. See
{\hyperref[cloudy:ssec\string-cloudy\string-output]{\crossref{\DUrole{std,std-ref}{Full Description of cloudy\_slug Output}}}}.

\item {} 
The cloudy\_slug output will contain output line luminosities only
if the cloudy input file contains a \code{save last line list emergent
absolute column} command. See {\hyperref[cloudy:ssec\string-cloudy\string-output]{\crossref{\DUrole{std,std-ref}{Full Description of cloudy\_slug Output}}}}.

\item {} 
The cloudy\_slug output will contain output physical conditions and
dimensionless values only if the cloudy input file contains a
\code{save last hydrogen conditions} command. See
{\hyperref[cloudy:ssec\string-cloudy\string-output]{\crossref{\DUrole{std,std-ref}{Full Description of cloudy\_slug Output}}}}.

\item {} 
If any other outputs are produced by the input file, they will
neither be processed nor moved, deleted, or otherwise changed by
the cloudy\_slug script.

\item {} 
Running cloudy in grid mode is not currently supported.

\end{enumerate}

An example cloudy input file with reasonable parameter choices is
provided as \code{cloudy\_slug/cloudy\_in.template} in the main directory
of the SLUG repository.

In addition to the input file, the default template makes use of a
cloudy line list file to specify which line luminosities should be
output (see the \href{http://nublado.org}{cloudy documentation} for
details). The template points to the file
\code{cloudy\_slug/LineList\_HII.data} (which is identical to cloudy's
default line list for HII regions), but any other valid cloudy line
list file would work as well.


\section{The cloudy\_slug Interface Script}
\label{cloudy:ssec-cloudy-slug-options}\label{cloudy:the-cloudy-slug-interface-script}
The \code{cloudy\_slug.py} script provides the interface between SLUG and
cloudy. Usage for this script is as follows:

\begin{Verbatim}[commandchars=\\\{\}]
cloudy\PYGZus{}slug.py [\PYGZhy{}h] [\PYGZhy{}a AGEMAX] [\PYGZhy{}\PYGZhy{}cloudypath CLOUDYPATH]
               [\PYGZhy{}\PYGZhy{}cloudytemplate CLOUDYTEMPLATE] [\PYGZhy{}cm]
               [\PYGZhy{}cf COVERINGFAC] [\PYGZhy{}d] [\PYGZhy{}hd HDEN] [\PYGZhy{}ip IONPARAM]
               [\PYGZhy{}ip0 IONPARAM0] [\PYGZhy{}ipm IONPARAMMAX]
               [\PYGZhy{}\PYGZhy{}ionparammin IONPARAMMIN] [\PYGZhy{}nl NICELEVEL]
               [\PYGZhy{}n NPROC] [\PYGZhy{}ps PARAMSAFETY] [\PYGZhy{}qm QH0MIN] [\PYGZhy{}r0 R0]
               [\PYGZhy{}r1 R1] [\PYGZhy{}s] [\PYGZhy{}\PYGZhy{}slugformat SLUGFORMAT]
               [\PYGZhy{}\PYGZhy{}slugpath SLUGPATH] [\PYGZhy{}t TMPDIR] [\PYGZhy{}v] [\PYGZhy{}wp WINDPARAM]
               [\PYGZhy{}wr]
               slug\PYGZus{}model\PYGZus{}name [start\PYGZus{}spec] [end\PYGZus{}spec]
\end{Verbatim}

The positional arguments are as follows:
\begin{itemize}
\item {} 
\code{slug\_model\_name}: this is the name of the SLUG output to be used
as a basis for the cloudy calculation. This should be the same as
the \code{model\_name} parameter used in the SLUG simulation, with the
optional addition of a path specification in front.

\item {} 
\code{start\_spec}: default behavior is to run cloudy on all the
integrated spectra or cluster spectra (see
{\hyperref[cloudy:sssec\string-cloudy\string-integrated\string-cluster]{\crossref{\DUrole{std,std-ref}{Integrated versus Cluster Spectra}}}}). If this
argument is set, cloudy will only be run in spectra starting with
the specified trial number or cluster number; numbers are
0-offset, so the first trial/cluster is 0, the next is 1, etc.

\item {} 
\code{end\_spec}: same as \code{start\_spec}, but specifying the last
cluster to be processed. Per standard python convention, the spectra
processed will go up to but not include \code{end\_spec}.

\end{itemize}

The following optional arguments control paths and file locations:
\begin{itemize}
\item {} 
\code{-{-}cloudypath CLOUDYPATH}: path to the cloudy executable; default
is \code{\$CLOUDY\_DIR/cloudy.exe}

\item {} 
\code{-{-}cloudytemplate CLOUDYTEMPLATE}: cloudy input file template (see
{\hyperref[cloudy:ssec\string-cloudy\string-template]{\crossref{\DUrole{std,std-ref}{The cloudy\_slug Input Template}}}}); default is
\code{\$SLUG\_DIR/cloudy\_slug/cloudy.in\_template}

\item {} 
\code{-{-}slugformat SLUGFORMAT}: the format of slug output data to use;
valid values are \code{ascii}, \code{bin}, \code{binary}, and \code{fits}. By
default \code{cloudy\_slug} checks for any output whose name and path
match the model name and search path, regardless of format.

\item {} 
\code{-{-}slugpath SLUGPATH}: path to the SLUG output data. If not set,
cloudy\_slug searches for an appropriately-named set of output files
first in the current working directory, and next in
\code{\$SLUG\_DIR/output}

\item {} 
\code{-t TMPDIR, -{-}tmpdir TMPDIR}: location of the temporary directory
where temporary files should be stored; defaults to
\titleref{./cloudy\_tmp\_MODEL\_NAME}.

\end{itemize}

The following arguments control how HII regions are processed:
\begin{itemize}
\item {} 
\code{-a AGEMAX, -{-}agemax AGEMAX}: maximum cluster age in Myr for
cloudy computation. Cloudy will not be run on clusters older than
this value, and the predicted nebular emission for such clusters
will be recorded as zero. Default value is 10 Myr. This argument only
has an effect if running in cluster mode (see
{\hyperref[cloudy:sssec\string-cloudy\string-integrated\string-cluster]{\crossref{\DUrole{std,std-ref}{Integrated versus Cluster Spectra}}}}); otherwise it is ignored.

\item {} 
\code{-cf COVERINGFRAC, -{-}coveringfrac COVERINGFRAC}: this sets the
covering fraction of the HII region, i.e., the fraction of ionizing
photons that are assumed to produce nebular emission; the output
luminosity is decreased by a factor of the covering fraction

\item {} 
\code{-cm, -{-}clustermode}: if this argument is set, then cloudy\_slug
will process cluster spectra; the default behavior is to process
integrated spectra

\item {} 
\code{-{-}ionparammax IONPARAMMAX}: maximum value for the inner radius
ionization parameter \(\mathcal{U}_0\). If the value falls
outside this range, the behavior is controlled by the setting of the
\code{paramsafety} option (see below).

\item {} 
\code{-{-}ionparammmin IONPARAMMIN}: same as \code{ionparammax}, but sets a
minimum instead of a maximum.

\item {} 
\code{-qm QH0MIN, -{-}qH0min QH0MIN}: minimum ionizing luminosity for
which to run cloudy (default = 0). As with \code{-{-}agemax}, for
clusters / times where cloudy is not run, that case will still
appear in the output, but the nebular spectra and nebular line
luminosities will all be set to zero.

\end{itemize}

The following parameters specify the physical properties of HII
regions, as explained in {\hyperref[cloudy:sssec\string-cloudy\string-nebular\string-properties]{\crossref{\DUrole{std,std-ref}{Nebular Properties}}}}. Parameters can be set to either fixed values, or to the
names of PDF files. Any numerical value given is interpreted as a
fixed constant, while non-numerical values are interpreted as the
names of PDF files that specify a PDF from which the corresponding
parameter is to be drawn. See {\hyperref[pdfs:sec\string-pdfs]{\crossref{\DUrole{std,std-ref}{Probability Distribution Functions}}}} for details on PDF file
formats.
\begin{itemize}
\item {} 
\code{-hd HDEN, -{-}hden HDEN}: hydrogen density in HII region,
\(n_{\mathrm{II}}\)

\item {} 
\code{-ip IONPARAM, -{-}ionparam IONPARAM}: volume-averaged ionization
parameter, \(\mathcal{U}\)

\item {} 
\code{-ip0 IONPARAM, -{-}ionparam0 IONPARAM0}: ionization parameter at
HII region inner edge, \(\mathcal{U}_0\)

\item {} 
\code{-r0 R0}: inner radius of the HII region

\item {} 
\code{-r1 R1}: outer radius of the HII region

\item {} 
\code{-wp WINDPARAM, -{-}windparam WINDPARAM}: wind parameter,
\(\Omega\)

\end{itemize}

The following arguments control general code behavior:
\begin{itemize}
\item {} 
\code{-h, -{-}help}: prints a help message and then exits

\item {} 
\code{-nl NICELEVEL, -{-}nicelevel NICELEVEL}: if this is set, then the
cloudy processes launched by the script will be run at this nice
level. If it is not set, they will not be nice'd. Note that this
option will only work correctly on platforms that support nice.

\item {} 
\code{-n NPROC, -{-}nproc NPROC}: number of simultaneous cloudy processes
to run; default is the number of cores available on the system

\item {} 
\code{-{-}ps PARAMSAFETY, -{-}paramsafety PARAMSAFETY}: specifies how to
handle situations where the combination of input HII region
parameters is not physically allowed, or falls outside the bounds
set by \code{ionparammin} and \code{ionparammax}. Available options are:
\begin{itemize}
\item {} 
\code{warn}: one of the input parameters is adjusted to bring it
to a physically-allowed value, and a warning is issued; the run
continues. This is the default behaviour.

\item {} 
\code{skip}: runs with unphysical parameter choices are skipped; the
parameters that were chosen are recorded in the output file, but
the output spectrum, all line luminosities, and all other
parameters are set to 0, and cloudy is not run.

\item {} 
\code{halt}: if a forbidden parameter combination is found, the
entire cloudy\_slug run is halted.

\item {} 
\code{redraw}: if a forbidden parameter combination is found, and
one or more parameters are being drawn from a PDF, a new set of
parameters will be drawn from the PDF. Redrawing will continue up
to 100 times until a physically-allowed parameter combination is
found. If no valid parameter combination is found after 100
attempts, revert to \code{skip}.

\end{itemize}

\item {} 
\code{-s, -{-}save}: by default, cloudy\_slug will extract line and
spectral data from the cloudy outputs and store them as described in
{\hyperref[cloudy:ssec\string-cloudy\string-output]{\crossref{\DUrole{std,std-ref}{Full Description of cloudy\_slug Output}}}}, then delete the cloudy output files. If
this option is set, the cloudy output files will NOT be deleted, and
will be left in place, in sub-directory of the working directory
called \code{cloudy\_tmp\_MODEL\_NAME} where \code{MODEL\_NAME} is the SLUG model
name. WARNING: cloudy's outputs are written in ASCII and are quite
voluminous, so choose this option only if you are running
cloudy on a small number of SLUG spectra and/or you are prepared to
store hundreds of GB or more. The data that \code{cloudy\_slug} extract
are much, much smaller, and (if you do not use ASCII format) are
stored in a much more compact form.

\item {} 
\code{-v, -{-}verbose}: if this option is set, cloudy\_slug produces
verbose output as it runs

\item {} 
\code{-wr, -{-}writeparams}: if set, this option causes \code{cloudy\_slug}
to write out a file beginning with \code{cloudy\_slug.param} for each
cloudy run. This file is written in the same directory used by the
save command, and it contains an ASCII printout of the various
parameters. This option is only applied if \code{-{-}save} is also set.

\end{itemize}


\section{Full Description of cloudy\_slug Output}
\label{cloudy:ssec-cloudy-output}\label{cloudy:full-description-of-cloudy-slug-output}
The cloudy\_slug script will automatically process the cloudy output
and produce a series of new output files, which will be written to the
same directory where the input SLUG files are located, and using the
same output mode (ASCII text, raw binary, or FITS -- see
{\hyperref[output:sec\string-output]{\crossref{\DUrole{std,std-ref}{Output Files and Format}}}}). If cloudy\_slug is called to process integrated
spectra, the four output files will be
\code{MODEL\_NAME\_integrated\_cloudyparams.ext},
\code{MODEL\_NAME\_integrated\_cloudylines.ext},
\code{MODEL\_NAME\_integrated\_cloudyphot.ext}, and
\code{MODEL\_NAME\_integrated\_cloudyspec.ext}, where the extension \code{.ext}
is one of \code{.txt}, \code{.bin}, or \code{.fits}, depending on the
\code{output\_mode}. If cloudy\_slug is run on cluster spectra, the four
output files will be
\code{MODEL\_NAME\_cluster\_cloudyparams.ext},
\code{MODEL\_NAME\_cluster\_cloudylines.ext},
\code{MODEL\_NAME\_cluster\_cloudyphot.ext}, and
\code{MODEL\_NAME\_cluster\_cloudyspec.ext}. All of these output files will
be read and processed automatically if the outputs are read using
\code{read\_integrated} or \code{read\_cluster} in the {\hyperref[slugpy:sec\string-slugpy]{\crossref{\DUrole{std,std-ref}{slugpy -- The Python Helper Library}}}}
library.

The format of these files is described below.


\subsection{The \texttt{integrated\_cloudyparams} File}
\label{cloudy:the-integrated-cloudyparams-file}
This file contains the input parameters for the cloudy runs, and
quantities derived from them. All parameters are as defined in
{\hyperref[cloudy:sssec\string-cloudy\string-nebular\string-properties]{\crossref{\DUrole{std,std-ref}{Nebular Properties}}}}. The output file consists of a
series of entries containin the following fields:
\begin{itemize}
\item {} 
\code{Trial}: which trial these data are from

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{Hden}: number density of hydrogen nuclei at the inner edge of the
HII region, in H/cm\textasciicircum{}3

\item {} 
\code{R0}: radius of the inner edge of the HII region, in cm

\item {} 
\code{R1}: radius of the outer edge of the HII region, in cm

\item {} 
\code{QH0}: ionizing luminosity input to cloudy, in photons/s

\item {} 
\code{CovFac}: covering factor used

\item {} 
\code{U}: volume-averaged ionization parameter \(\mathcal{U}\)

\item {} 
\code{U0}: inner edge ionization parameter \(\mathcal{U}_0\)

\item {} 
\code{Omega}: wind parameter \(\Omega\)

\item {} 
\code{zeta}: radiation pressure parameter \(\zeta\)

\end{itemize}

It will also contain the following fields if the cloudy template file
includes a \code{save last hydrogen conditions} command:
\begin{itemize}
\item {} 
\code{Hden\_out}: mean H number density for the HII region structure
computed by cloudy, in H/cm\textasciicircum{}3; the average is weighted by the
ionized volume, i.e., it is weighted by \(x\, dV\), where
\(x\) is the hydrogen ionization fraction.

\item {} 
\code{R1\_out}: HII region outer radius returned by cloudy

\item {} 
\code{Omega\_out}: wind parameter \(\Omega\), computed using
\code{R1\_out} instead of \code{R1}

\item {} 
\code{zeta\_out}: radiation pressure parameter \(\zeta\), computing
using \code{R1\_out} instead of \code{R1}

\end{itemize}

If the SLUG data input to cloudy\_slug were written in \code{ascii} mode,
these data are output as a text file containing a series of columns,
with different trials separated by lines of dashes.

If the SLUG data input to cloudy\_slug were written in \code{fits} mode,
the data are written in a FITS file containing a binary table
extension. The table contains one column whose name corresponds to the
list of fields above.

If the SLUG data input to cloudy\_slug were writtin in \code{binary} mode,
these data are written in a raw binary file that is formatted as
follows. First, there are four bytes specifying if the optional fields
are included:
\begin{itemize}
\item {} 
\code{Hden\_out\_set} (byte): 0 if the data do not include \code{Hden\_out}, 1 if
they do include it

\item {} 
\code{R1\_out\_set} (byte): 0 if the data do not include \code{R1\_out}, 1 if
they do include it

\item {} 
\code{Omega\_out\_set} (byte): 0 if the data do not include \code{Omega\_out}, 1 if
they do include it

\item {} 
\code{zeta\_out\_set} (byte): 0 if the data do not include \code{zeta\_out}, 1 if
they do include it

\end{itemize}

This is followed by a series of records containing the following fields:
\begin{itemize}
\item {} 
\code{Trial} (numpy \code{uint64})

\item {} 
\code{Time} (numpy \code{float64})

\item {} 
\code{Hden} (numpy \code{float64})

\item {} 
\code{R0} (numpy \code{float64})

\item {} 
\code{R1} (numpy \code{float64})

\item {} 
\code{QH0} (numpy \code{float64})

\item {} 
\code{covFac} (numpy \code{float64})

\item {} 
\code{U} (numpy \code{float64})

\item {} 
\code{U0} (numpy \code{float64})

\item {} 
\code{Omega} (numpy \code{float64})

\item {} 
\code{zeta} (numpy \code{float64})

\item {} 
\code{Hden\_out} (numpy \code{float64}; optional, only if the relevant byte
in the header is set to )

\item {} 
\code{R1\_out} (numpy \code{float64}; optional, only if the relevant byte
in the header is set to )

\item {} 
\code{Omega\_out} (numpy \code{float64}; optional, only if the relevant byte
in the header is set to )

\item {} 
\code{zeta\_out} (numpy \code{float64}; optional, only if the relevant byte
in the header is set to 1)

\end{itemize}

There is one such record for each output time, with different trials
ordered sequentially, so that all the times for one trial are output
before the first time for the next trial.


\subsection{The \texttt{integrated\_cloudylines} File}
\label{cloudy:the-integrated-cloudylines-file}
This file contains data on the nebular line emission produced by the
interaction of the stellar radiation field with the ISM. It consists
of a series of entries containing the following fields:
\begin{itemize}
\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{LineLabel}: four letter code labeling each line. These codes
are the codes used by cloudy (see the \href{http://nublado.org}{cloudy documentation})

\item {} 
\code{Wavelength}: wavelength of the line, in Angstrom. Note that
default cloudy behavior is to round wavelengths to the nearest
Angstrom.

\item {} 
\code{Luminosity}: line luminosity, in erg/s

\end{itemize}

If the SLUG data input to cloudy\_slug were written in \code{ascii} mode,
these data are output as a text file containing a series of columns,
with different trials separated by lines of dashes.

If the SLUG data input to cloudy\_slug were written in \code{fits} mode,
the data are written in a FITS file containing two binary table
extensions. The first extension contains two fields, \code{Line\_label} and
\code{Wavelength}, giving the four-letter cloudy line codes and central
wavelengths. The second extension contains three columns, giving the
trial number, time, and line luminosity for each line at each time in
each trial.

If the SLUG data input to cloudy\_slug were written in \code{binary} mode,
the data are written in a raw binary file. The file starts with a
header consisting of
\begin{itemize}
\item {} 
\code{NLine} (python \code{int}, equivalent to C \code{long}): number of lines

\item {} 
\code{LineLabel} (\code{NLine} entries stored as \code{ASCII text}): line
labels listed in ASCII, one label per line

\end{itemize}

This is followed by a series of entries of the form
\begin{itemize}
\item {} 
\code{Trial} (numpy \code{uint64})

\item {} 
\code{Time} (numpy \code{float64})

\item {} 
\code{LineLum} (\code{NLine} entries of type numpy \code{float64})

\end{itemize}

There is one such record for each output time, with different trials
ordered sequentially, so that all the times for one trial are output
before the first time for the next trial.


\subsection{The \texttt{integrated\_cloudyspec} File}
\label{cloudy:the-integrated-cloudyspec-file}\label{cloudy:sssec-int-cloudyspec-file}
This file contains data on the spectrum produced by interaction
between the stellar radiation field and the nebula. Each entry in the
output file contains the folling fields:
\begin{itemize}
\item {} 
\code{Trial}: which trial these data are from

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{Wavelength}: the wavelength at which the spectrum is evaluated,
in Angstrom

\item {} 
\code{Incident}: specific luminosity in erg/s/Angstrom at the specified
wavelength. In cloudy's terminology, this is the \emph{incident}
spectrum, i.e., the stellar radiation field entering the nebula. It
should be the same as the spectrum contained in the SLUG
\code{integrated\_spec} file for the corresponding time and trial,
except interpolated onto the wavelength grid used by cloudy.

\item {} 
\code{Transmitted}:  specific luminosity in erg/s/Angstrom at the specified
wavelength. In cloudy's terminology, this is the \emph{transmitted}
spectrum, i.e., the stellar spectrum exiting the HII region, not
including any emission produced within the nebula. This is what
would be detected by an observing aperture that included only the
stars, and none of the nebula.

\item {} 
\code{Emitted}:  specific luminosity in erg/s/Angstrom at the specified
wavelength. In cloudy's terminology, this is the \emph{emitted}
spectrum, i.e., the spectrum emitted by the diffuse gas in the HII
region, excluding any light from the stars themselves. This is what
would be seen by an observer whose aperture covered the nebula, but
masked the stars.

\item {} 
\code{Transmitted\_plus\_emitted}: this is just the sum of
\code{Transmitted} and \code{Emitted}. It represents what would be
observed in an aperture including both the stars and the HII
region.

\end{itemize}

If the SLUG data input to cloudy\_slug were written in \code{ascii} mode,
these data are output as a text file containing a series of columns,
with different trials separated by lines of dashes.

If the SLUG data input to cloudy\_slug were written in \code{fits} mode,
these data are written in a FITS file containing two binary table
extensions. The first extension contains one field, \code{Wavelength},
which gives the wavelengths of the spectra in Angstrom. The second
extension contains six fields: \code{Trial}, \code{Time},
\code{Incident\_spectrum}, \code{Transmitted\_spectrum}, \code{Emitted\_spectrum},
and \code{Transmitted\_plus\_emitted\_spectrum}. The first two of these give
the trial number and time, and the remaining four give the incident,
transmitted, emitted, and transmitted plus emitted spectra for the
corresponding time and trial.

If the SLUG data input to cloudy\_slug were written in \code{binary} mode,
these data are written in a raw binary file that is formatted as
follows. The file begins with a header consisting of
\begin{itemize}
\item {} 
\code{NWavelength} (numpy \code{int64}): number of wavelengths

\item {} 
\code{Wavelength} (\code{NWavelength} entries of numpy \code{float64})

\end{itemize}

and then contains a series of records of the form
\begin{itemize}
\item {} 
\code{Trial} (numpy \code{uint64})

\item {} 
\code{Time} (numpy \code{float64})

\item {} 
\code{Incident} (\code{NWavelength} entries of numpy \code{float64})

\item {} 
\code{Transmitted} (\code{NWavelength} entries of numpy \code{float64})

\item {} 
\code{Emitted} (\code{NWavelength} entries of numpy \code{float64})

\item {} 
\code{Transmitted\_plus\_emitted} (\code{NWavelength} entries of numpy
\code{float64})

\end{itemize}

There is one such record for each output time, with different trials ordered sequentially, so that all the times for one trial are output before the first time for the next trial.


\subsection{The \texttt{integrated\_cloudyphot} File}
\label{cloudy:the-integrated-cloudyphot-file}
This file contains photometric data computed for the spectra produced
by the interaction between the stellar radiation field and the HII
region. The file consists of a series of entries containing the
following fields:
\begin{itemize}
\item {} 
\code{Trial}: which trial these data are from

\item {} 
\code{Time}: evolution time at which the output is computed

\item {} 
\code{PhotFilter1\_trans}: photometric value for the \emph{Transmitted}
radiation field through filter 1, where filter 1 here is the same as
filter 1 in {\hyperref[output:ssec\string-int\string-phot\string-file]{\crossref{\DUrole{std,std-ref}{The integrated\_phot File}}}}; units are also the same as
in that file.

\item {} 
\code{PhotFilter1\_emit}: photometric value for the \emph{Emitted}
radiation field through filter 1

\item {} 
\code{PhotFilter1\_trans\_emit}: photometric value for the
\emph{Transmitted\_plus\_emitted} radiation field through filter 1

\item {} 
\code{PhotFilter2\_trans}

\item {} 
\code{PhotFilter2\_emit}

\item {} 
\code{PhotFilter2\_trans\_emit}

\item {} 
\code{...}

\end{itemize}

For distinctions between the \emph{Transmitted}, \emph{Emitted}, and
\emph{Transmitted\_plus\_emitted} radiation fields, see
{\hyperref[cloudy:sssec\string-int\string-cloudyspec\string-file]{\crossref{\DUrole{std,std-ref}{The integrated\_cloudyspec File}}}}, or the \href{http://nublado.org}{cloudy documentaiton}. Note that we do not record photometry for the
incident spectrum, since that would be, up to the accuracy of the
numerical integration, identical to the photometry already recorded in
the {\hyperref[output:ssec\string-int\string-phot\string-file]{\crossref{\DUrole{std,std-ref}{The integrated\_phot File}}}}.

If the SLUG data input to cloudy\_slug were written in \code{ascii} mode,
these data are output as a text file containing a series of columns,
with different trials separated by lines of dashes.

If the SLUG data input to cloudy\_slug were written in \code{fits} mode,
these data are written in a FITS file containing one binary table
extension, consisting of a series of columns. The columns are
\code{Trial}, \code{Time}, \code{Filter1\_Transmitted}, \code{Filter1\_Emitted},
\code{Filter1\_Transmitted\_plus\_emitted}, \code{...}. The first two columns
give the trial number and the time, and the remainder give the
photometric values for the transmitted, emitted, and transmitted plus
emitted spectra in each filter.

If the SLUG data input to cloudy\_slug were written in \code{binary} mode,
these data are written to a raw binary file that is formatted as
follows. The file starts with an ASCII header consisting of the
following, each on a separate line:
\begin{itemize}
\item {} 
\code{NFilter} (stored as \code{ASCII text}): number of filters used

\item {} 
\code{FilterName} \code{FilterUnit} (\code{NFilter} entries stored as \code{ASCII
text}): the name and units for each filter are listed in ASCII, one
filter-unit pair per line

\end{itemize}

This is followed by a series of entries of the form:
\begin{itemize}
\item {} 
\code{PhotFilter\_Transmitted} (\code{NFilter} entries of numpy
\code{float64}), giving the transmitted photometry in each filter

\item {} 
\code{PhotFilter\_Emitted} (\code{NFilter} entries of numpy
\code{float64}), giving the emitted photometry in each filter

\item {} 
\code{PhotFilter\_Transmitted\_plus\_emitted} (\code{NFilter} entries of numpy
\code{float64}), giving the transmitted plus emitted photometry in each
filter

\end{itemize}

There is one such record for each output time, with different trials
ordered sequentially, so that all the times for one trial are output
before the first time for the next trial.


\subsection{The \texttt{cluster\_cloudyparams} File}
\label{cloudy:the-cluster-cloudyparams-file}
This file contains the input parameters for the cloudy runs, and
quantities derived from them. It consists of a series of entries
containin the following fields:
\begin{itemize}
\item {} 
\code{UniqueID}: a unique identifier number for each cluster that is
preserved across times and output files

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{Hden}: number density of hydrogen nuclei at the inner edge of the
HII region whose structure cloudy computes, in H/cm\textasciicircum{}3

\item {} 
\code{R0}: radius of the inner edge of the HII region, in cm

\item {} 
\code{R1}: radius of the outer edge of the HII region, cm

\item {} 
\code{QH0}: ionizing luminosity input to cloudy, in photons/s

\item {} 
\code{CovFac}: covering factor used

\item {} 
\code{U}: volume-averaged ionization parameter \(\mathcal{U}\)

\item {} 
\code{U0}: inner edge ionization parameter \(\mathcal{U}_0\)

\item {} 
\code{Omega}: wind parameter \(\Omega\)

\item {} 
\code{zeta}: radiation pressure parameter \(\zeta\)

\end{itemize}

It will also contain the following fields if the cloudy template file
includes a \code{save last hydrogen conditions} command:
\begin{itemize}
\item {} 
\code{Hden\_out}: mean H number density for the HII region structure
computed by cloudy, in H/cm\textasciicircum{}3; the average is weighted by the
ionized volume, i.e., it is weighted by \(x\, dV\), where
\(x\) is the hydrogen ionization fraction.

\item {} 
\code{R1\_out}: HII region outer radius returned by cloudy

\item {} 
\code{Omega\_out}: wind parameter \(\Omega\), computed using
\code{R1\_out} instead of \code{R1}

\item {} 
\code{zeta\_out}: radiation pressure parameter \(\zeta\), computing
using \code{R1\_out} instead of \code{R1}

\end{itemize}

If the SLUG data input to cloudy\_slug were written in \code{ascii} mode,
these data are output as a text file containing a series of columns,
with different trials separated by lines of dashes.

If the SLUG data input to cloudy\_slug were written in \code{fits} mode,
the data are written in a FITS file containing a binary table
extension. The table contains one column whose name corresponds to the
list of fields above.

If the SLUG data input to cloudy\_slug were writtin in \code{binary} mode,
these data are written in a raw binary file that is formatted as
follows. First, there are four bytes specifying if the optional fields
are included:
\begin{itemize}
\item {} 
\code{Hden\_out\_set} (byte): 0 if the data do not include \code{Hden\_out}, 1 if
they do include it

\item {} 
\code{R1\_out\_set} (byte): 0 if the data do not include \code{R1\_out}, 1 if
they do include it

\item {} 
\code{Omega\_out\_set} (byte): 0 if the data do not include \code{Omega\_out}, 1 if
they do include it

\item {} 
\code{zeta\_out\_set} (byte): 0 if the data do not include \code{zeta\_out}, 1 if
they do include it

\end{itemize}

Next there are a series of records, one for each output time, with
different trials ordered sequentially, so that all the times for one
trial are output before the first time for the next trial. Each record
consists of a header containing
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{NCluster} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): number of non-disrupted clusters present at this time

\end{itemize}

This is followed by \code{NCluster} entries of the following form:
\begin{itemize}
\item {} 
\code{UniqueID} (numpy \code{uint64})

\item {} 
\code{Time} (numpy \code{float64})

\item {} 
\code{Hden} (numpy \code{float64})

\item {} 
\code{R0} (numpy \code{float64})

\item {} 
\code{R1} (numpy \code{float64})

\item {} 
\code{QH0} (numpy \code{float64})

\item {} 
\code{covFac} (numpy \code{float64})

\item {} 
\code{U} (numpy \code{float64})

\item {} 
\code{U0} (numpy \code{float64})

\item {} 
\code{Omega} (numpy \code{float64})

\item {} 
\code{zeta} (numpy \code{float64})

\item {} 
\code{Hden\_out} (numpy \code{float64}; optional, only if the relevant byte
in the header is set to )

\item {} 
\code{R1\_out} (numpy \code{float64}; optional, only if the relevant byte
in the header is set to )

\item {} 
\code{Omega\_out} (numpy \code{float64}; optional, only if the relevant byte
in the header is set to )

\item {} 
\code{zeta\_out} (numpy \code{float64}; optional, only if the relevant byte
in the header is set to 1)

\end{itemize}


\subsection{The \texttt{cluster\_cloudylines} File}
\label{cloudy:the-cluster-cloudylines-file}
This file contains data on the nebular line emission produced by the
interaction of the stellar radiation field with the ISM around each
cluster. It consists of a series of entries containing the following
fields:
\begin{itemize}
\item {} 
\code{UniqueID}: a unique identifier number for each cluster that is
preserved across times and output files

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{LineLabel}: four letter code labeling each line. These codes
are the codes used by cloudy (see the \href{http://nublado.org}{cloudy documentation})

\item {} 
{}`{}` Wavelength{}`{}`: wavelength of the line, in Angstrom. Note that
default cloudy behavior is to round wavelengths to the nearest
Angstrom.

\item {} 
\code{Luminosity}: line luminosity, in erg/s

\end{itemize}

If the SLUG data input to cloudy\_slug were written in \code{ascii} mode,
these data are output as a text file containing a series of columns,
with different trials separated by lines of dashes.

If the SLUG data input to cloudy\_slug were written in \code{fits} mode,
the data are written in a FITS file containing two binary table
extensions. The first extension contains two fields, \code{Line\_label} and
\code{Wavelength}, giving the four-letter cloudy line codes and central
wavelengths. The second extension contains four columns, giving the
unique ID, trial number, time, and line luminosity for each line at
each time in each trial.

If the SLUG data input to cloudy\_slug were written in \code{binary} mode,
the data are written in a raw binary file. The file starts with a
header consisting of
\begin{itemize}
\item {} 
\code{NLine} (python \code{int}, equivalent to C \code{long}): number of lines

\item {} 
\code{LineLabel} (\code{NLine} entries stored as \code{ASCII text}): line
labels listed in ASCII, one label per line

\end{itemize}

This is followed by a series of records, one for each output time,
with different trials ordered sequentially, so that all the times for
one trial are output before the first time for the next trial. Each
record consists of a header containing
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{NCluster} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): number of non-disrupted clusters present at this time

\end{itemize}

This is followed by \code{NCluster} entries of the following form:
\begin{itemize}
\item {} 
\code{UniqueID} (numpy \code{uint64})

\item {} 
\code{LineLum} (\code{NLine} entries of numpy \code{float64})

\end{itemize}


\subsection{The \texttt{cluster\_cloudyspec} File}
\label{cloudy:the-cluster-cloudyspec-file}
This file contains data on the spectra produced by the interaction of
the stellar radiation field with the ISM around each cluster. It
consists of a series of entries containing the following fields:
\begin{itemize}
\item {} 
\code{UniqueID}: a unique identifier number for each cluster that is
preserved across times and output files

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{Wavelength}: observed frame wavelength at which the spectrum is evaluated

\item {} 
\code{Incident}: specific luminosity in erg/s/Angstrom at the specified
wavelength for the \emph{incident} radiation field

\item {} 
\code{Transmitted}: specific luminosity in erg/s/Angstrom at the specified
wavelength for the \emph{transmitted} radiation field

\item {} 
\code{Emitted}: specific luminosity in erg/s/Angstrom at the specified
wavelength for the \emph{emitted} radiation field

\item {} 
\code{Transmitted\_plus\_emitted}: specific luminosity in erg/s/Angstrom
at the specified wavelength for the \emph{transmitted plus emitted}
radiation field

\end{itemize}

For explanations of the distinction between the incident, transmitted,
emitted, and transmitted plus emitted radiation fields, see
{\hyperref[cloudy:sssec\string-int\string-cloudyspec\string-file]{\crossref{\DUrole{std,std-ref}{The integrated\_cloudyspec File}}}}.

If the SLUG data input to cloudy\_slug were written in \code{ascii} mode,
these data are output as a text file containing a series of columns,
with different trials separated by lines of dashes.

If the SLUG data input to cloudy\_slug were written in \code{fits} mode,
these data are written in a FITS file containing two binary table
extensions. The first table contains a column \code{Wavelength} listing
the wavelengths at which the spectra are given. The second table
consists of seven columns: \code{Trial}, \code{UniqueID}, \code{Time},
\code{Incident\_spectrum}, \code{Transmitted\_spectrum}, \code{Emitted\_spectrum},
and \code{Transmitted\_plus\_emitted\_spectrum}. The first three of these
give the trial number, unique ID of the cluster, and the time. The
remaining four give the incident, transmitted, emitted, and
transmitted plus emitted spectra for the corresponding cluster.

If the SLUG data input to cloudy\_slug were written in \code{binary} mode,
these data are written to a raw binary file formatted as follows. The
file starts with
\begin{itemize}
\item {} 
\code{NWavelength} (numpy \code{int64}): the number of wavelength entries in the spectra

\item {} 
\code{Wavelength} (\code{NWavelength} entries of type \code{double})

\end{itemize}

and then contains a series of records, one for each output time, with
different trials ordered sequentially, so that all the times for one
trial are output before the first time for the next trial. Each record
consists of a header containing
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{NCluster} (python \code{int}): number of non-disrupted clusters present at this time

\end{itemize}

This is followed by \code{NCluster} entries of the following form:
\begin{itemize}
\item {} 
\code{UniqueID} (\code{unsigned long})

\item {} 
\code{Incident} (\code{NWavelength} entries of numpy \code{float64})

\item {} 
\code{Transmitted} (\code{NWavelength} entries of numpy \code{float64})

\item {} 
\code{Emitted} (\code{NWavelength} entries of numpy \code{float64})

\item {} 
\code{Transmitted\_plus\_emitted} (\code{NWavelength} entries of numpy
\code{float64})

\end{itemize}


\subsection{The \texttt{cluster\_cloudyphot} File}
\label{cloudy:the-cluster-cloudyphot-file}
This file contains data on the photometry of the spectra produced by
the interaction of the stellar radiation field with the ISM around
each cluster. It consists of a series of entries containing the
following fields:
\begin{itemize}
\item {} 
\code{UniqueID}: a unique identifier number for each cluster that is
preserved across times and output files

\item {} 
\code{Time}: evolution time at which the output is produced

\item {} 
\code{PhotFilter1\_trans}: photometric value for the \emph{Transmitted}
radiation field through filter 1, where filter 1 here is the same as
filter 1 in {\hyperref[output:ssec\string-int\string-phot\string-file]{\crossref{\DUrole{std,std-ref}{The integrated\_phot File}}}}; units are also the same as
in that file.

\item {} 
\code{PhotFilter1\_emit}: photometric value for the \emph{Emitted}
radiation field through filter 1

\item {} 
\code{PhotFilter1\_trans\_emit}: photometric value for the
\emph{Transmitted\_plus\_emitted} radiation field through filter 1

\item {} 
\code{PhotFilter2\_trans}

\item {} 
\code{PhotFilter2\_emit}

\item {} 
\code{PhotFilter2\_trans\_emit}

\item {} 
\code{...}

\end{itemize}

For distinctions between the \emph{Transmitted}, \emph{Emitted}, and
\emph{Transmitted\_plus\_emitted} radiation fields, see
{\hyperref[cloudy:sssec\string-int\string-cloudyspec\string-file]{\crossref{\DUrole{std,std-ref}{The integrated\_cloudyspec File}}}}, or the \href{http://nublado.org}{cloudy documentaiton}. Note that we do not record photometry for the
incident spectrum, since that would be, up to the accuracy of the
numerical integration, identical to the photometry already recorded in
the {\hyperref[output:ssec\string-cluster\string-phot\string-file]{\crossref{\DUrole{std,std-ref}{The cluster\_phot File}}}}.

If the SLUG data input to cloudy\_slug were written in \code{ascii} mode,
these data are output as a text file containing a series of columns,
with different trials separated by lines of dashes.

If the SLUG data input to cloudy\_slug were written in \code{fits} mode,
these data are written in a FITS file containing one binary table
extension. The columns in this FITS file are \code{Trial}, \code{UniqueID},
\code{Time}, \code{Filter1\_Transmitted}, \code{Filter1\_Emitted},
\code{Filter1\_Transmitted\_plus\_emitted}, \code{...}. The first three columns
give the trial number, cluster unique ID, and the time, and the
remainder give the photometric values for the transmitted, emitted,
and transmitted plus emitted spectra in each filter.

If the SLUG data input to cloudy\_slug were written in \code{binary} mode,
these data are written in a raw binary file that is formatted as
follows. The file starts with an ASCII text header consisting of the
following, each on a separate line:
\begin{itemize}
\item {} 
\code{NFilter} (stored as \code{ASCII text}): number of filters used

\item {} 
\code{FilterName} \code{FilterUnit} (\code{NFilter} entries stored as \code{ASCII
text}): the name and units for each filter are listed in ASCII, one
filter-unit pair per line

\end{itemize}

This is followed by a series of entries of that each begin with a
header
\begin{itemize}
\item {} 
\code{Time} (\code{double})

\item {} 
\code{NCluster} (\code{std::vector\textless{}double\textgreater{}::size\_type}, usually \code{unsigned long long}): number of non-disrupted clusters present at this time

\end{itemize}

This is followed by \code{NCluster} entries of the following form:
\begin{itemize}
\item {} 
\code{UniqueID} (\code{unsigned long})

\item {} 
\code{PhotFilter\_Transmitted} (\code{NFilter} entries of numpy
\code{float64}), giving the transmitted photometry in each filter

\item {} 
\code{PhotFilter\_Emitted} (\code{NFilter} entries of numpy
\code{float64}), giving the emitted photometry in each filter

\item {} 
\code{PhotFilter\_Transmitted\_plus\_emitted} (\code{NFilter} entries of numpy
\code{float64}), giving the transmitted plus emitted photometry in each
filter

\end{itemize}


\section{Full Documentation of slugpy.cloudy}
\label{cloudy:module-slugpy.cloudy}\label{cloudy:full-documentation-of-slugpy-cloudy}\index{slugpy.cloudy (module)}\index{read\_cloudy\_continuum() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_cloudy_continuum}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_cloudy\_continuum}}{\emph{filename}, \emph{r0=None}}{}
Reads a cloudy continuum output, produced by save last continuum
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{filename}] \leavevmode{[}string{]}
name of the file to be read

\item[{r0}] \leavevmode{[}float{]}
inner radius, in cm; if included, the quantities returned will
be total energies instead of energy emission rates instead of
rates per unit area

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{wl}] \leavevmode{[}array{]}
wavelengths in Angstrom

\item[{incident}] \leavevmode{[}array{]}
incident radiation field intensity

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_cloudy\_hcon() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_cloudy_hcon}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_cloudy\_hcon}}{\emph{hcon\_file}, \emph{r0=0.0}}{}
Reads cloudy outputs produce by the `save last hydrogen
conditions' and `save last hydrogen ionization' file, and uses
these to return various HII region diagnostic parameters.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{hcon\_file}] \leavevmode{[}str{]}
Name of hydrogen conditions file to be read

\item[{r0}] \leavevmode{[}float{]}
Inner radius for the calculation

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{r1}] \leavevmode{[}float{]}
outer radius, in cm

\item[{nII}] \leavevmode{[}float{]}
average number density of H nuclei

\item[{Omega}] \leavevmode{[}float{]}
wind parameter, defined as r0\textasciicircum{}3 / (r1\textasciicircum{}3 - r0\textasciicircum{}3)

\end{description}

\item[{Notes}] \leavevmode
All averages are averages over the ionized volume, i.e., the
average is taken with a weighting factor 4 pi r\textasciicircum{}2 (n\_H+/n\_H) dV.

\end{description}

\end{fulllineitems}

\index{read\_cloudy\_linelist() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_cloudy_linelist}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_cloudy\_linelist}}{\emph{filename}}{}
Reads a cloudy line list output, produced by save last line list
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{filename}] \leavevmode{[}string{]}
name of the file to be read

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{labels}] \leavevmode{[}array, dtype `S4'{]}
list of line labels

\item[{wl}] \leavevmode{[}array{]}
array of line wavelengths, in Angstrom

\item[{lum}] \leavevmode{[}array{]}
array of line luminosities; this will be in whatever units the
cloudy output is in

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_cluster\_cloudyparams() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_cluster_cloudyparams}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_cluster\_cloudyparams}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 cluster\_cloudyparams file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}`txt' \textbar{} `ascii' \textbar{} `bin' \textbar{} `binary' \textbar{} `fits' \textbar{} `fits2'{]}
Format for the file to be read. If one of these is set, the
function will only attempt to open ASCII-(`txt' or `ascii'), 
binary (`bin' or `binary'), or FITS (`fits' or `fits2')
formatted output, ending in .txt., .bin, or .fits,
respectively. If set to None, the code will try to open
ASCII files first, then if it fails try binary files, and if
it fails again try FITS files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
time at which cluster's properties are being evaluated

\item[{cloudy\_hden}] \leavevmode{[}array{]}
number density of H nuclei at the inner edge of the ionized
region simulated by cloudy

\item[{cloudy\_r0}] \leavevmode{[}array{]}
inner radius of the ionized region simulated by cloudy

\item[{cloudy\_r1}] \leavevmode{[}array{]}
outer radius of the ionized region simulated by cloudy (approximate!)

\item[{cloudy\_QH0}] \leavevmode{[}array{]}
ionizing luminosity used in the cloudy computation

\item[{cloudy\_covFac}] \leavevmode{[}array{]}
covering factor assumed in the cloudy computation; only a
fraction covFac of the ionizing photons are assumed to
produce emission within the HII region, while the remainder
are assumed to escape

\item[{cloudy\_U}] \leavevmode{[}array{]}
volume-averaged ionization parameter of the HII region
simulated by cloudy

\item[{cloudy\_U0}] \leavevmode{[}array{]}
ionization parameter of the HII reegion at its inner edge

\item[{cloudy\_Omega}] \leavevmode{[}array{]}
Yeh \& Matzner (2012) wind parameter for the HII region
simulated by cloudy

\item[{cloudy\_zeta}] \leavevmode{[}array{]}
Krumholz \& Matzner (2009) radiation pressure parameter for
the HII region, again approximate; values of zeta \textgreater{}\textasciitilde{}1
indicate that radiation pressure is dynamically important

\end{description}

The following fields may or may not be present, depending on
what is found in the output file:
\begin{description}
\item[{cloudy\_hden\_out}] \leavevmode{[}array{]}
volume-averaged number density produced by the cloudy
calculation

\item[{cloudy\_r1\_out}] \leavevmode{[}array{]}
HII region outer radius produced by cloudy

\item[{cloudy\_Omega\_out}] \leavevmode{[}array{]}
value of Omega computed using cloudy\_r1\_out

\item[{cloudy\_zeta\_out}] \leavevmode{[}array{]}
value of zeta computed using cloudy\_r1\_out

\end{description}

\end{description}

Notes
\begin{quote}

The relationships between U, U0, Omega, r0, r1, hden, and
QH0 used in deriving various parameters are valid only in
the limit of negligible radiation pressure. They may be
significantly off if radiation pressure is significant,
i.e., if zeta \textgreater{}\textasciitilde{} 1.
\end{quote}

\end{fulllineitems}

\index{read\_cluster\_cloudyphot() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_cluster_cloudyphot}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_cluster\_cloudyphot}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{nofilterdata=False}, \emph{photsystem=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 cluster\_cloudyphot file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{nofilterdata}] \leavevmode{[}bool{]}
If True, the routine does not attempt to read the filter
response data from the standard location

\item[{photsystem}] \leavevmode{[}None or string{]}
If photsystem is None, the data will be returned in the same
photometric system in which they were read. Alternately, if it
is a string, the data will be converted to the specified
photometric system. Allowable values are `L\_nu', `L\_lambda',
`AB', `STMAG', and `Vega', corresponding to the options defined
in the SLUG code. If this is set and the conversion requested
involves a conversion from a wavelength-based system to a
frequency-based one, nofilterdata must be False so that the
central wavelength of the photometric filters is available.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
times at which cluster spectra are output, in yr

\item[{cloudy\_filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{cloudy\_filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{cloudy\_filter\_wl\_eff}] \leavevmode{[}list{]}
effective wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{cloudy\_filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1; omitted if
nofilterdata is True

\item[{cloudy\_filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1; omitted
if nofilterdata is True

\item[{cloudy\_filter\_beta}] \leavevmode{[}list{]}
powerlaw index beta for each filter; used to normalize the
photometry

\item[{cloudy\_filter\_wl\_c}] \leavevmode{[}list{]}
pivot wavelength for each filter; used to normalize the photometry

\item[{cloudy\_phot\_trans}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value for each cluster in each filter for the
transmitted light (i.e., the starlight remaining after it has
passed through the HII region); units are as indicated in
the units field

\item[{cloudy\_phot\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value for each cluster in each filter for the
emitted light (i.e., the diffuse light emitted by the HII
region); units are as indicated in the units field

\item[{cloudy\_phot\_trans\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_filter){]}
photometric value in each filter for each cluster for the
transmitted plus emitted light (i.e., the light coming
directly from the stars after absorption by the HII region,
plus the diffuse light emitted by the HII region); units are as
indicated in the units field

\end{description}

\item[{Raises}] \leavevmode
IOError, if no photometry file can be opened;
ValueError, if photsystem is set to an unknown value

\end{description}

\end{fulllineitems}

\index{read\_cluster\_cloudylines() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_cluster_cloudylines}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_cluster\_cloudylines}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 cluster\_cloudylines file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
times at which cluster spectra are output, in yr

\item[{cloudy\_linelabel}] \leavevmode{[}array, dtype='S4', shape (N\_lines){]}
labels for the lines, following cloudy's 4 character line label
notation

\item[{cloudy\_linewl}] \leavevmode{[}array, shape (N\_lines){]}
rest wavelength for each line, in Angstrom

\item[{cloudy\_linelum}] \leavevmode{[}array, shape (N\_cluster, N\_lines){]}
luminosity of each line at each time for each trial, in erg/s

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_cluster\_cloudyspec() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_cluster_cloudyspec}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_cluster\_cloudyspec}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 cluster\_cloudyspec file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{id}] \leavevmode{[}array, dtype uint{]}
unique ID of cluster

\item[{trial: array, dtype uint}] \leavevmode
which trial was this cluster part of

\item[{time}] \leavevmode{[}array{]}
times at which cluster spectra are output, in yr

\item[{cloudy\_wl}] \leavevmode{[}array{]}
wavelength, in Angstrom

\item[{cloudy\_inc}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of the cluster's stellar radiation field at
each wavelength, in erg/s/A

\item[{cloudy\_trans}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of the stellar radiation field after it has
passed through the HII region, at each wavelength, in erg/s/A

\item[{cloudy\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
specific luminosity of the radiation field emitted by the HII
region, at each wavelength, in erg/s/A

\item[{cloudy\_trans\_emit}] \leavevmode{[}array, shape (N\_cluster, N\_wavelength){]}
the sum of the emitted and transmitted fields; this is what
would be seen by an observer looking at both the star cluster
and its nebula

\end{description}

\item[{Raises}] \leavevmode
IOError, if no spectrum file can be opened

\end{description}

\end{fulllineitems}

\index{read\_integrated\_cloudylines() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_integrated_cloudylines}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_integrated\_cloudylines}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 integrated\_cloudylines file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{time}] \leavevmode{[}array, shape (N\_times) or shape (N\_trials){]}
Times at which data are output; shape is either N\_times (if
the run was done with fixed output times) or N\_trials (if
the run was done with random output times)

\item[{cloudy\_linelabel}] \leavevmode{[}array, dtype='S4', shape (N\_lines){]}
labels for the lines, following cloudy's 4 character line label
notation

\item[{cloudy\_linewl}] \leavevmode{[}array, shape (N\_lines){]}
rest wavelength for each line, in Angstrom

\item[{cloudy\_linelum}] \leavevmode{[}array, shape (N\_lines, N\_times, N\_trials){]}
luminosity of each line at each time for each trial, in erg/s

\end{description}

\end{description}

\end{fulllineitems}

\index{read\_integrated\_cloudyparams() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_integrated_cloudyparams}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_integrated\_cloudyparams}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 integrated\_cloudyparams file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{time}] \leavevmode{[}array, shape (N\_times) or shape (N\_trials){]}
Times at which data are output; shape is either N\_times (if
the run was done with fixed output times) or N\_trials (if
the run was done with random output times)

\item[{cloudy\_hden}] \leavevmode{[}array{]}
number density of H nuclei at the inner edge of the ionized
region simulated by cloudy

\item[{cloudy\_r0}] \leavevmode{[}array{]}
inner radius of the ionized region simulated by cloudy

\item[{cloudy\_r1}] \leavevmode{[}array{]}
outer radius of the ionized region simulated by cloudy (approximate!)

\item[{cloudy\_QH0}] \leavevmode{[}array{]}
ionizing luminosity used in the cloudy computation

\item[{cloudy\_covFac}] \leavevmode{[}array{]}
covering factor assumed in the cloudy computation; only a
fraction covFac of the ionizing photons are assumed to
produce emission within the HII region, while the remainder
are assumed to escape

\item[{cloudy\_U}] \leavevmode{[}array{]}
volume-averaged ionization parameter of the HII region
simulated by cloudy

\item[{cloudy\_U0}] \leavevmode{[}array{]}
ionization parameter of the HII reegion at its inner edge

\item[{cloudy\_Omega}] \leavevmode{[}array{]}
Yeh \& Matzner (2012) wind parameter for the HII region
simulated by cloudy

\item[{cloudy\_zeta}] \leavevmode{[}array{]}
Krumholz \& Matzner (2009) radiation pressure parameter for
the HII region, again approximate; values of zeta \textgreater{}\textasciitilde{}1
indicate that radiation pressure is dynamically important

\end{description}

The following fields may or may not be present, depending on
what is found in the output file:
\begin{description}
\item[{cloudy\_hden\_out}] \leavevmode{[}array{]}
volume-averaged number density produced by the cloudy
calculation

\item[{cloudy\_r1\_out}] \leavevmode{[}array{]}
HII region outer radius produced by cloudy

\item[{cloudy\_Omega\_out}] \leavevmode{[}array{]}
value of Omega computed using cloudy\_r1\_out

\item[{cloudy\_zeta\_out}] \leavevmode{[}array{]}
value of zeta computed using cloudy\_r1\_out

\end{description}

\end{description}

Notes
\begin{quote}

The relationships between U, U0, Omega, r0, r1, hden, and
QH0 used in deriving various parameters are valid only in
the limit of negligible radiation pressure. They may be
significantly off if radiation pressure is significant,
i.e., if zeta \textgreater{}\textasciitilde{} 1.
\end{quote}

\end{fulllineitems}

\index{read\_integrated\_cloudyphot() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_integrated_cloudyphot}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_integrated\_cloudyphot}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{nofilterdata=False}, \emph{photsystem=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 integrated\_cloudyphot file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{nofilterdata}] \leavevmode{[}bool{]}
If True, the routine does not attempt to read the filter
response data from the standard location

\item[{photsystem}] \leavevmode{[}None or string{]}
If photsystem is None, the data will be returned in the same
photometric system in which they were read. Alternately, if it
is a string, the data will be converted to the specified
photometric system. Allowable values are `L\_nu', `L\_lambda',
`AB', `STMAG', and `Vega', corresponding to the options defined
in the SLUG code. If this is set and the conversion requested
involves a conversion from a wavelength-based system to a
frequency-based one, nofilterdata must be False so that the
central wavelength of the photometric filters is available.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{time}] \leavevmode{[}array, shape (N\_times) or shape (N\_trials){]}
Times at which data are output; shape is either N\_times (if
the run was done with fixed output times) or N\_trials (if
the run was done with random output times)

\item[{cloudy\_filter\_names}] \leavevmode{[}list of string{]}
a list giving the name for each filter

\item[{cloudy\_filter\_units}] \leavevmode{[}list of string{]}
a list giving the units for each filter

\item[{cloudy\_filter\_wl\_eff}] \leavevmode{[}list{]}
effective wavelength of each filter; this is set to None for the
filters Lbol, QH0, QHe0, and QHe1; omitted if nofilterdata is
True

\item[{cloudy\_filter\_wl}] \leavevmode{[}list of arrays{]}
a list giving the wavelength table for each filter; this is
None for the filters Lbol, QH0, QHe0, and QHe1; omitted if
nofilterdata is True

\item[{cloudy\_filter\_response}] \leavevmode{[}list of arrays{]}
a list giving the photon response function for each filter;
this is None for the filters Lbol, QH0, QHe0, and QHe1; omitted
if nofilterdata is True

\item[{cloudy\_filter\_beta}] \leavevmode{[}list{]}
powerlaw index beta for each filter; used to normalize the
photometry

\item[{cloudy\_filter\_wl\_c}] \leavevmode{[}list{]}
pivot wavelength for each filter; used to normalize the photometry

\item[{cloudy\_phot\_trans}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial for
the transmitted light (i.e., the starlight remaining after it
has passed through the HII region); units are as indicated in
the units field

\item[{cloudy\_phot\_emit}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial for
the emitted light (i.e., the diffuse light emitted by the HII
region); units are as indicated in the units field

\item[{cloudy\_phot\_trans\_emit}] \leavevmode{[}array, shape (N\_filter, N\_times, N\_trials){]}
photometric value in each filter at each time in each trial for
the transmitted plus emitted light (i.e., the light coming
directly from the stars after absorption by the HII region,
plus the diffuse light emitted by the HII region); units are as
indicated in the units field

\end{description}

\item[{Raises}] \leavevmode
IOError, if no photometry file can be opened;
ValueError, if photsystem is set to an unknown value

\end{description}

\end{fulllineitems}

\index{read\_integrated\_cloudyspec() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.read_integrated_cloudyspec}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{read\_integrated\_cloudyspec}}{\emph{model\_name}, \emph{output\_dir=None}, \emph{fmt=None}, \emph{verbose=False}, \emph{read\_info=None}}{}
Function to read a SLUG2 integrated\_cloudyspec file.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{model\_name}] \leavevmode{[}string{]}
The name of the model to be read

\item[{output\_dir}] \leavevmode{[}string{]}
The directory where the SLUG2 output is located; if set to None,
the current directory is searched, followed by the SLUG\_DIR
directory if that environment variable is set

\item[{fmt}] \leavevmode{[}string{]}
Format for the file to be read. Allowed values are `ascii',
`bin' or `binary, and `fits'. If one of these is set, the code
will only attempt to open ASCII-, binary-, or FITS-formatted
output, ending in .txt., .bin, or .fits, respectively. If set
to None, the code will try to open ASCII files first, then if
it fails try binary files, and if it fails again try FITS
files.

\item[{verbose}] \leavevmode{[}bool{]}
If True, verbose output is printed as code runs

\item[{read\_info}] \leavevmode{[}dict{]}
On return, this dict will contain the keys `fname' and
`format', giving the name of the file read and the format it
was in; `format' will be one of `ascii', `binary', or `fits'

\end{description}

\item[{Returns}] \leavevmode
A namedtuple containing the following fields:
\begin{description}
\item[{time}] \leavevmode{[}array, shape (N\_times) or shape (N\_trials){]}
Times at which data are output; shape is either N\_times (if
the run was done with fixed output times) or N\_trials (if
the run was done with random output times)

\item[{cloudy\_wl}] \leavevmode{[}array{]}
wavelength, in Angstrom

\item[{cloudy\_inc}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity of the stellar radiation field at each
wavelength and each time for each trial, in erg/s/A

\item[{cloudy\_trans}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity of the stellar radiation field after it has
passed through the HII region, at each wavelength and each time
for each trial, in erg/s/A

\item[{cloudy\_emit}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
specific luminosity of the radiation field emitted by the HII
region, at each wavelength and each time for each trial, in
erg/s/A

\item[{cloudy\_trans\_emit}] \leavevmode{[}array, shape (N\_wavelength, N\_times, N\_trials){]}
the sum of emitted and transmitted; this is what would be seen
by an observer looking at both the star cluster and its nebula

\end{description}

\end{description}

\end{fulllineitems}

\index{write\_cluster\_cloudyparams() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.write_cluster_cloudyparams}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{write\_cluster\_cloudyparams}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Write out photometry for nebular emission computed by cloudy on a
slug spectrum for a series of clusters
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Cluster cloudy parameter data; a namedtuple containing the
fields id, trial, time, cloudy\_hden, cloudy\_r0, 
cloudy\_r1, cloudy\_QH0, cloudy\_covFac, cloudy\_U, cloudy\_U0,
cloudy\_Omega, and cloudy\_zeta; may also optionally contain
the fields cloudy\_r1\_out, cloudy\_hden\_out, cloudy\_Omega\_out,
and cloudy\_zeta\_out

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}

\index{write\_cluster\_cloudyphot() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.write_cluster_cloudyphot}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{write\_cluster\_cloudyphot}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Write out photometry for nebular emission computed by cloudy on a
slug spectrum for a series of clusters
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Cluster cloudy photometry data to be written; a namedtuple
containing the fields id, time, cloudy\_filter\_names, 
cloudy\_filter\_units, cloudy\_phot\_trans, cloudy\_phot\_emit,
and cloudy\_phot\_trans\_emit

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}

\index{write\_cluster\_cloudylines() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.write_cluster_cloudylines}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{write\_cluster\_cloudylines}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Write out data computed by cloudy on a slug spectrum
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Cloudy spectral data for clusters to be written; a namedtuple
containing the fields time, cloudy\_linelist, cloudy\_linewl, 
cloudy\_linelum

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}

\index{write\_cluster\_cloudyspec() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.write_cluster_cloudyspec}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{write\_cluster\_cloudyspec}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Write out data computed by cloudy on a slug spectrum
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Cloudy spectral data for clusters to be written; a namedtuple
containing the fields id, time, cloudy\_wl, cloudy\_inc, cloudy\_trans,
cloudy\_emit, and cloudy\_trans\_emit

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}

\index{write\_integrated\_cloudyparams() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.write_integrated_cloudyparams}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{write\_integrated\_cloudyparams}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Write out photometry for nebular emission computed by cloudy on a
slug spectrum for a series of clusters
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Cluster cloudy parameter data; a namedtuple containing the
fields time, cloudy\_hden, cloudy\_r0, cloudy\_r1, 
cloudy\_QH0, cloudy\_covFac, cloudy\_U, cloudy\_U0, cloudy\_Omega, and
cloudy\_zeta; may also optionally contain the fields
cloudy\_r1\_out, cloudy\_hden\_out, cloudy\_Omega\_out, and
cloudy\_zeta\_out

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}

\index{write\_integrated\_cloudylines() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.write_integrated_cloudylines}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{write\_integrated\_cloudylines}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Write out line luminosities computed by cloudy on a slug spectrum
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Integrated cloudy line data to be written; a namedtuple
containing the fields time, cloudy\_linelist, cloudy\_linewl, 
cloudy\_linelum

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}

\index{write\_integrated\_cloudyphot() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.write_integrated_cloudyphot}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{write\_integrated\_cloudyphot}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Write out photometry for nebular emission computed by cloudy on a
slug spectrum
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Integrated cloudy photometry data to be written; a namedtuple
containing the fields time, cloudy\_filter\_names, 
cloudy\_filter\_units, cloudy\_phot\_trans, cloudy\_phot\_emit,
and cloudy\_phot\_trans\_emit

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}

\index{write\_integrated\_cloudyspec() (in module slugpy.cloudy)}

\begin{fulllineitems}
\phantomsection\label{cloudy:slugpy.cloudy.write_integrated_cloudyspec}\pysiglinewithargsret{\code{slugpy.cloudy.}\bfcode{write\_integrated\_cloudyspec}}{\emph{data}, \emph{model\_name}, \emph{fmt}}{}
Write out data computed by cloudy on a slug spectrum
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{data}] \leavevmode{[}namedtuple{]}
Integrated cloudy spectral data to be written; a namedtuple
containing the field time, cloudy\_wl, cloudy\_inc, cloudy\_trans,
cloudy\_emit, and cloudy\_trans\_emit

\item[{model\_name}] \leavevmode{[}string{]}
Base file name to give the model to be written. Can include a
directory specification if desired.

\item[{fmt}] \leavevmode{[}string{]}
Format for the output file. Allowed values are `ascii', `bin'
or `binary, and `fits'.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}



\chapter{bayesphot: Bayesian Inference for Stochastic Stellar Populations}
\label{bayesphot:sec-bayesphot}\label{bayesphot::doc}\label{bayesphot:bayesphot-bayesian-inference-for-stochastic-stellar-populations}

\section{What Does bayesphot Do?}
\label{bayesphot:what-does-bayesphot-do}
Bayesphot is a package for performing Bayesian inference for the physical properties of a stellar system using its measured photometric properties, in a case where the photometric properties vary non-deterministically with the physical properties. Formally, bayesphot answers the following question: consider a stellar system characterized by a vector of \(\mathbf{x} = (x_1, x_2, \ldots x_N)\) physical properties. We have a physical model that lets us sample the expected photometric properties as a function of physical properties, i.e., that for some sample of \(K\) systems with physical properties \(\mathbf{x}_k\) we are able to compute the corresponding photometric properties \(\mathbf{y}_k = \mathbf{y} = (y_1, y_2, \ldots y_M)_k\). Now suppose that we observe such a system, and we observe it to have photometric properties \(\mathbf{y}_{\mathrm{obs}}\), with some set of photometric errors \(\mathbf{\sigma}_{\mathbf{y}} = (\sigma_{y_1}, \sigma_{y_2}, \ldots \sigma_{y_M})\), which are assumed to be Gaussian-distributed. What should we infer about the posterior probability distribution of the physical properties, i.e., given a set of prior probabilities \(p(\mathbf{x})\), plus our measurements, what is \(p(\mathbf{x} \mid \mathbf{y}_{\mathrm{obs}}, \mathbf{\sigma}_{\mathrm{y}})\)?

The kernel density estimation algorithm that bayesphot uses to answer this question is described and derived in the slug methods paper. Bayesphot is implemented in two parts: a shared object library that is implemented in c, and that is compiled at the same time that slug is built, and a python wrapper class called \code{bp} that is included in the slugpy.bayesphot module. The following sections describe how to use \code{bp} objects to generate posterior PDFs.


\section{Creating \texttt{bp} Objects}
\label{bayesphot:creating-bp-objects}
The \code{bp} class can be imported via:

\begin{Verbatim}[commandchars=\\\{\}]
from slugpy.bayesphot import *
\end{Verbatim}

or:

\begin{Verbatim}[commandchars=\\\{\}]
from slugpy.bayesphot import bp
\end{Verbatim}

Once imported, a \code{bp} object can be instantiated. The call signature for the \code{bp} constructor class is:

\begin{Verbatim}[commandchars=\\\{\}]
def \PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}(self, dataset, nphys, filters=None, bandwidth=\PYGZsq{}auto\PYGZsq{},
             ktype=\PYGZsq{}gaussian\PYGZsq{}, priors=None, sample\PYGZus{}density=None,
             reltol=1.0e\PYGZhy{}3, abstol=1.0e\PYGZhy{}10, leafsize=16):
\end{Verbatim}

A full description of all options is included in the {\hyperref[bayesphot:ssec\string-slugpy\string-bayesphot]{\crossref{\DUrole{std,std-ref}{Full Documentation of slugpy.bayesphot}}}}, but the essential features are summarized here.

The argument \code{dataset} is an array of shape (N, M) that contains the library of N models that represents the training set for the Bayesian analysis. Each model consists of M properties; the first \code{nphys} of these are physical properties that are the quantities to be inferred from the observations, while the remaining ones are photometric properties. An important point is that the \code{dataset} object is NOT copied, so altering it after the \code{bp} object is created will result in erroneous results.

The \code{priors} and \code{sample\_density} arguments are used to compute the weighting to apply to the input models. The \code{priors} argument specifies the prior probability to assign to each model; it can be either an array giving a prior probability directly, or a callable that can take the physical properties of models as an input and return the prior probability as an output. Similarly, the \code{sample\_density} argument specifies the probablity distribution from which the physical models were selected; as with \code{priors}, it can be an array or a callable.

The \code{bandwidth} argument specifies the bandwidth to use in the kernel density estimation; this need not be the same in each dimension. The \code{bandwidth} can be specified as a float, in which case it is the same for every dimension, or as an array of M elements giving the bandwidth for every dimension. Finally, it can be set to the string \code{auto}, in which case the \code{bp} will attempt to make a reasonable choice of bandwidth autonomously. However, this autonomous choice will probably perform less well than something that is hand-chosen by the user based on their knowledge of the library. As a rule of thumb, bandwidths should be chosen so that, for typical input photometric values, there are \textasciitilde{}10 simulations within the 1 kernel size.

Note that both \code{priors} and \code{bandwidth} are properties of the \code{bp} class, and can be altered after the \code{bp} object is created. This makes it possible to alter the priors and bandwidth without incurring the computational or memory cost of generating an entirely new \code{bp} object.


\section{Using \texttt{bp} Objects}
\label{bayesphot:using-bp-objects}
Once a \code{bp} object is instantiated, it can be used to compute likelihood functions, marginal probabilities, and MCMC sample ensembles, and to search the library for the best matches to an input set of photometry.

The likelihood function is implemented via the \code{bp.logL} method, which has the call signature:

\begin{Verbatim}[commandchars=\\\{\}]
def logL(self, physprop, photprop, photerr=None):
\end{Verbatim}

The argument \code{physprop} is a set of physical properties, the argument \code{photprop} is a set of photometric properties, and the argument \code{photerr} is an (optional) set of photometric errors. All of these must be arrays, the size of whose trailing dimension matches the number of physical properties (for \code{physprop}) or the number of photometric properties (for \code{photprop} and \code{photerr}); the leading dimensions of these arrays are broadcast together using normal broadcasting rules. The quantity returned is the log of the joint probability distribution of physical and photometric properties. Specifically, the quantity returned for each input set of physical and photometric properties is
\begin{equation*}
\begin{split}\log p(\mathbf{x}, \mathbf{y}, \sigma_{\mathbf{y}}) = \log A \sum_{i=1}^N w_i G(\mathbf{x}, \mathbf{y}; \mathbf{h}')\end{split}
\end{equation*}
where \(A\) is a normalization constant chosen to ensure that the PDF integrated over all space is unity, \(\mathbf{x}\) is the vector of physical properties, \(\mathbf{y}\) is the vector of photometric properties, \(\sigma_\mathbf{y}\) is the vector of photomtric errors, \(w_i\) is the weight of the ith model as determined by the priors and sample density,
\begin{equation*}
\begin{split}G\left(\mathbf{x}, \mathbf{y}; \mathbf{h}'\right) \propto \exp\left[-\left(\frac{x_1^2}{2h_{x_1}'^2} + \cdots + \frac{x_N^2}{2h_{x_N}'^2} + \frac{y_1^2}{2h_{y_1}'^2} + \cdots + \frac{y_M^2}{2h_{y_M}'^2} \right)\right]\end{split}
\end{equation*}
is the N-dimensional Gaussian function, and
\begin{equation*}
\begin{split}\mathbf{h'} = \sqrt{\mathbf{h}^2 + \sigma_{\mathbf{y}}^2}\end{split}
\end{equation*}
is the modified bandwidth, which is equal to the bandwidth used for kernel density estimation added in quadrature sum with the errors in the photometric quantities (see the slug method paper for details).

Estimation of marginal PDFs is done via the \code{bp.mpdf} method, which has the call signature:

\begin{Verbatim}[commandchars=\\\{\}]
def mpdf(self, idx, photprop, photerr=None, ngrid=128,
         qmin=None, qmax=None, grid=None, norm=True):
\end{Verbatim}

The argument \code{idx} is an int or a list of ints between 0 and nphys-1, which specifies for which physical quantity or physical quantities the marginal PDF is to be computed. These indices refer to the indices in the \code{dataset} array that was input when the \code{bp} object was instantiated. The arguments \code{photprop} and \code{photerr} give the photometric measurements and their errors for which the marginal PDFs are to be computed; they must be arrays whose trailing dimension is equal to the number of photometric quantities. The leading dimensions of these arrays are broadcast together following the normal broadcasting rules. By default each physical quantity will be estimated on a grid of 128 points, evenly spaced from the lowest value of that physical property in the model library to the highest value. The parameters \code{qmin}, \code{qmax}, \code{ngrid}, and \code{grid} can be used to override this behavior and set the grid of evaluation points manually. The function returns a tuple \code{grid\_out, pdf}; here \code{grid\_out} is the grid of points on which the marginal PDF has been computed, and \code{pdf} is the value of the marginal PDF evaluated at those gridpoints.

MCMC calculations are implemented via the method \code{bp.mcmc}; this method relies on the \href{http://dan.iel.fm/emcee/current/}{emcee} python module, and will only function if it is installed. The call signature is:

\begin{Verbatim}[commandchars=\\\{\}]
def mcmc(self, photprop, photerr=None, mc\PYGZus{}walkers=100,
         mc\PYGZus{}steps=500, mc\PYGZus{}burn\PYGZus{}in=50):
\end{Verbatim}

The quantities \code{photprop} and \code{photerr} have the same meaning as for \code{bp.mpdf}, and the quantities \code{mc\_walkers}, \code{mc\_steps}, and \code{mc\_burn\_in} are passed directly to \code{emcee}, and are described in \href{http://dan.iel.fm/emcee/current/}{emcee's documentation}. The quantity returned is an array of sample points computed by the MCMC; its format is also described in \href{http://dan.iel.fm/emcee/current/}{emcee's documentation}. Note that, although \code{bp.mcmc} can be used to compute marginal PDFs of the physical quantities, for marginal PDFs of 1 quantity or joint PDFs of 2 quantities it is almost always faster to use \code{bp.mpdf} than \code{bp.mcmc}. This is because \code{bp.mpdf} takes advantage of the fact that integrals of cuts through N-dimensional Gaussians can be integrated analytically to compute the marginal PDFs directly, though needing to evaluate the likelihood function point by point. In contrast, the general MCMC algorithm used by \code{emcee} effectively does the integral numerically.

The \code{bp.bestmatch} method searches through the model library and finds the N library entries that are closest to an input set of photometry. The call signature is:

\begin{Verbatim}[commandchars=\\\{\}]
def bestmatch(self, phot, nmatch=1, bandwidth\PYGZus{}units=False):
\end{Verbatim}

Here \code{phot} is the set of photometric properties, which is identical to the \code{photprop} parameter used by \code{logL}, \code{mpdf}, and \code{mcmc}. The argument \code{nmatch} specifies how many matches to return, and the argument \code{bandwidth\_units} specifies whether distances are to be measured using an ordinary Euclidean metric, or in units of the kernel bandwidth in a given direction. The function returns, for each input set of photometry, the physical and photometric properties of the \code{nmatch} models in the library that are closest to the input photometric values. This can be used to judge if a good match to the input photometry is present in the library.


\section{Support for Parallelism in bayesphot}
\label{bayesphot:ssec-bayesphot-threading}\label{bayesphot:support-for-parallelism-in-bayesphot}
The \code{bp} class supports parallel calculations of posterior PDFs and
related quantities, through the python \href{https://docs.python.org/2.7/library/multiprocessing.html}{multiprocessing module}. This
allows efficient use of multiple cores on a shared memory machine,
circumventing the python global interpreter lock, without the need for
every process to read a large simulation library or store it in
memory. The recommended method for writing threaded code using \code{bp}
objects is to use have a master process create the \code{bp} object, and
then use a \href{https://docs.python.org/2.7/library/multiprocessing.html\#multiprocessing.Process}{Process}
or \href{https://docs.python.org/2.7/library/multiprocessing.html\#module-multiprocessing.pool}{Pool}
object to create child processes the perform computations using \code{bp}
methods such as \code{bp.logL} or \code{bp.mpdf}. It is often most efficient
to combine this with shared memory objects such as \href{https://docs.python.org/2.7/library/multiprocessing.html\#module-multiprocessing.sharedctypes}{RawArray}
to hold the outputs.

An example use case for computing 1D marginal PDFs on a large set of photometric values is:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{} Import what we need
from slugpy.bayesphot import bp
from multiprocessing import Pool, RawArray
from ctypes import c\PYGZus{}double
import numpy as np

\PYGZsh{} Some code here to create / read the data set to be used by
\PYGZsh{} bayesphot and store it in a variable called dataset

\PYGZsh{} Create the bayesphot object
my\PYGZus{}bp = bp(dataset, nphys)

\PYGZsh{} Some code here to create / read the photometric data we want to
\PYGZsh{} process using bayesphot and store it in an array called phot,
\PYGZsh{} which is of shape (nphot, nfilter). There is also an array of
\PYGZsh{} photometric errors, called photerr, of the same shape.

\PYGZsh{} Create a holder for the output
pdf\PYGZus{}base = RawArray(c\PYGZus{}double, 128*nphot)
pdf = np.frombuffer(pdf\PYGZus{}base, dtype=c\PYGZus{}double). \PYGZbs{}
      reshape((nphot,128))
grd\PYGZus{}base = RawArray(c\PYGZus{}double, 128)
grd = np.frombuffer(grd\PYGZus{}base, dtype=c\PYGZus{}double)

\PYGZsh{} Define the function that will be responsible for computing the
\PYGZsh{} marginal PDF
def mpdf\PYGZus{}apply(i):
    grd[:], pdf[i] = my\PYGZus{}bp.mpdf(0, phot[i], photerr[i])

\PYGZsh{} Main thread starts up a process pool and starts the computation
if \PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{} == \PYGZsq{}\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}\PYGZsq{}:
    pool = Pool()
    pool.map(mpdf\PYGZus{}apply, range(nphot))

    \PYGZsh{} At this point the grd and PDF contain the same as they would
    \PYGZsh{} if we had done
    \PYGZsh{}     grd, pdf = my\PYGZus{}bp.mpdf(0, phot, photerr)
    \PYGZsh{} but the results will be computed much faster this way
\end{Verbatim}

For an example of a more complex use case, see the \href{https://bitbucket.org/krumholz/legus-cluster-pipeline/overview}{LEGUS cluster pipeline}.

The full list of \code{bp} methods that are thread-safe is:
\begin{itemize}
\item {} 
\code{bp.logL}

\item {} 
\code{bp.mpdf}

\item {} 
\code{bp.mcmc}

\item {} 
\code{bp.bestmatch}

\item {} 
\code{bp.make\_approx\_phot}

\item {} 
\code{bp.make\_approx\_phys}

\item {} 
\code{bp.squeeze\_rep}

\item {} 
\code{bp.mpdf\_approx}

\end{itemize}

Thread safety involves a very modest overhead in terms of memory and speed, but for non-threaded computations this can be avoided by specifying:

\begin{Verbatim}[commandchars=\\\{\}]
bp.thread\PYGZus{}safe = False
\end{Verbatim}

or by setting the \code{thread\_safe} keyword to \code{False} when the \code{bp}
object is constructed.

Finally, note that this parallel paradigm avoids duplicating the large
library only on unix-like operating systems that support copy-on-write
semantics for \href{https://en.wikipedia.org/wiki/Fork\_(system\_call)}{fork}. Code such as the
above example should still work on windows (though this has not been
tested), but each worker process will duplicate the library in
physical memory, thereby removing one of the main advantages of
working in parallel.


\section{Full Documentation of slugpy.bayesphot}
\label{bayesphot:module-slugpy.bayesphot.bp}\label{bayesphot:ssec-slugpy-bayesphot}\label{bayesphot:full-documentation-of-slugpy-bayesphot}\index{slugpy.bayesphot.bp (module)}
This defines a class that can be used to estimate the PDF of physical
quantities from a set of input photometry in various bands, together
with a training data set.
\index{bp (class in slugpy.bayesphot.bp)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp}\pysiglinewithargsret{\strong{class }\code{slugpy.bayesphot.bp.}\bfcode{bp}}{\emph{dataset}, \emph{nphys}, \emph{filters=None}, \emph{bandwidth='auto'}, \emph{ktype='gaussian'}, \emph{priors=None}, \emph{pobs=None}, \emph{sample\_density=None}, \emph{reltol=0.01}, \emph{abstol=1e-10}, \emph{leafsize=16}, \emph{nosort=None}, \emph{thread\_safe=True}}{}
A class that can be used to estimate the PDF of the physical
properties of stellar population from a training set plus a set of
measured photometric values.
\begin{description}
\item[{Properties}] \leavevmode\begin{description}
\item[{priors}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} None{]}
prior probability on each data point; interpretation
depends on the type passed; array, shape (N): values are
interpreted as the prior probability of each data point;
callable: the callable must take as an argument an array
of shape (N, nphys), and return an array of shape (N)
giving the prior probability at each data point; None:
no reweighting is performed, so all data points in the library
have equal prior probability, i.e. the prior is the same as
the sampling of points in the library

\item[{pobs}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} None{]}
the probability that a particular object would be observed,
which is used, like prior, to weight the library;
interpretation depends on type. None means all objects are
equally likely to be observed, array is an array giving the
observation probability of each object in the library, and
callable means must be a function that takes an array
containing the photometry, of shape (N, nhpot), as an
argument, and returns an array of shape (N) giving the
probability of observation for that object

\item[{bandwidth}] \leavevmode{[}`auto' \textbar{} float \textbar{} array, shape (M){]}
bandwidth for kernel density estimation; if set to
`auto', the bandwidth will be estimated automatically; if
set to a scalar quantity, the same bandwidth is used for all
dimensions

\item[{nphys}] \leavevmode{[}int{]}
number of physical properties in the library

\item[{nphot}] \leavevmode{[}int{]}
number of photometric properties in the library

\item[{ndim}] \leavevmode{[}int{]}
nphys + nphot

\end{description}

\end{description}
\index{\_\_init\_\_() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{dataset}, \emph{nphys}, \emph{filters=None}, \emph{bandwidth='auto'}, \emph{ktype='gaussian'}, \emph{priors=None}, \emph{pobs=None}, \emph{sample\_density=None}, \emph{reltol=0.01}, \emph{abstol=1e-10}, \emph{leafsize=16}, \emph{nosort=None}, \emph{thread\_safe=True}}{}
Initialize a bp object.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{dataset}] \leavevmode{[}array, shape (N, M){]}
training data set; this is a set of N sample stellar
populations, having M properties each; the first nphys
represent physical properties (e.g., log mass, log age),
while the next M - nphys represent photometric
properties

\item[{nphys}] \leavevmode{[}int{]}
number of physical properties in dataset

\item[{filters}] \leavevmode{[}listlike of strings{]}
names of photometric filters; not used, but can be
stored for convenience

\item[{bandwidth}] \leavevmode{[}`auto' \textbar{} float \textbar{} array, shape (M){]}
bandwidth for kernel density estimation; if set to
`auto', the bandwidth will be estimated automatically; if
set to a scalar quantity, the same bandwidth is used for all
dimensions

\item[{ktype}] \leavevmode{[}string{]}
type of kernel to be used in densty estimation; allowed
values are `gaussian' (default), `epanechnikov', and
`tophat'; only Gaussian can be used with error bars

\item[{priors}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} None{]}
prior probability on each data point; interpretation
depends on the type passed; array, shape (N): values are
interpreted as the prior probability of each data point;
callable: the callable must take as an argument an array
of shape (N, nphys), and return an array of shape (N)
giving the prior probability at each data point; None:
all data points have equal prior probability

\item[{pobs}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} None{]}
the probability that a particular object would be observed,
which is used, like prior, to weight the library;
interpretation depends on type. None means all objects are
equally likely to be observed, array is an array giving the
observation probability of each object in the library, and
callable means must be a function that takes an array
containing the photometry, of shape (N, nhpot), as an
argument, and returns an array of shape (N) giving the
probability of observation for that object

\item[{sample\_density}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} `auto' \textbar{} None{]}
the density of the data samples at each data point; this
need not match the prior density; interpretation depends
on the type passed; array, shape (N): values are
interpreted as the density of data sampling at each
sample point; callable: the callable must take as an
argument an array of shape (N, nphys), and return an
array of shape (N) giving the sampling density at each
point; `auto': the sample density will be computed
directly from the data set; note that this can be quite
slow for large data sets, so it is preferable to specify
this analytically if it is known; None: data are assumed
to be uniformly sampled

\item[{reltol}] \leavevmode{[}float{]}
relative error tolerance; errors on all returned
probabilities p will satisfy either
abs(p\_est - p\_true) \textless{}= reltol * p\_est   OR
abs(p\_est - p\_true) \textless{}= abstol,
where p\_est is the returned estimate and p\_true is the
true value

\item[{abstol}] \leavevmode{[}float{]}
absolute error tolerance; see above

\item[{leafsize}] \leavevmode{[}int{]}
number of data points in each leaf of the KD tree

\item[{nosort}] \leavevmode{[}arraylike of bool, shape (N) \textbar{} None{]}
if specified, this keyword causes the KD tree not to be
sorted along the dimensions for which nosort is True

\item[{thread\_safe}] \leavevmode{[}bool{]}
if True, bayesphot will make extra copies of internals
as needed to ensure thread safety when the computation
routines (logL, mpdf, mcmc, bestmatch, make\_approx\_phot,
make\_approx\_phys, mpdf\_approx) are used with
multiprocessing; this incurs a minor performance
penalty, and can be disabled by setting to False if the
code will not be run with the multiprocessing module

\end{description}

\item[{Returns}] \leavevmode
Nothing

\item[{Raises}] \leavevmode
IOError, if the bayesphot c library cannot be found

\item[{Notes}] \leavevmode
Because the data sets passed in may be large, this class
does not make copies of any of its arguments, and instead
modifies them in place. However, this means it is the
responsibility of the user not to alter the any of the
arguments once they are passed to this class; for example,
dataset must not be modified after it is passed. Altering the
arguments, except through this class's methods, may cause
incorrect results to be generated.

\end{description}

\end{fulllineitems}

\index{\_\_weakref\_\_ (slugpy.bayesphot.bp.bp attribute)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.__weakref__}\pysigline{\bfcode{\_\_weakref\_\_}}
list of weak references to the object (if defined)

\end{fulllineitems}

\index{bandwidth (slugpy.bayesphot.bp.bp attribute)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.bandwidth}\pysigline{\bfcode{bandwidth}}
The current bandwidth

\end{fulllineitems}

\index{bestmatch() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.bestmatch}\pysiglinewithargsret{\bfcode{bestmatch}}{\emph{phot}, \emph{photerr=None}, \emph{nmatch=1}, \emph{bandwidth\_units=False}}{}
Searches through the simulation library and returns the closest
matches to an input set of photometry.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{phot}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors, which must have the
same shape as phot; if this is not None,
then distances will be measured in units of the
photometric error if bandwidth\_units is False, or in
units of the bandwidth added in quadrature with the
errors if it is True

\item[{nmatch}] \leavevmode{[}int{]}
number of matches to return; returned matches will be
ordered by distance from the input

\item[{bandwidth\_units}] \leavevmode{[}bool{]}
if False, distances are computed based on the
logarithmic difference in luminosity; if True, they are
measured in units of the bandwidth

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{matches}] \leavevmode{[}array, shape (..., nmatch, nphys + nfilter){]}
best matches to the input photometry; shape in the
leading dimensions will be the same as for phot, and if
nmatch == 1 then that dimension will be omitted

\item[{dist}] \leavevmode{[}array, shape (..., nmatch){]}
distances between the matches and the input photometry

\end{description}

\end{description}

\end{fulllineitems}

\index{bestmatch\_phys() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.bestmatch_phys}\pysiglinewithargsret{\bfcode{bestmatch\_phys}}{\emph{phys}, \emph{nmatch=1}, \emph{bandwidth\_units=False}}{}
Searches through the simulation library and returns the closest
matches to an input set of photometry.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{phot}] \leavevmode{[}arraylike, shape (nphys) or (..., nphys){]}
array giving the physical values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{nmatch}] \leavevmode{[}int{]}
number of matches to return; returned matches will be
ordered by distance from the input

\item[{bandwidth\_units}] \leavevmode{[}bool{]}
if False, distances are computed based on the
logarithmic difference in physical properties; if True,
they are measured in units of the bandwidth

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{matches}] \leavevmode{[}array, shape (..., nmatch, nphys + nfilter){]}
best matches to the input properties; shape in the
leading dimensions will be the same as for phot, and if
nmatch == 1 then that dimension will be omitted

\item[{dist}] \leavevmode{[}array, shape (..., nmatch){]}
distances between the matches and the input physical
properties

\end{description}

\end{description}

\end{fulllineitems}

\index{logL() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.logL}\pysiglinewithargsret{\bfcode{logL}}{\emph{physprop}, \emph{photprop}, \emph{photerr=None}}{}
This function returns the natural log of the likelihood
function evaluated at a particular log mass, log age,
extinction, and set of log luminosities
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{physprop}] \leavevmode{[}arraylike, shape (nphys) or (..., nphys){]}
array giving values of the physical properties; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photprop}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{logL}] \leavevmode{[}float or arraylike{]}
natural log of the likelihood function

\end{description}

\end{description}

\end{fulllineitems}

\index{make\_approx\_phot() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.make_approx_phot}\pysiglinewithargsret{\bfcode{make\_approx\_phot}}{\emph{phys}, \emph{squeeze=True}, \emph{filter\_ignore=None}}{}
Returns an object that can be used for a fast approximation of
the PDF of photometric properties that corresponds to a set of
physical properties. The PDF produced by summing over the
points returned is guaranteed to account for at least 1-reltol
of the marginal photometric probability, and to represent the
shape of the PDF in photometric space within a local accuracy
of reltol as well.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{phys}] \leavevmode{[}arraylike, shape (nphys) or (N, nphys){]}
the set or sets of physical properties for which the
approximation is to be generated

\item[{squeeze}] \leavevmode{[}bool{]}
if True, the representation returned will be squeezed to
minimize the number of points included, using reltol as
the error tolerance

\item[{filter\_ignore}] \leavevmode{[}None or listlike of bool{]}
if None, the kernel density representation returned
covers all filters; otherwise this must be a listlike of
bool, one entry per filter, with a value of False
indicating that filter should be excluded from the
values returned; suppressing filters can allow for more
efficient representations

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{x}] \leavevmode{[}array, shape (M, nphot), or a list of such arrays{]}
an array containing the list of points to be used for
the approximation, where nphot is the number of
photometric filters being returned

\item[{wgts}] \leavevmode{[}array, shape (M), or a list of such arrays{]}
an array containing the weights of the points

\end{description}

\item[{Notes:}] \leavevmode
if the requested relative tolerance cannot be reached for
numerical reasons (usually because the input point is too
far from the library to allow accurate computation), x and
wgts will be return as None, and a warning will be issued

\end{description}

\end{fulllineitems}

\index{make\_approx\_phys() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.make_approx_phys}\pysiglinewithargsret{\bfcode{make\_approx\_phys}}{\emph{phot}, \emph{photerr=None}, \emph{squeeze=True}, \emph{phys\_ignore=None}}{}
Returns an object that can be used for a fast approximation of
the PDF of physical properties that corresponds to a set of
photometric properties. The PDF produced by summing over the
points returned is guaranteed to account for at least 1-reltol
of the marginal photometric probability, and to represent the
shape of the PDF in photometric space within a local accuracy
of reltol as well.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{phot}] \leavevmode{[}arraylike, shape (nfilter) or (N, nfilter){]}
the set or sets of photometric properties for which the
approximation is to be generated

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (N, nfilter){]}
array giving photometric errors; the number of elements
in the output lists will be the size that results from
broadcasting together the leading dimensions of phot and
photerr

\item[{squeeze}] \leavevmode{[}bool{]}
if True, the representation returned will be squeezed to
minimize the number of points included, using reltol as
the error tolerance

\item[{phys\_ignore}] \leavevmode{[}None or listlike of bool{]}
if None, the kernel density representation returned
covers all physical properties; otherwise this must be a
listlike of bool, one entry per physical dimension, with
a value of False indicating that dimension should be
excluded from the values returned; suppressing
dimensions can allow for more efficient representations

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{x}] \leavevmode{[}array, shape (M, nphys), or a list of such arrays{]}
an array containing the list of points to be used for
the approximation, where nphys is the number of
physical dimensions being returned

\item[{wgts}] \leavevmode{[}array, shape (M), or a list of such arrays{]}
an array containing the weights of the points

\end{description}

\item[{Notes:}] \leavevmode
if the requested relative tolerance cannot be reached for
numerical reasons (usually because the input point is too
far from the library to allow accurate computation), x and
wgts will be return as None, and a warning will be issued

\end{description}

\end{fulllineitems}

\index{mcmc() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.mcmc}\pysiglinewithargsret{\bfcode{mcmc}}{\emph{photprop}, \emph{photerr=None}, \emph{mc\_walkers=100}, \emph{mc\_steps=500}, \emph{mc\_burn\_in=50}}{}
This function returns a sample of MCMC walkers sampling the
physical parameters at a specified set of photometric values.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{photprop}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\item[{mc\_walkers}] \leavevmode{[}int{]}
number of walkers to use in the MCMC

\item[{mc\_steps}] \leavevmode{[}int{]}
number of steps in the MCMC

\item[{mc\_burn\_in}] \leavevmode{[}int{]}
number of steps to consider ``burn-in'' and discard

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{samples}] \leavevmode{[}array{]}
array of sample points returned by the MCMC

\end{description}

\end{description}

\end{fulllineitems}

\index{mpdf() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.mpdf}\pysiglinewithargsret{\bfcode{mpdf}}{\emph{idx}, \emph{photprop}, \emph{photerr=None}, \emph{ngrid=128}, \emph{qmin=None}, \emph{qmax=None}, \emph{grid=None}, \emph{norm=True}}{}
Returns the marginal probability for one or mode physical
quantities for one or more input sets of photometric
properties. Output quantities are computed on a grid of
values, in the same style as meshgrid.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{idx}] \leavevmode{[}int or listlike containing ints{]}
index of the physical quantity whose PDF is to be
computed; if this is an iterable, the joint distribution of
the indicated quantities is returned

\item[{photprop}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\item[{ngrid}] \leavevmode{[}int or listlike containing ints{]}
number of points in each dimension of the output grid;
if this is an iterable, it must have the same number of
elements as idx

\item[{qmin}] \leavevmode{[}float or listlike{]}
minimum value in the output grid in each quantity; if
left as None, defaults to the minimum value in the
library; if this is an iterable, it must contain the
same number of elements as idx

\item[{qmax}] \leavevmode{[}float or listlike{]}
maximum value in the output grid in each quantity; if
left as None, defaults to the maximum value in the
library; if this is an iterable, it must contain the
same number of elements as idx

\item[{grid}] \leavevmode{[}listlike of arrays{]}
set of values defining the grid on which the PDF is to
be evaluated, in the same format used by meshgrid

\item[{norm}] \leavevmode{[}bool{]}
if True, returned pdf's will be normalized to integrate
to 1

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{grid\_out}] \leavevmode{[}array{]}
array of values at which the PDF is evaluated; contents
are the same as returned by meshgrid

\item[{pdf}] \leavevmode{[}array{]}
array of marginal posterior probabilities at each point
of the output grid, for each input set of photometry; the leading
dimensions match the leading dimensions produced by
broadcasting the leading dimensions of photprop and
photerr together, while the trailing dimensions match
the dimensions of the output grid

\end{description}

\end{description}

\end{fulllineitems}

\index{mpdf\_approx() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.mpdf_approx}\pysiglinewithargsret{\bfcode{mpdf\_approx}}{\emph{x}, \emph{wgts}, \emph{dims='phys'}, \emph{dims\_return=None}, \emph{ngrid=64}, \emph{qmin='all'}, \emph{qmax='all'}, \emph{grid=None}, \emph{norm=True}}{}
Returns the marginal posterior PDF computed from a kernel
density approximation returned by make\_approx\_phys or
make\_approx\_phot. Outputs are computed on a grid of values, in
the same style as meshgrid.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{x}] \leavevmode{[}array, shape (M, ndim), or a list of such arrays{]}
array of points retured by make\_approx\_phot or
make\_approx\_phys

\item[{wgts}] \leavevmode{[}array, shape (M) or a list of such arrays{]}
array of weights returned by make\_approx\_phot or
make\_approx\_phys

\item[{dims}] \leavevmode{[}`phys' \textbar{} `phot' \textbar{} arraylike of ints{]}
dimensions covered by x and wgts; the strings `phys' or
`phot' indicate that they cover all physical or
photometric dimensions, and correspond to the defaults
returned by make\_approx\_phys and make\_approx\_phot,
respectively; if dims is an array of ints, these specify
the dimensions covered by x and wgts, where the
physical dimensions are numbered 0, 1, ... nphys-1, and
the photometric ones are nphys, nphys+1,
... nphys+nphot-1

\item[{dims\_return}] \leavevmode{[}None or arraylike of ints{]}
if None, the output PDF has the same dimensions as
specified in dms; if not, then dimreturn must be a
subset of dim, and a marginal PDF in certain dimensions
will be generated

\item[{ngrid}] \leavevmode{[}int or listlike containing ints{]}
number of points in each dimension of the output grid;
if this is an iterable, it must have the same number of
elements as idx

\item[{qmin}] \leavevmode{[}float \textbar{} listlike \textbar{} `zoom' \textbar{} `all' {]}
minimum value in the output grid in each quantity; if
this a float, it is applied to each dimension; if it is
an iterable, it must contain the same number of elements
as the number of dimensions being returned, as gives the
minimum in each dimension; if it is `zoom' or `all', the
minimum is chosen automatically, with `zoom' focusing on
a region encompassing the probability maximum, and `all'
encompassing all the points in the representation

\item[{qmax}] \leavevmode{[}float \textbar{} listlike \textbar{} `zoom' \textbar{} `all'{]}
same as qmin, but for the maximum of the output grid

\item[{grid}] \leavevmode{[}listlike of arrays{]}
set of values defining the grid on which the PDF is to
be evaluated, in the same format used by meshgrid

\item[{norm}] \leavevmode{[}bool{]}
if True, returned pdf's will be normalized to integrate
to 1

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{grid\_out}] \leavevmode{[}array{]}
array of values at which the PDF is evaluated; contents
are the same as returned by meshgrid

\item[{pdf}] \leavevmode{[}array{]}
array of marginal posterior probabilities at each point
of the output grid, for each input cluster; the leading
dimensions match the leading dimensions produced by
broadcasting the leading dimensions of photprop and
photerr together, while the trailing dimensions match
the dimensions of the output grid

\end{description}

\end{description}

\end{fulllineitems}

\index{mpdf\_gen() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.mpdf_gen}\pysiglinewithargsret{\bfcode{mpdf\_gen}}{\emph{fixeddim}, \emph{fixedprop}, \emph{margindim}, \emph{ngrid=128}, \emph{qmin=None}, \emph{qmax=None}, \emph{grid=None}, \emph{norm=True}}{}
Returns the marginal probability for one or more physical or
photometric properties, keeping other properties fixed and
marginalizing over other quantities. This is the most general
marginal PDF routine provided.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{fixeddim}] \leavevmode{[}int \textbar{} arraylike of ints \textbar{} None{]}
The index or indices of the physical or photometric
properties to be held fixed; physical properties are
numbered 0 ... nphys-1, and phtometric ones are numbered
nphys ... nphys + nphot - 1. This can also be set to
None, in which case no properties are held fixed.

\item[{fixedprop}] \leavevmode{[}array \textbar{} None{]}
The values of the properties being held fixed; the size
of the final dimension must be equal to the number of
elements in fixeddim, and if fixeddim is None, this must
be too

\item[{margindim}] \leavevmode{[}int \textbar{} arraylike of ints \textbar{} None{]}
The index or indices of the physical or photometric
properties to be maginalized over, numbered in the same
way as with fixeddim; if set to None, no marginalization
is performed

\item[{ngrid}] \leavevmode{[}int or listlike containing ints{]}
number of points in each dimension of the output grid;
if this is an iterable, it must have nphys + nphot -
len(fixeddim) - len(margindim) elements

\item[{qmin}] \leavevmode{[}float \textbar{} arraylike{]}
minimum value in the output grid in each quantity; if
left as None, defaults to the minimum value in the
library; if this is an iterable, it must contain a
number of elements equal to nphys + nphot -
len(fixeddim) - len(margindim)

\item[{qmax}] \leavevmode{[}float \textbar{} arraylike{]}
maximum value in the output grid in each quantity; if
left as None, defaults to the maximum value in the
library; if this is an iterable, it must have the same
number of elements as qmin

\item[{grid}] \leavevmode{[}listlike of arrays{]}
set of values defining the grid on which the PDF is to
be evaluated, in the same format used by meshgrid

\item[{norm}] \leavevmode{[}bool{]}
if True, returned pdf's will be normalized to integrate
to 1

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{grid\_out}] \leavevmode{[}array{]}
array of values at which the PDF is evaluated; contents
are the same as returned by meshgrid

\item[{pdf}] \leavevmode{[}array{]}
array of marginal posterior probabilities at each point
of the output grid, for each input set of properties; the leading
dimensions match the leading dimensions produced by
broadcasting the leading dimensions of photprop and
photerr together, while the trailing dimensions match
the dimensions of the output grid

\end{description}

\end{description}

\end{fulllineitems}

\index{mpdf\_phot() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.mpdf_phot}\pysiglinewithargsret{\bfcode{mpdf\_phot}}{\emph{idx}, \emph{physprop}, \emph{ngrid=128}, \emph{qmin=None}, \emph{qmax=None}, \emph{grid=None}, \emph{norm=True}}{}
Returns the marginal probability for one or mode photometric
quantities corresponding to an input set of physical
properties. Output quantities are computed on a grid of
values, in the same style as meshgrid.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{idx}] \leavevmode{[}int or listlike containing ints{]}
index of the photometric quantity whose PDF is to be
computed, starting at 0; if this is an iterable, the
joint distribution of the indicated quantities is returned

\item[{physprop}] \leavevmode{[}arraylike, shape (nphys) or (..., nphys){]}
physical properties to be used; if this is an array of
nphys elements, these give the physical properties; if
it is a multidimensional array, the operation is
vectoried over the leading dimensions
physical properties -- the function must take an array
of (nphys) elements as an input, and return a floating
point value representing the PDF evaluated at that set
of physical properties as an output

\item[{ngrid}] \leavevmode{[}int or listlike containing ints{]}
number of points in each dimension of the output grid;
if this is an iterable, it must have the same number of
elements as idx

\item[{qmin}] \leavevmode{[}float or listlike{]}
minimum value in the output grid in each quantity; if
left as None, defaults to the minimum value in the
library; if this is an iterable, it must contain the
same number of elements as idx

\item[{qmax}] \leavevmode{[}float or listlike{]}
maximum value in the output grid in each quantity; if
left as None, defaults to the maximum value in the
library; if this is an iterable, it must contain the
same number of elements as idx

\item[{grid}] \leavevmode{[}listlike of arrays{]}
set of values defining the grid on which the PDF is to
be evaluated, in the same format used by meshgrid

\item[{norm}] \leavevmode{[}bool{]}
if True, returned pdf's will be normalized to integrate
to 1

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{grid\_out}] \leavevmode{[}array{]}
array of values at which the PDF is evaluated; contents
are the same as returned by meshgrid

\item[{pdf}] \leavevmode{[}array{]}
array of marginal posterior probabilities at each point
of the output grid, for each input set of properties; the leading
dimensions match the leading dimensions produced by
broadcasting the leading dimensions of photprop and
photerr together, while the trailing dimensions match
the dimensions of the output grid

\end{description}

\end{description}

\end{fulllineitems}

\index{ndim (slugpy.bayesphot.bp.bp attribute)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.ndim}\pysigline{\bfcode{ndim}}
This is the number of physical properties for the bayesphot
object.

\end{fulllineitems}

\index{nphot (slugpy.bayesphot.bp.bp attribute)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.nphot}\pysigline{\bfcode{nphot}}
This is the number of physical properties for the bayesphot
object.

\end{fulllineitems}

\index{nphys (slugpy.bayesphot.bp.bp attribute)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.nphys}\pysigline{\bfcode{nphys}}
This is the number of physical properties for the bayesphot
object.

\end{fulllineitems}

\index{pobs (slugpy.bayesphot.bp.bp attribute)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.pobs}\pysigline{\bfcode{pobs}}
The current set of observation probabilities for every
simulation in the library; data returned are in the same order
as the data originally used to construct the library

\end{fulllineitems}

\index{priors (slugpy.bayesphot.bp.bp attribute)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.priors}\pysigline{\bfcode{priors}}
The current set of prior probabilities for every
simulation in the library; data returned are in the same order
as the data originally used to construct the library

\end{fulllineitems}

\index{sample\_density (slugpy.bayesphot.bp.bp attribute)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.sample_density}\pysigline{\bfcode{sample\_density}}
The density with which the library was sampled, evaluated for
each simulation in the library

\end{fulllineitems}

\index{squeeze\_rep() (slugpy.bayesphot.bp.bp method)}

\begin{fulllineitems}
\phantomsection\label{bayesphot:slugpy.bayesphot.bp.bp.squeeze_rep}\pysiglinewithargsret{\bfcode{squeeze\_rep}}{\emph{x}, \emph{wgts}, \emph{dims=None}}{}
Takes an input array of positions and weights that form a
kernel density representation and approximates them using
fewer points, using an error tolerance of reltol
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{x}] \leavevmode{[}array, shape (N, ndim){]}
an array of points forming a kernel density
representation; on exit, x will be resized to (M, ndim)
with M \textless{}= N

\item[{wgts}] \leavevmode{[}array, shape (N){]}
an array of weights for the kernel density
representation; on exit, wgts will be resized to (M),
with M \textless{}= N

\item[{dims}] \leavevmode{[}array, shape (ndim){]}
array specifying which dimensions in the kernel density
representation the coordinates in x correspond to; if
left as None, they are assumed to correspond to the
first ndim dimensions in the data set

\end{description}

\item[{Returns:}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}


\end{fulllineitems}



\chapter{cluster\_slug: Bayesian Inference of Star Cluster Properties}
\label{cluster_slug:sec-cluster-slug}\label{cluster_slug::doc}\label{cluster_slug:cluster-slug-bayesian-inference-of-star-cluster-properties}
The slugpy.cluster\_slug module computes posterior probabilities for the mass, age, and extinction of star clusters from a set of input photometry.  It is implemented as a wrapper around {\hyperref[bayesphot:sec\string-bayesphot]{\crossref{\DUrole{std,std-ref}{bayesphot: Bayesian Inference for Stochastic Stellar Populations}}}}, so for details on how the calculation is performed see the bayesphot documentation.


\section{Getting the Default Library}
\label{cluster_slug:getting-the-default-library}
The cluster\_slug module requires a pre-computed library of slug simulations to use as a ``training set'' for its calculations. Due to its size, the default library \emph{is not} included in the slug git repository. Instead, it is provided for download from the \href{http://www.slugsps.com/data}{SLUG data products website}. Download the two files \code{clusterslug\_mw\_cluster\_phot.fits} and \code{clusterslug\_mw\_cluster\_prop.fits} and save them in the \code{cluster\_slug} directory of the main respository. If you do not do so, and do not provide your own library when you attempt to use cluster\_slug, you will be prompted to download the default library.


\section{Basic Usage}
\label{cluster_slug:basic-usage}
For an example of how to use cluster\_slug, see the file \code{cluster\_slug/cluster\_slug\_example.py} in the repository. All funtionality is provided through the cluster\_slug class. The basic steps are as follows:
\begin{enumerate}
\item {} 
Import the library and instantiate an \code{sfr\_slug} object (see {\hyperref[cluster_slug:ssec\string-cluster\string-slug\string-full]{\crossref{\DUrole{std,std-ref}{Full Documentation of slugpy.cluster\_slug}}}} for full details):

\begin{Verbatim}[commandchars=\\\{\}]
from slugpy.cluster\PYGZus{}slug import cluster\PYGZus{}slug
cs = cluster\PYGZus{}slug(photsystem=photsystem)
\end{Verbatim}

\end{enumerate}

This creates a cluster\_slug object, using the default simulation library. If you have another library of simulations you'd rather use, you can use the \code{libname} keyword to the \code{cluster\_slug} constructor to select it. The optional argument \code{photsystem} specifies the photometric system you will be using for your data. Possible values are \code{L\_nu} (flux per unit frequency, in erg/s/Hz), \code{L\_lambda} (flux per unit wavelength, in erg/s/Angstrom), \code{AB} (AB magnitudes), \code{STMAG} (ST magnitudes), and \code{Vega} (Vega magnitudes). If left unspecified, the photometric system will be whatever the library was written in; the default library is in the \code{L\_nu} system. Finally, if you have already read a library into memory using \code{read\_cluster}, you can set the keyword \code{lib} in the \code{cluster\_slug} constructor to specify that library should be used.
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
Specify your filter(s), for example:

\begin{Verbatim}[commandchars=\\\{\}]
cs.add\PYGZus{}filters([\PYGZsq{}WFC3\PYGZus{}UVIS\PYGZus{}F336W\PYGZsq{}, \PYGZsq{}WFC3\PYGZus{}UVIS\PYGZus{}F438W\PYGZsq{}, \PYGZsq{}WFC3\PYGZus{}UVIS\PYGZus{}F555W\PYGZsq{},
                \PYGZsq{}WFC3\PYGZus{}UVIS\PYGZus{}F814W\PYGZsq{}, \PYGZsq{}WFC3\PYGZus{}UVIS\PYGZus{}F657N\PYGZsq{}])
\end{Verbatim}

\end{enumerate}

The \code{add\_filter} method takes as an argument a string or list of strings specifying which filters were used for the observations you're going to analyze. You can have more than one set of filters active at a time (just by calling \code{add\_filters} more than once), and then specify which set of filters you're using for any given calculation.
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Specify your priors, for example:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{} Set priors to be flat in log T and A\PYGZus{}V, but vary with log M as
\PYGZsh{} p(log M) \PYGZti{} 1/M
def priorfunc(physprop):
   \PYGZsh{} Note: physprop is an array of shape (N, 3) where physprop[:,0] =
   \PYGZsh{} log M, physprop[:,1] = log T, physprop[:,2] = A\PYGZus{}V
   return 1.0/exp(physprop[:,0])
cs.priors = prorfunc
\end{Verbatim}

\end{enumerate}

The \code{priors} property specifies the assumed prior probability distribution on the physical properties of star clusters. It can be either \code{None} (in which case all simulations in the library are given equal prior probability), an array with as many elements as there are simulations in the library giving the prior for each one, or a callable that takes a vector of physical properties as input and returns the prior for it.
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Generate a marginal posterior probability distribuiton via:

\begin{Verbatim}[commandchars=\\\{\}]
logm, pdf = cs.mpdf(idx, phot, photerr = photerr)
\end{Verbatim}

\end{enumerate}

The first argument \code{idx} is an index for which posterior distribution should be computed -- a value of 0 generates the posterior in log mass, a value of 1 generates the posterion on log age, and a value of generates the posterior in A\_V. The second argument \code{phot} is an array giving the photometric values in the filters specified in step 2; make sure you're using the same photometric system you used in step 1. For the array \code{phot}, the trailing dimension must match the number of filters, and the marginal posterior-finding exercise is repeated over every value in the leading dimensions. If you have added two or more filter sets, you need to specify which one you want to use via the \code{filters} keyword. The optional argument \code{photerr} can be used to provide errors on the photometric values. The shape rules on it are the same as on \code{phot}, and the two leading dimensions of the two arrays will be broadcast together using normal broadcasting rules.

The \code{cluster\_slug.mpdf} method returns a tuple of two quantities. The first is a grid of values for log M, log T, or A\_V, depending on the value of \code{idx}. The second is the posterior probability distribution at each value of of the grid. Posteriors are normalized to have unit integral. If the input consisted of multiple sets of photometric values, the output will contains marginal posterior probabilities for each input. The output grid will be created automatically be default, but all aspects of it (shape, size, placement of grid points) can be controlled by keywords -- see {\hyperref[cluster_slug:ssec\string-cluster\string-slug\string-full]{\crossref{\DUrole{std,std-ref}{Full Documentation of slugpy.cluster\_slug}}}}.


\section{Using cluster\_slug in Parallel}
\label{cluster_slug:using-cluster-slug-in-parallel}
The \code{cluster\_slug} module has full support for threaded computation using the python \href{https://docs.python.org/2.7/library/multiprocessing.html}{multiprocessing module}. This allows efficient use of multiple cores on a shared memory machine, without the need for every project to read a large simulation library or store it in memory. See {\hyperref[bayesphot:ssec\string-bayesphot\string-threading]{\crossref{\DUrole{std,std-ref}{Support for Parallelism in bayesphot}}}} for full details on the recommended paradigm for parallel computing. The full list of thread-safe \code{cluster\_slug} methods is:
\begin{itemize}
\item {} 
\code{cluster\_slug.logL}

\item {} 
\code{cluster\_slug.mpdf}

\item {} 
\code{cluster\_slug.mcmc}

\item {} 
\code{cluster\_slug.bestmatch}

\item {} 
\code{cluster\_slug.make\_approx\_phot}

\item {} 
\code{cluster\_slug.make\_approx\_phys}

\item {} 
\code{cluster\_slug.squeeze\_rep}

\item {} 
\code{cluster\_slug.mpdf\_approx}

\end{itemize}


\section{Making Your Own Library}
\label{cluster_slug:making-your-own-library}
You can generate your own library by running slug; you might want to do this, for example, to have a library that works at different metallicity or for a different set of stellar tracks. An example parameter file (the one that was used to generate the default clusterslug\_mw library) is included in the \code{cluster\_slug} directory. This file uses slug's capability to pick the output time and the cluster mass from specified PDFs.

One subtle post-processing step you should take once you've generated your library is to read it in using {\hyperref[slugpy:sec\string-slugpy]{\crossref{\DUrole{std,std-ref}{slugpy -- The Python Helper Library}}}} and then write the photometry back out using the \code{slugpy.write\_cluster\_phot} routine with the format set to \code{fits2}. This uses an alternative FITS format that is faster to search when you want to load only a few filters out of a large library. For large data sets, this can reduce cluster\_slug load times by an order of magnitude. (To be precise: the default format for FITS outputs to put all filters into a single binary table HDU, while the \code{fits2} format puts each filter in its own HDU. This puts all data for a single filter into a contiguous block, rather than all the data for a single cluster into a contiguous block, and is therefore faster to load when one wants to load the data filter by filter.)


\section{Variable Mode IMF}
\label{cluster_slug:variable-mode-imf}
If your library was run with variable IMF parameters, these can also be used in \code{cluster\_slug}. When creating a \code{cluster\_slug} object, you can pass the array \code{vp\_list} as an argument. This list should have an element for each variable parameter in your library. Each element should then be either \code{True} or \code{False} depending on whether you wish to include this parameter in the analysis.
For example, for a library with four variable parameters you could have:

\begin{Verbatim}[commandchars=\\\{\}]
vp\PYGZus{}list=[True,False,True,True]
\end{Verbatim}


\section{Full Documentation of slugpy.cluster\_slug}
\label{cluster_slug:ssec-cluster-slug-full}\label{cluster_slug:full-documentation-of-slugpy-cluster-slug}\index{cluster\_slug (class in slugpy.cluster\_slug)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug}\pysiglinewithargsret{\strong{class }\code{slugpy.cluster\_slug.}\bfcode{cluster\_slug}}{\emph{libname=None}, \emph{filters=None}, \emph{photsystem=None}, \emph{lib=None}, \emph{bw\_phys=0.1}, \emph{bw\_phot=None}, \emph{ktype='gaussian'}, \emph{priors=None}, \emph{sample\_density=None}, \emph{reltol=0.01}, \emph{abstol=1e-08}, \emph{leafsize=16}, \emph{use\_nebular=True}, \emph{use\_extinction=True}, \emph{thread\_safe=True}, \emph{vp\_list={[}{]}}}{}
A class that can be used to estimate the PDF of star cluster
properties (mass, age, extinction) from a set of input photometry
in various bands.
\begin{description}
\item[{Properties}] \leavevmode\begin{description}
\item[{priors}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} None{]}
prior probability on each data point; interpretation
depends on the type passed; array, shape (N): values are
interpreted as the prior probability of each data point;
callable: the callable must take as an argument an array
of shape (N, nphys), and return an array of shape (N)
giving the prior probability at each data point; None:
all data points have equal prior probability

\item[{abstol}] \leavevmode{[}float{]}
absolute error tolerance for kernel density estimation

\item[{reltol}] \leavevmode{[}float{]}
relative error tolerance for kernel density estimation

\item[{thread\_safe}] \leavevmode{[}bool{]}
if True, the computation routines will run in thread-safe
mode, allowing use with multiprocessing; this incurs a small
performance penalty

\end{description}

\item[{Methods}] \leavevmode\begin{description}
\item[{filters()}] \leavevmode{[}{]}
returns list of filters available in the library

\item[{filtersets() :}] \leavevmode
return a list of the currently-loaded filter sets

\item[{filter\_units() :}] \leavevmode
returns units for available filters

\item[{add\_filters()}] \leavevmode{[}{]}
adds a set of filters for use in parameter estimation

\item[{logL()}] \leavevmode{[}{]}
compute log likelihood at a particular set of physical and
photometric parameters

\item[{mpdf()}] \leavevmode{[}{]}
computer marginal posterior probability distribution for a
set of photometric measurements

\item[{mcmc() :}] \leavevmode
due MCMC estimation of the posterior PDF on a set of
photometric measurments

\item[{bestmatch()}] \leavevmode{[}{]}
find the simulations in the library that are the closest
matches to the input photometry

\item[{make\_approx\_phot() :}] \leavevmode
given a set of physical properties, return a set of points
that can be used for fast approximation of the corresponding
photometric properties

\item[{make\_approx\_phys() :}] \leavevmode
given a set of photometric properties, return a set of points
that can be used for fast approximation of the corresponding
physical properties

\end{description}

\end{description}
\index{\_\_init\_\_() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{libname=None}, \emph{filters=None}, \emph{photsystem=None}, \emph{lib=None}, \emph{bw\_phys=0.1}, \emph{bw\_phot=None}, \emph{ktype='gaussian'}, \emph{priors=None}, \emph{sample\_density=None}, \emph{reltol=0.01}, \emph{abstol=1e-08}, \emph{leafsize=16}, \emph{use\_nebular=True}, \emph{use\_extinction=True}, \emph{thread\_safe=True}, \emph{vp\_list={[}{]}}}{}
Initialize a cluster\_slug object.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{libname}] \leavevmode{[}string{]}
name of the SLUG model to load; if left as None, the default
is \$SLUG\_DIR/cluster\_slug/modp020\_chabrier\_MW

\item[{lib}] \leavevmode{[}object{]}
a library read by the read\_cluster function; if specified
this overrides the libname option; the library must
contain both physical properties and photometry, and
must include filter data; if this is not None, then the 
photsystem keyword is ignored

\item[{filters}] \leavevmode{[}iterable of stringlike{]}
list of filter names to be used for inferenence

\item[{photsystem}] \leavevmode{[}None or string{]}
If photsystem is None, the library will be left in
whatever photometric system was used to write
it. Alternately, if it is a string, the data will be
converted to the specified photometric system. Allowable
values are `L\_nu', `L\_lambda', `AB', `STMAG', and
`Vega', corresponding to the options defined in the SLUG
code. Once this is set, any subsequent photometric data
input are assumed to be in the same photometric system.

\item[{bw\_phys}] \leavevmode{[}`auto' \textbar{} float \textbar{} array, shape (2) \textbar{} array, shape (3){]}
bandwidth for the physical quantities in the kernel
density estimation; if set to `auto', the bandwidth will
be estimated automatically; if set to a scalar quantity,
this will be used for all physical quantities; if set to
an array, the array must have 2 elements if
use\_extinction is False, or 3 if it is True

\item[{bw\_phot}] \leavevmode{[}None \textbar{} `auto' \textbar{} float \textbar{} array{]}
bandwidth for the photometric quantities; if set to
None, defaults to 0.25 mag / 0.1 dex; if set to `auto',
bandwidth is estimated automatically; if set to a float,
this bandwidth is used for all photometric dimensions;
if set to an array, the array must have the same number
of dimensions as len(filters)

\item[{ktype}] \leavevmode{[}string{]}
type of kernel to be used in densty estimation; allowed
values are `gaussian' (default), `epanechnikov', and
`tophat'; only Gaussian can be used with error bars

\item[{priors}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} None{]}
prior probability on each data point; interpretation
depends on the type passed; array, shape (N): values are
interpreted as the prior probability of each data point;
callable: the callable must take as an argument an array
of shape (N, nphys), and return an array of shape (N)
giving the prior probability at each data point; None:
all data points have equal prior probability

\item[{pobs}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} None{]}
probability of being observed on each data point; interpretation
depends on the type passed; array, shape (N): values are
interpreted as the prior probability of each data point;
callable: the callable must take as an argument an array
of shape (N, nphys), and return an array of shape (N)
giving the prior probability at each data point; None:
all data points have equal prior probability

\item[{sample\_density}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} `auto' \textbar{} None{]}
the density of the data samples at each data point; this
need not match the prior density; interpretation depends
on the type passed; array, shape (N): values are
interpreted as the density of data sampling at each
sample point; callable: the callable must take as an
argument an array of shape (N, nphys), and return an
array of shape (N) giving the sampling density at each
point; `auto': the sample density will be computed
directly from the data set; note that this can be quite
slow for large data sets, so it is preferable to specify
this analytically if it is known; None: data are assumed
to be uniformly sampled, or to be sampled as the default
library is if libname is also None

\item[{reltol}] \leavevmode{[}float{]}
relative error tolerance; errors on all returned
probabilities p will satisfy either
abs(p\_est - p\_true) \textless{}= reltol * p\_est   OR
abs(p\_est - p\_true) \textless{}= abstol,
where p\_est is the returned estimate and p\_true is the
true value

\item[{abstol}] \leavevmode{[}float{]}
absolute error tolerance; see above

\item[{leafsize}] \leavevmode{[}int{]}
number of data points in each leaf of the KD tree

\item[{use\_nebular}] \leavevmode{[}bool{]}
if True, photometry including nebular emission will be
used if available; if not, nebular emission will be
omitted

\item[{use\_extinction}] \leavevmode{[}bool{]}
if True, photometry including extinction will be used;
if not, it will be omitted, and in this case no results
making use of the A\_V dimension will be available

\item[{thread\_safe}] \leavevmode{[}bool{]}
if True, cluster\_slug will make extra copies of internals
as needed to ensure thread safety when the computation
routines (logL, mpdf, mcmc, bestmatch, make\_approx\_phot,
make\_approx\_phys, mpdf\_approx) are used with
multiprocessing; this incurs a minor performance
penalty, and can be disabled by setting to False if the
code will not be run with the multiprocessing module

\item[{vp\_list}] \leavevmode{[}list{]}
A list with an element for each of the variable parameters
in the data. An element is set to True if we wish to use
that parameter here, or False if we do not.

\end{description}

\item[{Returns}] \leavevmode
Nothing

\item[{Raises}] \leavevmode
IOError, if the library cannot be found

\end{description}

\end{fulllineitems}

\index{\_\_weakref\_\_ (slugpy.cluster\_slug.cluster\_slug attribute)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.__weakref__}\pysigline{\bfcode{\_\_weakref\_\_}}
list of weak references to the object (if defined)

\end{fulllineitems}

\index{add\_filters() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.add_filters}\pysiglinewithargsret{\bfcode{add\_filters}}{\emph{filters}, \emph{bandwidth=None}, \emph{pobs=None}}{}
Add a set of filters to use for cluster property estimation
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{filters}] \leavevmode{[}iterable of stringlike{]}
list of filter names to be used for inferenence

\item[{bandwidth}] \leavevmode{[}None \textbar{} `auto' \textbar{} float \textbar{} array{]}
bandwidth for the photometric quantities; if set to
None, the bandwidth is unchanged for an existing filter
set, and for a newly-created one the default physical
and photometric bandwidths are used; if set to `auto',
bandwidth is estimated automatically; if set to a float,
this bandwidth is used for all physical and photometric
dimensions; if set to an array, the array must have the
same number of entries as nphys+len(filters)

\item[{pobs}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} `equal' \textbar{} None{]}
the probability that a particular object would be observed,
which is used, like prior, to weight the library;
interpretation depends on type. `equal' means all objects are
equally likely to be observed, array is an array giving the
observation probability of each object in the library, and
callable means must be a function that takes an array
containing the photometry, of shape (N, nhpot), as an
argument, and returns an array of shape (N) giving the
probability of observation for that object. Finally,
None leaves the observational probability unchanged

\end{description}

\item[{Returns}] \leavevmode
nothing

\end{description}

\end{fulllineitems}

\index{bestmatch() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.bestmatch}\pysiglinewithargsret{\bfcode{bestmatch}}{\emph{phot}, \emph{photerr=None}, \emph{nmatch=1}, \emph{bandwidth\_units=False}, \emph{filters=None}}{}
Searches through the simulation library and returns the closest
matches to an input set of photometry.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{phot}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors, which must have the
same shape as phot; if this is not None,
then distances will be measured in units of the
photometric error if bandwidth\_units is False, or in
units of the bandwidth added in quadrature with the
errors if it is True

\item[{nmatch}] \leavevmode{[}int{]}
number of matches to return; returned matches will be
ordered by distance from the input

\item[{bandwidth\_units}] \leavevmode{[}bool{]}
if False, distances are computed based on the
logarithmic difference in luminosity; if True, they are
measured in units of the bandwidth

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{matches}] \leavevmode{[}array, shape (..., nmatch, nphys + nfilter){]}
best matches to the input photometry; shape in the
leading dimensions will be the same as for phot, and if
nmatch == 1 then that dimension will be omitted; in the
final dimension, the first 3 elements give log M, log T,
and A\_V, while the last nfilter give the photometric
values; if created with use\_extinct = False, the A\_V
dimension is omitted

\item[{dist}] \leavevmode{[}array, shape (..., nmatch){]}
distances between the matches and the input photometry

\end{description}

\end{description}

\end{fulllineitems}

\index{bestmatch\_phys() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.bestmatch_phys}\pysiglinewithargsret{\bfcode{bestmatch\_phys}}{\emph{phys}, \emph{nmatch=1}, \emph{bandwidth\_units=False}, \emph{filters=None}}{}
Searches through the simulation library and returns the closest
matches to an input set of photometry.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{phys}] \leavevmode{[}arraylike, shape (nphys) or (..., nphys){]}
array giving the physical values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{nmatch}] \leavevmode{[}int{]}
number of matches to return; returned matches will be
ordered by distance from the input

\item[{bandwidth\_units}] \leavevmode{[}bool{]}
if False, distances are computed based on the
logarithmic difference in physical properties; if True,
they are measured in units of the bandwidth

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{matches}] \leavevmode{[}array, shape (..., nmatch, nphys + nfilter){]}
best matches to the input properties; shape in the
leading dimensions will be the same as for phot, and if
nmatch == 1 then that dimension will be omitted

\item[{dist}] \leavevmode{[}array, shape (..., nmatch){]}
distances between the matches and the input physical
properties

\end{description}

\end{description}

\end{fulllineitems}

\index{del\_filters() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.del_filters}\pysiglinewithargsret{\bfcode{del\_filters}}{\emph{filters}}{}
Remove a set of filters, freeing the memory associated with
them. Note that this does not delete the underlying library
data, just the data for the KD tree used internally.
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{filters}] \leavevmode{[}iterable of stringlike{]}
list of filter names

\end{description}

\item[{Returns}] \leavevmode
Nothing

\item[{Raises}] \leavevmode
KeyError if the input set of filters is not loaded

\end{description}

\end{fulllineitems}

\index{filter\_units() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.filter_units}\pysiglinewithargsret{\bfcode{filter\_units}}{}{}
Returns list of all available filter units
\begin{description}
\item[{Parameters:}] \leavevmode
None

\item[{Returns:}] \leavevmode\begin{description}
\item[{units}] \leavevmode{[}list of strings{]}
list of available filter units

\end{description}

\end{description}

\end{fulllineitems}

\index{filters() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.filters}\pysiglinewithargsret{\bfcode{filters}}{}{}
Returns list of all available filters
\begin{description}
\item[{Parameters:}] \leavevmode
None

\item[{Returns:}] \leavevmode\begin{description}
\item[{filters}] \leavevmode{[}list of strings{]}
list of available filter names

\end{description}

\end{description}

\end{fulllineitems}

\index{filtersets() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.filtersets}\pysiglinewithargsret{\bfcode{filtersets}}{}{}
Returns list of all currently-loaded filter sets
\begin{description}
\item[{Parameters:}] \leavevmode
None

\item[{Returns:}] \leavevmode\begin{description}
\item[{filtersets}] \leavevmode{[}list of list of strings{]}
list of currently-loaded filter sets

\end{description}

\end{description}

\end{fulllineitems}

\index{load\_data() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.load_data}\pysiglinewithargsret{\bfcode{load\_data}}{\emph{filter\_name}, \emph{bandwidth=None}, \emph{force\_reload=False}}{}
Loads photometric data for the specified filter into memory
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{filter\_name}] \leavevmode{[}string{]}
name of filter to load

\item[{bandwidth}] \leavevmode{[}float{]}
default bandwidth for this filter

\item[{force\_reload}] \leavevmode{[}bool{]}
if True, reinitialize the data even if has already been
stored

\end{description}

\item[{Returns:}] \leavevmode
None

\item[{Raises:}] \leavevmode
ValueError, if filter\_name is not one of the available
filters

\end{description}

\end{fulllineitems}

\index{logL() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.logL}\pysiglinewithargsret{\bfcode{logL}}{\emph{physprop}, \emph{photprop}, \emph{photerr=None}, \emph{filters=None}}{}
This function returns the natural log of the likelihood
function evaluated at a particular log mass, log age,
extinction, and set of log luminosities
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{physprop}] \leavevmode{[}arraylike, shape (nhpys) or (..., nphys){]}
array giving values of the log M, log T, and A\_V; for a
multidimensional array, the operation is vectorized over
the leading dimensions; if created with use\_extinct =
False, the A\_V dimension should be omitted.
Will also include variable any variable parameters VPx if
they are requested.

\item[{photprop}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{logL}] \leavevmode{[}float or arraylike{]}
natural log of the likelihood function

\end{description}

\end{description}

\end{fulllineitems}

\index{make\_approx\_phot() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.make_approx_phot}\pysiglinewithargsret{\bfcode{make\_approx\_phot}}{\emph{phys}, \emph{squeeze=True}, \emph{filter\_ignore=None}, \emph{filters=None}}{}
Returns an object that can be used for a fast approximation of
the PDF of photometric properties that corresponds to a set of
physical properties. The PDF produced by summing over the
points returned is guaranteed to account for at least 1-reltol
of the marginal photometric probability, and to represent the
shape of the PDF in photometric space within a local accuracy
of reltol as well.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{phys}] \leavevmode{[}arraylike, shape (nphys) or (N, nphys){]}
the set or sets of physical properties for which the
approximation is to be generated

\item[{squeeze}] \leavevmode{[}bool{]}
if True, the representation returned will be squeezed to
minimize the number of points included, using reltol as
the error tolerance

\item[{filter\_ignore}] \leavevmode{[}None or listlike of bool{]}
if None, the kernel density representation returned
covers all filters; otherwise this must be a listlike of
bool, one entry per filter, with a value of False
indicating that filter should be excluded from the
values returned; suppressing filters can allow for more
efficient representations

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{x}] \leavevmode{[}array, shape (M, nphot), or a list of such arrays{]}
an array containing the list of points to be used for
the approximation

\item[{wgts}] \leavevmode{[}array, shape (M), or a list of such arrays{]}
an array containing the weights of the points

\end{description}

\item[{Notes:}] \leavevmode
if the requested relative tolerance cannot be reached for
numerical reasons (usually because the input point is too
far from the library to allow accurate computation), x and
wgts will be return as None, and a warning will be issued

\end{description}

\end{fulllineitems}

\index{make\_approx\_phys() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.make_approx_phys}\pysiglinewithargsret{\bfcode{make\_approx\_phys}}{\emph{phot}, \emph{photerr=None}, \emph{squeeze=True}, \emph{phys\_ignore=None}, \emph{filters=None}}{}
Returns an object that can be used for a fast approximation of
the PDF of physical properties that corresponds to a set of
photometric properties. The PDF produced by summing over the
points returned is guaranteed to account for at least 1-reltol
of the marginal photometric probability, and to represent the
shape of the PDF in photometric space within a local accuracy
of reltol as well.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{phot}] \leavevmode{[}arraylike, shape (nfilter) or (N, nfilter){]}
the set or sets of photometric properties for which the
approximation is to be generated

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (N, nfilter){]}
array giving photometric errors; the number of elements
in the output lists will be the size that results from
broadcasting together the leading dimensions of phot and
photerr

\item[{squeeze}] \leavevmode{[}bool{]}
if True, the representation returned will be squeezed to
minimize the number of points included, using reltol as
the error tolerance

\item[{phys\_ignore}] \leavevmode{[}None or listlike of bool{]}
if None, the kernel density representation returned
covers all physical properties; otherwise this must be a
listlike of bool, one entry per physical dimension, with
a value of False indicating that dimension should be
excluded from the values returned; suppressing
dimensions can allow for more efficient representations

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{x}] \leavevmode{[}array, shape (M, nphys), or a list of such arrays{]}
an array containing the list of points to be used for
the approximation, where nphys is the number of
physical dimensions being returned

\item[{wgts}] \leavevmode{[}array, shape (M), or a list of such arrays{]}
an array containing the weights of the points

\end{description}

\item[{Notes:}] \leavevmode
if the requested relative tolerance cannot be reached for
numerical reasons (usually because the input point is too
far from the library to allow accurate computation), x and
wgts will be return as None, and a warning will be issued

\end{description}

\end{fulllineitems}

\index{mcmc() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.mcmc}\pysiglinewithargsret{\bfcode{mcmc}}{\emph{photprop}, \emph{photerr=None}, \emph{mc\_walkers=100}, \emph{mc\_steps=500}, \emph{mc\_burn\_in=50}, \emph{filters=None}}{}
This function returns a sample of MCMC walkers for cluster
mass, age, and extinction
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{photprop}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\item[{mc\_walkers}] \leavevmode{[}int{]}
number of walkers to use in the MCMC

\item[{mc\_steps}] \leavevmode{[}int{]}
number of steps in the MCMC

\item[{mc\_burn\_in}] \leavevmode{[}int{]}
number of steps to consider ``burn-in'' and discard

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{samples}] \leavevmode{[}array{]}
array of sample points returned by the MCMC

\end{description}

\end{description}

\end{fulllineitems}

\index{mpdf() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.mpdf}\pysiglinewithargsret{\bfcode{mpdf}}{\emph{idx}, \emph{photprop}, \emph{photerr=None}, \emph{ngrid=128}, \emph{qmin=None}, \emph{qmax=None}, \emph{grid=None}, \emph{norm=True}, \emph{filters=None}}{}
Returns the marginal probability for one or mode physical
quantities for one or more input sets of photometric
properties. Output quantities are computed on a grid of
values, in the same style as meshgrid
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{idx}] \leavevmode{[}int or listlike containing ints{]}
index of the physical quantity whose PDF is to be
computed; 0 = log M, 1 = log T, 2 = A\_V, (2 or 3)+x = VPx; 
if this is an iterable, the joint distribution of the indicated
quantities is returned

\item[{photprop}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\item[{ngrid}] \leavevmode{[}int or listlike containing ints{]}
number of points in each dimension of the output grid;
if this is an iterable, it must have the same number of
elements as idx

\item[{qmin}] \leavevmode{[}float or listlike{]}
minimum value in the output grid in each quantity; if
left as None, defaults to the minimum value in the
library; if this is an iterable, it must contain the
same number of elements as idx

\item[{qmax}] \leavevmode{[}float or listlike{]}
maximum value in the output grid in each quantity; if
left as None, defaults to the maximum value in the
library; if this is an iterable, it must contain the
same number of elements as idx

\item[{grid}] \leavevmode{[}listlike of arrays{]}
set of values defining the grid on which the PDF is to
be evaluated, in the same format used by meshgrid

\item[{norm}] \leavevmode{[}bool{]}
if True, returned pdf's will be normalized to integrate
to 1

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{grid\_out}] \leavevmode{[}array{]}
array of values at which the PDF is evaluated; contents
are the same as returned by meshgrid

\item[{pdf}] \leavevmode{[}array{]}
array of marginal posterior probabilities at each point
of the output grid, for each input cluster; the leading
dimensions match the leading dimensions produced by
broadcasting the leading dimensions of photprop and
photerr together, while the trailing dimensions match
the dimensions of the output grid

\end{description}

\end{description}

\end{fulllineitems}

\index{mpdf\_approx() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.mpdf_approx}\pysiglinewithargsret{\bfcode{mpdf\_approx}}{\emph{x}, \emph{wgts}, \emph{dims='phys'}, \emph{dims\_return=None}, \emph{ngrid=64}, \emph{qmin='all'}, \emph{qmax='all'}, \emph{grid=None}, \emph{norm=True}, \emph{filters=None}}{}
Returns the marginal posterior PDF computed from a kernel
density approximation returned by make\_approx\_phys or
make\_approx\_phot. Outputs are computed on a grid of values, in
the same style as meshgrid.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{x}] \leavevmode{[}array, shape (M, ndim), or a list of such arrays{]}
array of points retured by make\_approx\_phot or
make\_approx\_phys

\item[{wgts}] \leavevmode{[}array, shape (M) or a list of such arrays{]}
array of weights returned by make\_approx\_phot or
make\_approx\_phys

\item[{dims}] \leavevmode{[}`phys' \textbar{} `phot' \textbar{} arraylike of ints{]}
dimensions covered by x and wgts; the strings `phys' or
`phot' indicate that they cover all physical or
photometric dimensions, and correspond to the defaults
returned by make\_approx\_phys and make\_approx\_phot,
respectively; if dims is an array of ints, these specify
the dimensions covered by x and wgts, where the
physical dimensions are numbered 0, 1, ... nphys-1, and
the photometric ones are nphys, nphys+1,
... nphys+nphot-1

\item[{dims\_return}] \leavevmode{[}None or arraylike of ints{]}
if None, the output PDF has the same dimensions as
specified in dims; if not, then dims\_return must be a
subset of dims, and a marginal PDF in certain dimensions
will be generated

\item[{ngrid}] \leavevmode{[}int or listlike containing ints{]}
number of points in each dimension of the output grid;
if this is an iterable, it must have the same number of
elements as idx

\item[{qmin}] \leavevmode{[}float \textbar{} listlike \textbar{} `zoom' \textbar{} `all' {]}
minimum value in the output grid in each quantity; if
this a float, it is applied to each dimension; if it is
an iterable, it must contain the same number of elements
as the number of dimensions being returned, as gives the
minimum in each dimension; if it is `zoom' or `all', the
minimum is chosen automatically, with `zoom' focusing on
a region encompassing the probability maximum, and `all'
encompassing all the points in the representation

\item[{qmax}] \leavevmode{[}float \textbar{} listlike \textbar{} `zoom' \textbar{} `all'{]}
same as qmin, but for the maximum of the output grid

\item[{grid}] \leavevmode{[}listlike of arrays{]}
set of values defining the grid on which the PDF is to
be evaluated, in the same format used by meshgrid

\item[{norm}] \leavevmode{[}bool{]}
if True, returned pdf's will be normalized to integrate
to 1

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{grid\_out}] \leavevmode{[}array{]}
array of values at which the PDF is evaluated; contents
are the same as returned by meshgrid

\item[{pdf}] \leavevmode{[}array{]}
array of marginal posterior probabilities at each point
of the output grid, for each input cluster; the leading
dimensions match the leading dimensions produced by
broadcasting the leading dimensions of photprop and
photerr together, while the trailing dimensions match
the dimensions of the output grid

\end{description}

\end{description}

\end{fulllineitems}

\index{mpdf\_gen() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.mpdf_gen}\pysiglinewithargsret{\bfcode{mpdf\_gen}}{\emph{fixeddim}, \emph{fixedprop}, \emph{margindim}, \emph{ngrid=128}, \emph{qmin=None}, \emph{qmax=None}, \emph{grid=None}, \emph{norm=True}, \emph{filters=None}}{}
Returns the marginal probability for one or more physical or
photometric properties, keeping other properties fixed and
marginalizing over other quantities. This is the most general
marginal PDF routine provided.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{fixeddim}] \leavevmode{[}int \textbar{} arraylike of ints \textbar{} None{]}
The index or indices of the physical or photometric
properties to be held fixed; physical properties are
numbered 0 ... nphys-1, and photometric ones are numbered
nphys ... nphys + nphot - 1. This can also be set to
None, in which case no properties are held fixed.

\item[{fixedprop}] \leavevmode{[}array \textbar{} None{]}
The values of the properties being held fixed; the size
of the final dimension must be equal to the number of
elements in fixeddim, and if fixeddim is None, this must
be too

\item[{margindim}] \leavevmode{[}int \textbar{} arraylike of ints \textbar{} None{]}
The index or indices of the physical or photometric
properties to be maginalized over, numbered in the same
way as with fixeddim; if set to None, no marginalization
is performed

\item[{ngrid}] \leavevmode{[}int or listlike containing ints{]}
number of points in each dimension of the output grid;
if this is an iterable, it must have nphys + nphot -
len(fixeddim) - len(margindim) elements

\item[{qmin}] \leavevmode{[}float \textbar{} arraylike{]}
minimum value in the output grid in each quantity; if
left as None, defaults to the minimum value in the
library; if this is an iterable, it must contain a
number of elements equal to nphys + nphot -
len(fixeddim) - len(margindim)

\item[{qmax}] \leavevmode{[}float \textbar{} arraylike{]}
maximum value in the output grid in each quantity; if
left as None, defaults to the maximum value in the
library; if this is an iterable, it must have the same
number of elements as qmin

\item[{grid}] \leavevmode{[}listlike of arrays{]}
set of values defining the grid on which the PDF is to
be evaluated, in the same format used by meshgrid

\item[{norm}] \leavevmode{[}bool{]}
if True, returned pdf's will be normalized to integrate
to 1

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{grid\_out}] \leavevmode{[}array{]}
array of values at which the PDF is evaluated; contents
are the same as returned by meshgrid

\item[{pdf}] \leavevmode{[}array{]}
array of marginal posterior probabilities at each point
of the output grid, for each input set of properties; the leading
dimensions match the leading dimensions produced by
broadcasting the leading dimensions of photprop and
photerr together, while the trailing dimensions match
the dimensions of the output grid

\end{description}

\end{description}

\end{fulllineitems}

\index{mpdf\_phot() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.mpdf_phot}\pysiglinewithargsret{\bfcode{mpdf\_phot}}{\emph{idx}, \emph{physprop}, \emph{ngrid=128}, \emph{qmin=None}, \emph{qmax=None}, \emph{grid=None}, \emph{norm=True}, \emph{filters=None}}{}
Returns the marginal probability for one or more photometric
quantities corresponding to an input set or distribution of
physical properties. Output quantities are computed on a grid of
values, in the same style as meshgrid.
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{idx}] \leavevmode{[}int or listlike containing ints{]}
index of the photometric quantity whose PDF is to be
computed, starting at 0; indices correspond to the order
of elements in the filters argument; if this is an
iterable, the joint distribution of the indicated
quantities is returned

\item[{physprop}] \leavevmode{[}arraylike, shape (nphys) or (..., nphys){]}
physical properties to be used; if this is an array of
nphys elements, these give the physical properties; if
it is a multidimensional array, the operation is
vectoried over the leading dimensions
physical properties -- the function must take an array
of (nphys) elements as an input, and return a floating
point value representing the PDF evaluated at that set
of physical properties as an output

\item[{ngrid}] \leavevmode{[}int or listlike containing ints{]}
number of points in each dimension of the output grid;
if this is an iterable, it must have the same number of
elements as idx

\item[{qmin}] \leavevmode{[}float or listlike{]}
minimum value in the output grid in each quantity; if
left as None, defaults to the minimum value in the
library; if this is an iterable, it must contain the
same number of elements as idx

\item[{qmax}] \leavevmode{[}float or listlike{]}
maximum value in the output grid in each quantity; if
left as None, defaults to the maximum value in the
library; if this is an iterable, it must contain the
same number of elements as idx

\item[{grid}] \leavevmode{[}listlike of arrays{]}
set of values defining the grid on which the PDF is to
be evaluated, in the same format used by meshgrid

\item[{norm}] \leavevmode{[}bool{]}
if True, returned pdf's will be normalized to integrate
to 1

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{grid\_out}] \leavevmode{[}array{]}
array of values at which the PDF is evaluated; contents
are the same as returned by meshgrid

\item[{pdf}] \leavevmode{[}array{]}
array of marginal posterior probabilities at each point
of the output grid, for each input set of properties; the leading
dimensions match the leading dimensions produced by
broadcasting the leading dimensions of photprop and
photerr together, while the trailing dimensions match
the dimensions of the output grid

\end{description}

\end{description}

\end{fulllineitems}

\index{squeeze\_rep() (slugpy.cluster\_slug.cluster\_slug method)}

\begin{fulllineitems}
\phantomsection\label{cluster_slug:slugpy.cluster_slug.cluster_slug.squeeze_rep}\pysiglinewithargsret{\bfcode{squeeze\_rep}}{\emph{x}, \emph{wgts}, \emph{dims=None}, \emph{filters=None}}{}
Takes an input array of positions and weights that form a
kernel density representation and approximates them using
fewer points, using an error tolerance of reltol
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{x}] \leavevmode{[}array, shape (N, ndim){]}
an array of points forming a kernel density
representation; on exit, x will be resized to (M, ndim)
with M \textless{}= N

\item[{wgts}] \leavevmode{[}array, shape (N){]}
an array of weights for the kernel density
representation; on exit, wgts will be resized to (M),
with M \textless{}= N

\item[{dims}] \leavevmode{[}array, shape (ndim){]}
array specifying which dimensions in the kernel density
representation the coordinates in x correspond to; if
left as None, they are assumed to correspond to the
first ndim dimensions in the data set

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns:}] \leavevmode
Nothing

\end{description}

\end{fulllineitems}


\end{fulllineitems}



\chapter{sfr\_slug: Bayesian Inference of Star Formation Rates}
\label{sfr_slug:sfr-slug-bayesian-inference-of-star-formation-rates}\label{sfr_slug::doc}\label{sfr_slug:sec-sfr-slug}
The slugy.sfr\_slug module computes posterior probabilities on star formation rates given a set of star formation rates estimated using the ``point mass estimate'' (i.e., the estimate you would get for a fully sampled stellar population) for the SFR based on the ionizing, FUV, or bolometric luminosity. It is implemented as a wrapper around {\hyperref[bayesphot:sec\string-bayesphot]{\crossref{\DUrole{std,std-ref}{bayesphot: Bayesian Inference for Stochastic Stellar Populations}}}}, so for details on how the calculation is performed see the bayesphot documentation.


\section{Getting the Default Library}
\label{sfr_slug:getting-the-default-library}
The sfr\_slug module requires a pre-computed library of slug simulations to use as a ``training set'' for its calculations. Due to its size, the default library \emph{is not} included in the slug git repository. Instead, it is provided for download from the \href{http://www.slugsps.com/data}{SLUG data products website}. Download the two files \code{SFR\_SLUG\_integrated\_phot.fits} and \code{SFR\_SLUG\_integrated\_prop.fits} and save them in the \code{sfr\_slug} directory of the main respository. If you do not do so, and do not provide your own library when you attempt to use sfr\_slug, you will be prompted to download the default library.


\section{Basic Usage}
\label{sfr_slug:basic-usage}
The \code{sfr\_slug/sfr\_slug\_example.py} file in the repository provides an example of how to use sfr\_slug. Usage of is simple, as the functionality is all implemented through a single class, sfr\_slug. The required steps are as follows:
\begin{enumerate}
\item {} 
Import the library and instantiate an \code{sfr\_slug} object (see {\hyperref[sfr_slug:sec\string-sfr\string-slug\string-full]{\crossref{\DUrole{std,std-ref}{Full Documentation of slugpy.sfr\_slug}}}} for full details):

\begin{Verbatim}[commandchars=\\\{\}]
from slugpy.sfr\PYGZus{}slug import sfr\PYGZus{}slug
sfr\PYGZus{}estimator = sfr\PYGZus{}slug()
\end{Verbatim}

\end{enumerate}

This creates an sfr\_slug object, using the default simulation library, \$SLUG\_DIR/sfr\_slug/SFR\_SLUG. If you have another library of simulations you'd rather use, you can use the \code{libname} keyword to the \code{sfr\_slug} constructor to select it.
\begin{enumerate}
\setcounter{enumi}{1}
\item {} 
Specify your filter(s), for example:

\begin{Verbatim}[commandchars=\\\{\}]
sfr\PYGZus{}estimator.add\PYGZus{}filters(\PYGZsq{}QH0\PYGZsq{})
\end{Verbatim}

\end{enumerate}

The \code{add\_filter} method takes as an argument a string or list of strings specifying which filters you're going to point mass SFRs based on. You can have more than one set of filters active at a time (just by calling \code{add\_filters} more than once), and then specify which set of filters you're using for any given calculation.
\begin{enumerate}
\setcounter{enumi}{2}
\item {} 
Specify your priors, for example:

\begin{Verbatim}[commandchars=\\\{\}]
sfr\PYGZus{}estimator.priors = \PYGZsq{}schechter\PYGZsq{}
\end{Verbatim}

\end{enumerate}

The \code{priors} property specifies the assumed prior probability distribution on the star formation rate. It can be either \code{None} (in which case all simulations in the library are given equal prior probability), an array with as many elements as there are simulations in the library giving the prior for each one, a callable that takes a star formation rate as input and returns the prior for it, or a string whose value is either ``flat'' or ``prior''. The two strings specify, respectively, a prior distribution that is either flat in log SFR or follows the Schechter function SFR distribution from \href{http://adsabs.harvard.edu/abs/2011MNRAS.415.1815B}{Bothwell et al. (2011)}:
\begin{equation*}
\begin{split}p(\log\mathrm{SFR}) \propto \mathrm{SFR}^{\alpha} \exp(-\mathrm{SFR}/\mathrm{SFR}_*)\end{split}
\end{equation*}
with \(\alpha = -0.51\) and \(\mathrm{SFR}_* = 9.2\,M_\odot\,\mathrm{yr}^{-1}\).
\begin{enumerate}
\setcounter{enumi}{3}
\item {} 
Generate the posterior probability distribuiton of SFR via:

\begin{Verbatim}[commandchars=\\\{\}]
logSFR, pdf = sfr\PYGZus{}estimator.mpdf(logSFR\PYGZus{}in, logSFRphoterr = logSFR\PYGZus{}err)
\end{Verbatim}

\end{enumerate}

The argument \code{logSFR\_in} can be a float or an array specifying one or more point mass estimates of the SFR in your chosen filter. For a case with two or more filters, then \code{logSFR\_in} must be an array whose trailing dimension matches the number of filters. If you have added two or more filter sets, you need to specify which one you want to use via the \code{filters} keyword. The optional argument \code{logSFRphoterr} can be used to provide errors on the photometric SFRs. Like \code{logSFR\_in}, it can be a float or an array.

The \code{sfr\_slug.mpdf} method returns a tuple of two quantities. The first is a grid of log SFR values, and the second is the posterior probability distribution at each value of log SFR. If the input consisted of multiple photometric SFRs, the output will contains posterior probabilities for each input. The output grid will be created automatically be default, but all aspects of it (shape, size, placement of grid points) can be controlled by keywords -- see {\hyperref[sfr_slug:sec\string-sfr\string-slug\string-full]{\crossref{\DUrole{std,std-ref}{Full Documentation of slugpy.sfr\_slug}}}}.


\section{Full Documentation of slugpy.sfr\_slug}
\label{sfr_slug:sec-sfr-slug-full}\label{sfr_slug:full-documentation-of-slugpy-sfr-slug}\index{sfr\_slug (class in slugpy.sfr\_slug)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug}\pysiglinewithargsret{\strong{class }\code{slugpy.sfr\_slug.}\bfcode{sfr\_slug}}{\emph{libname=None}, \emph{detname=None}, \emph{filters=None}, \emph{bandwidth=0.1}, \emph{ktype='gaussian'}, \emph{priors=None}, \emph{sample\_density='read'}, \emph{reltol=0.001}, \emph{abstol=1e-10}, \emph{leafsize=16}}{}
A class that can be used to estimate the PDF of true star
formation rate from a set of input point mass estimates of the
star formation rate.
\begin{description}
\item[{Properties}] \leavevmode\begin{description}
\item[{priors}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} `flat' \textbar{} `schechter' \textbar{} None{]}
prior probability on each data point; interpretation
depends on the type passed; array, shape (N): values are
interpreted as the prior probability of each data point;
callable: the callable must take as an argument an array
of shape (N, nphys), and return an array of shape (N)
giving the prior probability at each data point; None:
all data points have equal prior probability; the values
`flat' and `schechter' use priors p(log SFR) \textasciitilde{} constant and
p(log SFR) \textasciitilde{} SFR\textasciicircum{}alpha exp(-SFR/SFR\_*), respectively, where
alpha = -0.51 and SFR\_* = 9.2 Msun/yr are the values
measured by Bothwell et al. (2011)

\item[{bandwidth}] \leavevmode{[}`auto' \textbar{} array, shape (M){]}
bandwidth for kernel density estimation; if set to
`auto', the bandwidth will be estimated automatically

\end{description}

\end{description}
\index{\_\_init\_\_() (slugpy.sfr\_slug.sfr\_slug method)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{libname=None}, \emph{detname=None}, \emph{filters=None}, \emph{bandwidth=0.1}, \emph{ktype='gaussian'}, \emph{priors=None}, \emph{sample\_density='read'}, \emph{reltol=0.001}, \emph{abstol=1e-10}, \emph{leafsize=16}}{}~\begin{quote}

Initialize an sfr\_slug object.
\end{quote}
\begin{description}
\item[{Parameters}] \leavevmode\begin{quote}
\begin{description}
\item[{libname}] \leavevmode{[}string{]}
name of the SLUG model to load; if left as None, the default
is \$SLUG\_DIR/sfr\_slug/SFR\_SLUG

\item[{detname}] \leavevmode{[}string{]}
name of a SLUG model run with the same parameters but no
stochasticity; used to establish the non-stochastic
photometry to SFR conversions; if left as None, the default
is libname\_DET

\item[{filters}] \leavevmode{[}iterable of stringlike{]}
list of filter names to be used for inferenence

\item[{bandwidth}] \leavevmode{[}`auto' \textbar{} float \textbar{} array, shape (M){]}
bandwidth for kernel density estimation; if set to
`auto', the bandwidth will be estimated automatically;
if set to a float, the same bandwidth is used in all
dimensions

\item[{ktype}] \leavevmode{[}string{]}
type of kernel to be used in densty estimation; allowed
values are `gaussian' (default), `epanechnikov', and
`tophat'; only Gaussian can be used with error bars

\item[{priors}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} None{]}
prior probability on each data point; interpretation
depends on the type passed; array, shape (N): values are
interpreted as the prior probability of each data point;
callable: the callable must take as an argument an array
of shape (N, nphys), and return an array of shape (N)
giving the prior probability at each data point; None:
all data points have equal prior probability

\item[{sample\_density}] \leavevmode{[}array, shape (N) \textbar{} callable \textbar{} `auto' \textbar{} `read' \textbar{} None{]}
the density of the data samples at each data point; this
need not match the prior density; interpretation depends
on the type passed; array, shape (N): values are
interpreted as the density of data sampling at each
sample point; callable: the callable must take as an
argument an array of shape (N, nphys), and return an
array of shape (N) giving the sampling density at each
point; `auto': the sample density will be computed
directly from the data set; note that this can be quite
slow for large data sets, so it is preferable to specify
this analytically if it is known; `read': the sample
density is to be read from a numpy save file whose name
matches that of the library, with the extension \_density.npy
added; None: data are assumed to be uniformly sampled

\item[{reltol}] \leavevmode{[}float{]}
relative error tolerance; errors on all returned
probabilities p will satisfy either
abs(p\_est - p\_true) \textless{}= reltol * p\_est   OR
abs(p\_est - p\_true) \textless{}= abstol,
where p\_est is the returned estimate and p\_true is the
true value

\item[{abstol}] \leavevmode{[}float{]}
absolute error tolerance; see above

\item[{leafsize}] \leavevmode{[}int{]}
number of data points in each leaf of the KD tree

\end{description}
\end{quote}
\begin{description}
\item[{Returns}] \leavevmode
Nothing

\item[{Raises}] \leavevmode
IOError, if the library cannot be found

\end{description}

\end{description}

\end{fulllineitems}

\index{\_\_weakref\_\_ (slugpy.sfr\_slug.sfr\_slug attribute)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug.__weakref__}\pysigline{\bfcode{\_\_weakref\_\_}}
list of weak references to the object (if defined)

\end{fulllineitems}

\index{add\_filters() (slugpy.sfr\_slug.sfr\_slug method)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug.add_filters}\pysiglinewithargsret{\bfcode{add\_filters}}{\emph{filters}}{}
Add a set of filters to use for cluster property estimation
\begin{description}
\item[{Parameters}] \leavevmode\begin{description}
\item[{filters}] \leavevmode{[}iterable of stringlike{]}
list of filter names to be used for inferenence

\end{description}

\item[{Returns}] \leavevmode
nothing

\end{description}

\end{fulllineitems}

\index{filters() (slugpy.sfr\_slug.sfr\_slug method)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug.filters}\pysiglinewithargsret{\bfcode{filters}}{}{}
Returns list of all available filters
\begin{description}
\item[{Parameters:}] \leavevmode
None

\item[{Returns:}] \leavevmode\begin{description}
\item[{filters}] \leavevmode{[}list of strings{]}
list of available filter names

\end{description}

\end{description}

\end{fulllineitems}

\index{logL() (slugpy.sfr\_slug.sfr\_slug method)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug.logL}\pysiglinewithargsret{\bfcode{logL}}{\emph{logSFR}, \emph{logSFRphot}, \emph{logSFRphoterr=None}, \emph{filters=None}}{}
This function returns the natural log of the likelihood
function evaluated at a particular log SFR and set of log
luminosities
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{logSFR}] \leavevmode{[}float or arraylike{]}
float or array giving values of the log SFR; for an
array, the operation is vectorized

\item[{logSFRphot}] \leavevmode{[}float or arraylike, shape (nfilter) or (..., nfilter){]}
float or array giving the SFR inferred from photometry using a
deterministic conversion; for an array, the operation is
vectorized over the leading dimensions

\item[{logSFRphoterr}] \leavevmode{[}float arraylike, shape (nfilter) or (..., nfilter){]}
float or array giving photometric SFR errors; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters used for the SFR estimation;
if left as None, and only 1 set of photometric filters
has been defined for the sfr\_slug object, that set will
be used by default

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{logL}] \leavevmode{[}float or arraylike{]}
natural log of the likelihood function

\end{description}

\end{description}

\end{fulllineitems}

\index{mcmc() (slugpy.sfr\_slug.sfr\_slug method)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug.mcmc}\pysiglinewithargsret{\bfcode{mcmc}}{\emph{photprop}, \emph{photerr=None}, \emph{mc\_walkers=100}, \emph{mc\_steps=500}, \emph{mc\_burn\_in=50}, \emph{filters=None}}{}
This function returns a sample of MCMC walkers for log SFR
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{photprop}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving the photometric values; for a
multidimensional array, the operation is vectorized over
the leading dimensions

\item[{photerr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\item[{mc\_walkers}] \leavevmode{[}int{]}
number of walkers to use in the MCMC

\item[{mc\_steps}] \leavevmode{[}int{]}
number of steps in the MCMC

\item[{mc\_burn\_in}] \leavevmode{[}int{]}
number of steps to consider ``burn-in'' and discard

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns}] \leavevmode\begin{description}
\item[{samples}] \leavevmode{[}array{]}
array of sample points returned by the MCMC

\end{description}

\end{description}

\end{fulllineitems}

\index{mpdf() (slugpy.sfr\_slug.sfr\_slug method)}

\begin{fulllineitems}
\phantomsection\label{sfr_slug:slugpy.sfr_slug.sfr_slug.mpdf}\pysiglinewithargsret{\bfcode{mpdf}}{\emph{logSFRphot}, \emph{logSFRphoterr=None}, \emph{ngrid=128}, \emph{qmin=None}, \emph{qmax=None}, \emph{grid=None}, \emph{norm=True}, \emph{filters=None}}{}
Returns the marginal probability of log SFR for one or more
input sets of photometric properties. Output quantities are
computed on a grid of values, in the same style as meshgrid
\begin{description}
\item[{Parameters:}] \leavevmode\begin{description}
\item[{logSFRphot}] \leavevmode{[}float or arraylike{]}
float or array giving the log SFR inferred from
photometry using a deterministic conversion; if the
argument is an array, the operation is vectorized over
it

\item[{logSFRphoterr}] \leavevmode{[}arraylike, shape (nfilter) or (..., nfilter){]}
array giving photometric errors; for a multidimensional
array, the operation is vectorized over the leading
dimensions

\item[{ngrid}] \leavevmode{[}int{]}
number of points in the output log SFR grid

\item[{qmin}] \leavevmode{[}float{]}
minimum value in the output log SFR grid

\item[{qmax}] \leavevmode{[}float{]}
maximum value in the output log SFR grid

\item[{grid}] \leavevmode{[}array{]}
set of values defining the grid of SFR values at which
to evaluate; if set, overrides ngrid, qmin, and qmax

\item[{norm}] \leavevmode{[}bool{]}
if True, returned pdf's will be normalized to integrate
to 1

\item[{filters}] \leavevmode{[}listlike of strings{]}
list of photometric filters to use; if left as None, and
only 1 set of photometric filters has been defined for
the cluster\_slug object, that set will be used by
default

\end{description}

\item[{Returns:}] \leavevmode\begin{description}
\item[{grid\_out}] \leavevmode{[}array{]}
array of log SFR values at which the PDF is evaluated

\item[{pdf}] \leavevmode{[}array{]}
array of marginal posterior probabilities at each point
of the output grid, for each input photometric value;
the leading dimensions match the leading dimensions
produced by broadcasting the leading dimensions of
photprop and photerr together, while the trailing
dimensions match the dimensions of the output grid

\end{description}

\end{description}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Test Problems}
\label{tests:test-problems}\label{tests::doc}\label{tests:sec-tests}
This section describes a set of problems that can be used to test and explore the different capabilities of SLUG. SLUG ships a
set of problems \code{problemname} that are specified by a parameter file \code{param/problemname.param}. Problems that require
multiple simulations are described instead by multiple paramater files, each with unique ID XX:  \code{param/problemnameXX.param}.
Users can reproduce the output of the test problems with the provided executable scripts  \code{test/run\_problemname.sh}.
For each problem, a script for analysis is distributed  in \code{test/problemname.py}. Details for each test problem are given below. Throughout this section, it is assumed that the \code{SLUG\_DIR} has been properly set.
These test problems are designed to work with outputs in FITS format, but that can be easily changed in the
\code{.param} files. To run all the problems and the analysis scripts in one go, the user can simply
run \code{test/run\_alltest.sh}. It will take around 15 minutes
for the script to complete on a standard laptop. About 700MB of data are generated.
If SLUG is correctly installed and working, the first part of the script (i.e. the SLUG
simulations) should run flawlessly. The second part of the script relies instead on external python procedures,
including slugpy, numpy, and matplotlib. While these packages are fairly standard, the user needs to ensure that
they are properly installed and visible to the script. This script has been written for and tested with Python 2.7.


\section{Problem \texttt{example\_galaxy}: basic galaxy simulation}
\label{tests:problem-example-galaxy-basic-galaxy-simulation}
This problem illustrates the basic usage of slugin \code{galaxy} mode by running 48 realizations of a galaxy with constant
\(\mathrm{SFR}=0.001\; M_\odot\;\mathrm{yr}^{-1}\), up to a maximum time of \(2\times 10^8\) yr. By issuing the
command \code{test/run\_example\_galaxy.sh} the output files \code{SLUG\_GALAXY\_EXAMPLE*} are generated. Once the models are ready,
\code{python test/plot\_example\_galaxy.py} produces a multi-panel figure \code{test/SLUG\_GALAXY\_EXAMPLE\_f1.pdf}.

The top-left panel shows the actual mass produced by SLUG for each of the 48 models at different time steps as a
function of the targeted mass. One can see that SLUG realizations only approximate the desired mass, which is a consequence
of SLUG core algorithm. The 1:1 relation is shown by a red dashed line.
The remaining panels show examples of integrated photometry (as labeled) of all simulated galaxies
at different time steps, as a function of the actual mass. Due to its stochastic nature, SLUG produces
distributions rather than single values for each time step. The expected rate of ionizing
photon and the bolometric luminosities for a deterministic model with a
continuous star formation rate of \(\mathrm{SFR}=0.001\; M_\odot\;\mathrm{yr}^{-1}\) are shown
by red dashed lines in the relevant panels.


\section{Problem \texttt{example\_cluster}: basic cluster simulation}
\label{tests:problem-example-cluster-basic-cluster-simulation}
This problem illustrates the basic usage of SLUG in \code{cluster} mode by running 1000 realizations of a cluster
with mass 500 \(M_\odot\), up to a maximum time of 10 Myr. By issuing the command
\code{test/run\_example\_cluster.sh} the output files \code{SLUG\_CLUSTER\_EXAMPLE*} are
generated. Once the models are ready, \code{python test/plot\_example\_cluster.py} produces a multi-panel
figure \code{test/SLUG\_CLUSTER\_EXAMPLE\_f1.pdf}.

This figure is divided in two columns: the left one shows outputs at the first time step, 1 Myr, while
the second one shows outputs at the last time step, 10 Myr.  The top row shows the actual cluster mass for an
input mass of \(500\;M_\odot\).
In \code{cluster} mode, all clusters are generated at the first time step and they evolve
passively after that. Thus, the mass does not change. As a consequence of the
random drawing from the IMF, masses are distributed around the input mass.
As the wanted mass is large enough to allow for many stars to be drawn, the
actual mass distribution is narrow.

The second row shows instead the distribution of the maximum mass of all stars that are still
alive at a given time step. At 1 Myr, this distribution is a good approximation of the
input distribution, which is the result of random draws from the IMF. At 10 Myr, which is the
typical lifetime of a 15-20 \(M_\odot\) star, the most massive stars have died, and
SLUG stops following them. The distribution of luminosities, and particularly those
most sensitive to the presence of massive stars, change accordingly
(third and fourth row for \(Q_{H_0}\) and FUV).


\section{Problem \texttt{constsampl}: importance of constrained sampling}
\label{tests:probsampl-label}\label{tests:problem-constsampl-importance-of-constrained-sampling}
This problem illustrates in more detail the effects of constrained sampling on SLUG simulations.
This is the first key ingredient in the core algorithm of SLUG. With the command \code{test/run\_constsampl.sh},
three different \code{cluster} simulations are run, each with 1000 trials, but with masses of \(50\;M_\odot\),
\(250\;M_\odot\), and \(500\;M_\odot\). A single timestep of \(10^6\) yr is generated.
The analysis script \code{python test/plot\_constsampl.py} produces a multi-panel
figure \code{test/SLUG\_CONSTSAMPL\_f1.pdf}.

This figure shows the maximum mass of the stars in these realizations (top row), the
rate of ionizing photons \(Q_{H_0}\) (central row), and the FUV luminosity (bottom row).
Histograms refer, form left to right, to clusters with \(50\;M_\odot\), \(250\;M_\odot\),
and \(500\;M_\odot\).

Due to the small timestep, the distributions of stellar masses shown in the top panels reflect
to good approximation the distribution of the maximum stellar masses that are drawn from the IMF by
SLUG in each realization. For a cluster of \(50\;M_\odot\), the vast majority of the
stars are drawn below  \(20-50\;M_\odot\). This is an obvious consequence of the
fact that a cluster cannot contain stars much more massive than its own mass. However, stars
more massive then the targeted mass are not impossible realizations for the default
sampling algorithm (see below). For instance, if the first star to be drawn has
mass \(60\;M_\odot\), then SLUG would add it to the cluster and stop. Leaving this star out
would indeed be a worse approximation than overshooting the targeted cluster mass by only
\(10\;M_\odot\).  From left to right, one can see that, as the targeted cluster mass increases, the
histogram shifts to progressively higher masses. In the limit of an infinite cluster,
all stellar masses would be represented, and the histogram would peak at \(120\;M_\odot\).
Essentially, this constrained sampling introduces a stochastic (and not deterministic)
variation in the IMF. An IMF truncated above \(60\;M_\odot\) would roughly
approximate the results of the left column; however, a deterministic cut-off
would not correctly reproduce the non-zero tail at higher masses, thus artificially
reducing the scatter introduced by random sampling.

The second and third row simply reflect what said above: for large clusters that can host
stars at all masses, the luminosity peaks around what is expected according to a deterministic
stellar population synthesis codes. At lower cluster masses, ionizing and UV fluxes
are instead suppresses, due to the lack of massive stars. However, tails to high values exist
in all cases.


\section{Problem \texttt{sampling}: different sampling techniques}
\label{tests:problem-sampling-different-sampling-techniques}
As highlighted in the previous section, the method with which stars are sampled from the
IMF has a great influence on the final output. Starting from v2, SLUG has the capability of
specifying the desired sampling algorithm for a given PDF.
The command  \code{test/run\_sampling.sh} runs four \code{cluster} simulations, each with 1000 trials
of masses of \(50\;M_\odot\), and a Kroupa (2002) IMF.
The following four sampling methods are chosen for each simulation: 1) \code{stop\_nearest},
which is the default in SLUG; 2) \code{stop\_before}; 3) \code{stop\_after}; 4) \code{sorted\_sampling}.
A description of each method is provided in Section {\hyperref[pdfs:sampling\string-metod\string-label]{\crossref{\DUrole{std,std-ref}{Sampling Methods}}}}.
The analysis script \code{python test/plot\_sampling.py} produces a multi-panel
figure \code{test/SLUG\_SAMPLING\_f1.pdf}.

By comparing the panels in each column, one can understand the fundamental differences
induced by the sampling technique. The top row shows the maximum stellar mass drawn from the
IMF in each realization. The targeted cluster mass is also shown with red vertical lines.
In the default mode, SLUG is allowed to overshoot the targeted mass if that constitutes
a good approximation for the total cluster mass. Thus, a tail at stellar masses above the
targeted cluster mass is visible. This tail is accentuated when the stop after method
is selected (third column). In this case, SLUG always overshoots the cluster mass, and thus
extreme realizations above \(100\;M_\odot\)  are possible. Conversely, in the
stop after method (second column), SLUG always under-fills the clusters, and (in this case)
the cluster mass becomes a limit to the maximum stellar mass that can be drawn. A similar effect
is seen when sorted sampling is enable (fourth column). However, the correspondence between the
cluster mass and the maximum stellar mass is not trivially established, as it depends on the
shape of the IMF. The second and third row show how the sampling techniques affect the output
photometry.


\section{Problem \texttt{imfchoice}: different IMF implementations}
\label{tests:problem-imfchoice-different-imf-implementations}\label{tests:probimf-label}
This problem highlights how SLUG can handle different IMF implementations by running
three simulations with a Kroupa, a Salpeter, and a Chabrier IMF. However, SLUG is not
restricted to these choices, as the user can in fact easily input an arbitrary IMF.
The command  \code{test/run\_imfchoice.sh} runs three \code{cluster} simulations, each with 1000 trials
of masses of \(500\;M_\odot\) and different IMF. The analysis script
\code{python test/plot\_imfchoice.py} produces a multi-panel figure \code{test/SLUG\_IMFCHOICE\_f1.pdf}.
Each column shows different statistics for the three IMF. From top to bottom, these are:
the maximum stellar mass in a cluster, the number of stars that SLUG treats stochastically,
and the distributions of \(Q_{H_0}\)  and bolometric luminosities.
As expected for a steep lower-end of the IMF, in the Salpeter case SLUG prefers to fill the
clusters with a higher number of low mass stars.


\section{Problem \texttt{clfraction}: cluster fraction at work}
\label{tests:problem-clfraction-cluster-fraction-at-work}
With the exception of the first example, these test problems have focused on how SLUG handles
cluster simulations, and how these clusters are filled with stars drawn from the IMF.
This new problem highlights instead the presence of additional stochasticity induced by a
second level in the hierarchy of \code{galaxy} simulations: how clusters are drawn from the CMF to satisfy the
targeted galaxy mass. Although it may not appear obvious at first,
the fraction of stars that are formed in clusters, \(f_c\), is a very important parameter that regulates
the stochastic behavior of SLUG. This can be understood by considering two limiting cases.
In the limit \(f_c \rightarrow 0\), SLUG fills a galaxy by drawing stars from the
IMF. Thus, because the mass of a galaxy is typically much larger than the mass of the upper
end of the IMF, the effects of mass-constrained sampling highlighted in {\hyperref[tests:probsampl\string-label]{\crossref{\DUrole{std,std-ref}{Problem constsampl: importance of constrained sampling}}}} are simply
not relevant anymore. In this case, stochasticity is minimal.
Conversely, in the limit \(f_c \rightarrow 1\), not only the IMF sampling contributes to the
stochastic behavior of SLUG, but also clusters themselves contribute to additional stochasticity,
as clusters are now drawn from the CMF to fill the targeted galaxy mass following the similar rules
to those specified for the IMF draws. Thus, in this case, constrained mass sampling applies to both
stars in clusters and clusters in galaxies, and stochasticity is amplified.

The command  \code{test/run\_clfraction.sh} runs three \code{galaxy} simulations, each with 500 trials
of continuous  SFR \(=0.001\rm\;M_\odot\;yr^{-1}\) which are evolved for a
single timestep of  \(2\times 10^6\rm\;yr\). A Chabrier IMF and a cluster mass function
\(\propto M^{-2}\) are adopted. Cluster disruption is disabled. The three simulations
differ only for the fraction of stars formed in clusters, respectively \(f_c=1,0.5,0.01\).
The analysis script \code{python test/plot\_clfraction.py} produces a multi-panel figure
\code{test/SLUG\_CLFRACTION\_f1.pdf}. Each column shows properties of simulations for different
fractions of stars formed in clusters.

The top row shows the maximum stellar mass in clusters. Clearly, \(f_c\) has no effect on the way
clusters are filled up with stars, but the normalization changes. Thus,  the least probable realizations
in the tail of the distribution simply do not appear for \(f_c \rightarrow 0\). The second row
shows the number of stars in clusters. Obviously, this scales directly with  \(f_c\), as it does the number
of field stars in the third row. This is expected as, by definition, \(f_c\) regulates the number of stars in
clusters versus the field. However, as discussed, \(f_c\) also affects the stochastic behavior of the
simulation. The fourth row shows histograms of the actual galaxy mass versus the targeted mass (red line).
As \(f_c\) increases, one can see that the spread around the targeted mass increase. This is again
a consequence of the mass-constrained sampling and the stop-nearest condition. For \(f_c \rightarrow 0\),
the code tries to fill a galaxy of mass \(0.001\rm\;M_\odot\;yr^{-1} \times 2\times 10^6\rm\;yr\)
with stars. Thus, since the targeted mass is at least a factor of 10 larger than the mass of the
building block, SLUG can approximate the desired mass very well (to better than \(120\rm\;M_\odot\), in fact).
Conversely, for \(f_c \rightarrow 1\), SLUG is using clusters as building blocks. As the typical
mass of the building blocks is now more comparable to the targeted galaxy mass, the problem of the
mass constrained sampling becomes a relevant one. Not only \(f_c\) affects the precision with which
SLUG builds galaxies, but, as shown in the bottom row, it also affects photometry. One can see that
\(Q_{H_0}\) increases as \(f_c\) decreases (the red lines indicate medians).
The reason for this behavior should now be clear:
in the case of clustered star formation (\(f_c \rightarrow 1\)), the mass of the most massive stars
is subject to the mass constrained sampling of the IMF at the cluster level, reducing the occurrence of
very massive stars and thus suppressing the flux of ionizing radiation. Conversely, for non clustered star formation
(\(f_c \rightarrow 0\)), the sampling of the IMF is constrained only at the galaxy mass level, and since this
is typically much greater than the mass of the most massive stars, one recovers higher fluxes on average.


\section{Problem \texttt{cmfchoice}: different CMF implementations}
\label{tests:problem-cmfchoice-different-cmf-implementations}
Given the ability of SLUG v2 to handle generic PDFs, the user can specify arbitrary CMF,
similarly to what shown in  {\hyperref[tests:probimf\string-label]{\crossref{\DUrole{std,std-ref}{Problem imfchoice: different IMF implementations}}}}.
The command  \code{test/run\_cmfchoice.sh} runs three \code{galaxy} simulations, each with 500 trials
of continuous  SFR \(=0.001\rm\;M_\odot\;yr^{-1}\) which are evolved for a
single timestep of  \(2\times 10^6\rm\;yr\). A Chabrier IMF and \(f_c=1\)
are adopted. Cluster disruption is disabled. The three simulations
differ only for the cluster mass function, which are:
1) the default powerlaw \(M^{-2}\) between \(20-10^{7}~\rm M_\odot\);
2) a truncated powerlaw \(M^{-2}\) between \(20-100~\rm M_\odot\);
3) a mass-independent CMF \(M^{0}\) between \(20-10^3~\rm M_\odot\).
The analysis script \code{python test/plot\_cmfchoice.py} produces a multi-panel figure
\code{test/SLUG\_CMFCHOICE\_f1.pdf}. Each column shows properties of simulations for the different
cluster mass functions.

The top row shows the maximum stellar mass in clusters. Compared to the default case,
the histogram of the truncated CMF is steeper towards low masses. Given that the upper end of the
CMF is comparable to the maximum stellar mass of the chosen IMF, low stellar masses are typically
preferred  as a result of the stop-nearest condition. A flat CMF
prefers instead more massive clusters on average, which in turn results in higher probabilities
of drawing massive stars. In this case, the residual slope of the distribution towards
low stellar masses is a result of the shape of the IMF. A reflection of the effects induced by the
shape of the CMF are also apparent in the bottom row, which shows the distribution of
ionizing photons from these simulations. The second row shows instead the difference
between the targeted galaxy mass (red line), and the distribution of actual masses.
The spread is minimal for the truncated CMF because, as discussed above, SLUG is using
small building blocks, and it can approximate the targeted galaxy mass very well.
Larger spread is visible in the case of the flat CMF, as this choice allows for clusters with masses
up to \(10^3~\rm M_\odot\), without imposing an excess of probability at the low
mass end. The largest scatter is visible for the default case, as this CMF is virtually
a pure powerlaw without cutoff at the high mass end, and thus clusters as massive as the entire galaxy
are accessible to SLUG.


\section{Problem \texttt{sfhsampling}: realizations of SFH}
\label{tests:problem-sfhsampling-realizations-of-sfh}
The algorithm at the heart of SLUG is quite simple: for a given star formation history
\(\dot\psi(t)\) a stellar population with mass \(\dot\psi(t)\times \Delta t\)
is generated at each timestep, according to the constraints set by IMF, CMF and other
controlling parameters. As discussed in the previous examples, SLUG builds a best
approximation for the targeted mass \(\dot\psi(t)\times \Delta t\). This means that
the input SFH and the output SFHs are not identical. SLUG receives an input SFH which
is used to constrain the rate with which clusters and stars are drawn to achieve the
desired targeted mass in each timestep. However, the output SFHs are only realizations
and not exact copies  of the input SFH. This problem is designed to illustrate this behavior.

The command  \code{test/run\_sfhsampling.sh} runs two \code{galaxy} simulations, each with 100 trials
of continuous  SFR \(=0.0001\rm\;M_\odot\;yr^{-1}\) which are evolved for a
10 timesteps of  \(5\times 10^6\rm\;yr\). A Chabrier IMF and a \(M^{-2}\)
CMF are adopted. Cluster disruption is disabled. The two simulations
differ only for the fraction of stars in clusters, \(f_c = 1\) and \(f_c = 0\) respectively.
The analysis script \code{python test/plot\_sfhsampling.py} produces a two-panel figure
\code{test/SLUG\_SFHSAMPLING\_f1.pdf}, showing the box plot for the output SFH of the two simulations
(\(f_c = 1\) top, and \(f_c = 0\) bottom).

In each panel, the median SFH over 100 trials is represented by the red lines, while the red squares
show the mean. The box sizes represent instead the first and third quartile, with the
ends of the whiskers representing the 5th and 95th percentiles. One can see that the input
SFH at \(\dot\psi(t)=10^{-4}\rm\;M_\odot\;yr^{-1}\) is recovered on average, albeit with
significant variation in each realization. The reason for this variation lies in the fact that,
at low SFRs, SLUG samples the input SFH with coarse sampling points, which are clusters and stars.
One can also notice a widely different scatter between the \(f_c = 1\) and \(f_c = 0\)
case. In the former case, the basic elements used by SLUG to sample the targeted mass in  a
given interval are clusters. In the latter case, they are stars. Given that the typical mass of a
cluster is of the same order of the targeted mass in each interval, the output SFH for
the \(f_c = 1\) case are more sensitive to the history of drawings from the CMF.
Conversely, for  \(f_c = 0\), the sampling elements are less massive than the
targeted mass in a given interval, resulting in an output SFH distribution which is
better converged towards the input value. Clearly, a comparable amplitude in the scatter
will be present in the output photometry, especially for the traces that are more sensitive
to variations in the SFHs on short timescales.


\section{Problem \texttt{cldisrupt}: cluster disruption at work}
\label{tests:problem-cldisrupt-cluster-disruption-at-work}
One additional ingredient in SLUG is the lifetime distribution for clusters. Since v2, SLUG is flexible in
controlling the rate with which clusters are disrupted. This problem shows a comparison between
two simulations with and without cluster disruption.

The command  \code{test/run\_cldisrup.sh} runs two \code{galaxy} simulations, each with 100 trials
which are evolved in timesteps of  \(5\times 10^5\rm\;yr\) up to a maximum age of
\(1\times 10^7\rm\;yr\). Both simulations are characterized by a burst of star formation
\(=0.001\rm\;M_\odot\;yr^{-1}\) within the first Myr. A Chabrier IMF and a \(M^{-2}\)
CMF are adopted, and \(f_c = 1\). For the first simulation, cluster disruption is
disabled. In the second simulation, cluster disruption operates at times \(>1\rm\;Myr\),
with a cluster lifetime function which is a powerlaw of index -1.9.
The analysis script \code{python test/plot\_cldisrup.py} produces the figure \code{test/SLUG\_CLDISRUP\_f1.pdf}.
The two columns show results with (right) and without (left) cluster disruption.

The first row shows the median stellar mass of the 100 trials as a function of time.
The blue dashed lines show the mass inside the galaxy, while the black solid lines show the
median mass in clusters. The red band shows the first and fourth quartile of the distribution.
One can see that in both cases the galaxy mass rises in the first Myr up to the desired
targeted mass of \(=1000\rm\;M_\odot\) given the input SFH. After 1Myr, star formation
stops and the galaxy mass does not evolve with time. Conversely, the cluster mass (black line, red
regions) evolves differently. In the case without cluster disruption, because \(f_c = 1\),
the cluster mass tracks the galaxy mass at all time. When cluster disruption is enabled (right),
one can see that the mass in clusters rise following the galaxy mass in the first Myr. Past that time,
clusters start being disrupted and the mass in clusters declines.
The same behavior is visible in the second row, which shows the median number of alive (black) and
disrupted (black) clusters. To the left, without cluster disruption, the number of clusters alive
tracks the galaxy mass. Conversely, this distribution declines with time to the right when cluster disruption is
enabled. The complementary quantity (number of disrupted clusters) rises accordingly.
The last two rows show instead the integrated fluxes in FUV and bolometric luminosity.
Again, medians are in black and the first and third quartiles in red. One can see a nearly identical distribution
in the left and right panels. In these simulations, the controlling factors of the integrated photometry
are the SFH and the sampling techniques, which do not depend on the cluster disruption rate. Clearly, the
photometry of stars in cluster would exhibit instead a similar dependence to what shown in the top panels.


\section{Problem \texttt{spectra}: full spectra}
\label{tests:problem-spectra-full-spectra}
Since v2, SLUG is able to generate spectra for star clusters and for galaxies, which can also be computed for
arbitrary redshifts. This problem highlights the new features.
It also demonstrates how SLUG can handle dust extinction, both in a deterministic and stochastic way.

The command  \code{test/run\_spectra.sh} runs four \code{galaxy} simulations, each with 500 trials
of continuous SFR \(=0.001\rm\;M_\odot\;yr^{-1}\) which are evolved for a
single timestep of  \(2\times 10^6\rm\;yr\). A Chabrier IMF and a \(M^{-2}\)
CMF are adopted, cluster disruption is disabled, and \(f_c = 1\).
The simulations differ in the following way:
1) the reference model, computed without extinction and at \(z = 0\);
2) same as the reference model, but at \(z = 3\);
3) same as the reference model at \(z = 0\), but with a deterministic extinction of \(A_V = 0.5\) and
a Calzetti+2000 starburst attenuation curve;
4) same as model number 3, but with stochastic extinction.
The analysis script \code{python test/plot\_spectra.py} produces the figure \code{test/SLUG\_SPECTRA\_f1.pdf},
which shows a gallery of galaxy SEDs for each model. The median SED is shown in black, the blue region
corresponds to the first and third quartile of the distribution, and the red shaded region
marks the 5 and 95 percentiles.

The top panel shows the default model, where stochasticity occurs as detailed in the previous examples.
The second panel from the top shows instead a model with deterministic extinction. This is simply
a scaled-down version of the reference model, according to the input dust law and normalization
coefficient \(A_V\). As the dust law extends only to 915 Angstrom the output SED is truncated.
The third panel shows that, once SLUG handles dust  in a stochastic way, the intrinsic scatter is
amplified. This is a simple consequence of applying dust extinction with varying normalizations, which
enhances the final scatter about the median. Finally, the bottom panel shows the trivial case in which
the spectrum is shifted in wavelength by a constant factor \((1+z)\). Obviously, redshift enhances
the stochasticity in the optical due to a simple shift of wavelengths.


\chapter{Using SLUG as a Library}
\label{library::doc}\label{library:using-slug-as-a-library}
In addition to running as a standalone program, SLUG can be
compiled as a library that can be called by external programs. This is
useful for including stellar population synthesis calculations within
some larger code, e.g., a galaxy simulation code in which star
particles represent individual star clusters, where the stars in them
are treated stochastically.


\section{Compiling in Library Mode}
\label{library:compiling-in-library-mode}\label{library:ssec-library-mode}
To compile in library mode, simply do:

\begin{Verbatim}[commandchars=\\\{\}]
make lib
\end{Verbatim}

in the main directory. This will cause a dynamically linked library
file \code{libslug.x} to be created in the \code{src} directory, where \code{x}
is whatever the standard extension for dynamically linked libraries on
your system is (\code{.so} for unix-like systems, \code{.dylib} for MacOS).

Alternately, if you prefer a statically-linked version, you can do:

\begin{Verbatim}[commandchars=\\\{\}]
make libstatic
\end{Verbatim}

and a statically-linked archive \code{libslug.y} will be created instead,
where \code{y} is the standard statically-linked library extension on
your system (generally \code{.a}).

In addition to \code{lib} and \code{libstatic}, the makefile supports
\code{lib-debug} and \code{libstatic-debug} as targets as well. These
compile the same libraries, but with optimization disabled and
debugging symbols enabled.


\section{Predefined Objects}
\label{library:ssec-predefined-objects}\label{library:predefined-objects}
In order to make it more convenient to use slug as a library, the
library pre-defines some of the most commonly-used classes, in order
to save users the need to construct them. These predefined objects can
be accessed by including the file \code{slug\_predefined.H} in your source
file. This function defines the class \code{slug\_predef}, which
pre-defines all the IMFs, evolutionary tracks, spectral synthesizers,
and yields that ship with slug, without forcing the user to interact
with the parameter parsing structure.

The \code{slug\_predef} class provides the methods \code{imf}, \code{tracks},
\code{specsyn}, and \code{yields}. These methods take as arguments a string
specifying one of the predefined names of an IMF, set of tracks, or
spectral synthesizer, and return an object of that class that can then
be passed to \code{slug\_cluster} to produce a cluster object. For
example, the following sytax creates a \code{slug\_cluster} with ID number
1, a mass of 100 solar masses, age 0, a Chabrier IMF, Padova solar
metallicity tracks, starburst99-style spectral synthesis, and slug's
default nuclear yields:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}include \PYGZdq{}slug\PYGZus{}predefined.H\PYGZdq{}
\PYGZsh{}include \PYGZdq{}slug\PYGZus{}cluster.H\PYGZdq{}

slug\PYGZus{}cluster *cluster =
   new slug\PYGZus{}cluster(1, 100.0, 0.0, slug\PYGZus{}predef.imf(\PYGZdq{}chabrier\PYGZdq{}),
                    slug\PYGZus{}predef.tracks(\PYGZdq{}modp020.dat\PYGZdq{}),
                    slug\PYGZus{}predef.specsyn(\PYGZdq{}sb99\PYGZdq{}),
                    nullptr, nullptr, nullptr,
                    slug\PYGZus{}predef.yields());
\end{Verbatim}


\section{Support for MPI Parallelism}
\label{library:support-for-mpi-parallelism}\label{library:ssec-mpi-support}
In large codes where one might wish to use slug for subgrid stellar
models, it is often necessary to pass information between processors
using MPI. Since slug's representation of stellar populations is
complex, and much information is shared between particles rather than
specific to individual particles (e.g., tables of yields and
evolutionary tracks), passing slug information between processors is
non-trivial.

To facilitate parallel implementations, slug provides routines that
wrap the base MPI routines and allow seamless and efficient exchange
of the slug\_cluster class (which slug uses to represent simple stellar
populations) between processors. The prototypes for these functions
are found in the \code{src/slug\_MPI.H} header file.

By default MPI support is not included in the library. To enable MPI
support, compile the library as follows:

\begin{Verbatim}[commandchars=\\\{\}]
make lib MPI=ENABLE\PYGZus{}MPI
\end{Verbatim}

This will enable MPI support. In addition, you may need to specify the
names of your preferred MPI C++ compiler by setting the variable
\code{MACH\_MPICXX} in your machine-specific makefile -- see
{\hyperref[compiling:ssec\string-machine\string-makefiles]{\crossref{\DUrole{std,std-ref}{Machine-Specific Makefiles}}}}. The Makefiles contain reasonable
guesses, but since MPI compiler names are much less standardized than
general compiler names, you may need to supply yours rather than
relying on the default.

Here is an example of MPI usage, in which one processor creates a
cluster and then sends it to another one:

\begin{Verbatim}[commandchars=\\\{\}]
\PYGZsh{}include \PYGZdq{}slug\PYGZus{}cluster.H\PYGZdq{}
\PYGZsh{}include \PYGZdq{}slug\PYGZus{}MPI.H\PYGZdq{}
\PYGZsh{}include \PYGZdq{}mpi.h\PYGZdq{}
\PYGZsh{}include \PYGZlt{}vector\PYGZgt{}
\PYGZsh{}include \PYGZlt{}cstdio\PYGZgt{}

int main(int argc, char *argv[]) \PYGZob{}

  // Start MPI
  MPI\PYGZus{}Init(\PYGZam{}argc, \PYGZam{}argv);

  // Get rank
  int rank;
  MPI\PYGZus{}Comm\PYGZus{}rank(MPI\PYGZus{}COMM\PYGZus{}WORLD, \PYGZam{}rank);

  // Rank 0 creates a cluster and prints out the masses of the stars
  slug\PYGZus{}cluster *cluster;
  if (rank == 0) \PYGZob{}
    cluster =
       new slug\PYGZus{}cluster(1, 100.0, 0.0, slug\PYGZus{}predef.imf(\PYGZdq{}chabrier\PYGZdq{}),
                        slug\PYGZus{}predef.tracks(\PYGZdq{}modp020.dat\PYGZdq{}),
                        slug\PYGZus{}predef.specsyn(\PYGZdq{}sb99\PYGZdq{}),
                        nullptr, nullptr, nullptr,
                        slug\PYGZus{}predef.yields());
    const std::vector\PYGZlt{}double\PYGZgt{} stars = cluster\PYGZhy{}\PYGZgt{}get\PYGZus{}stars();
    for (int j=0; j\PYGZlt{}stars.size(); j++)
      std::cout \PYGZlt{}\PYGZlt{} \PYGZdq{}rank 0, star \PYGZdq{} \PYGZlt{}\PYGZlt{} j
                \PYGZlt{}\PYGZlt{} \PYGZdq{}: \PYGZdq{} \PYGZlt{}\PYGZlt{} stars[j] \PYGZlt{}\PYGZlt{} std::endl;
  \PYGZcb{}

  // Barrier to make sure rank 0 outputs come first
  MPI\PYGZus{}Barrier(MPI\PYGZus{}COMM\PYGZus{}WORLD);

  // Rank 0 sends cluster, rank 1 receives it
  if (rank == 0) \PYGZob{}
    MPI\PYGZus{}send\PYGZus{}slug\PYGZus{}cluster(*cluster, 1, 0, MPI\PYGZus{}COMM\PYGZus{}WORLD);
  \PYGZcb{} else if (rank == 1) \PYGZob{}
    cluster = MPI\PYGZus{}recv\PYGZus{}slug\PYGZus{}cluster(0, 1, MPI\PYGZus{}COMM\PYGZus{}WORLD,
                                    slug\PYGZus{}predef.imf(\PYGZdq{}chabrier\PYGZdq{}),
                                    slug\PYGZus{}predef.tracks(\PYGZdq{}modp020.dat\PYGZdq{}),
                                    slug\PYGZus{}predef.specsyn(\PYGZdq{}sb99\PYGZdq{}),
                                    nullptr, nullptr, nullptr,
                                    slug\PYGZus{}predef.yields());
  \PYGZcb{}

  // Rank 1 prints the masses of the stars; the resulting masses
  // should be identical to that produced on rank 0
  if (rank == 1) \PYGZob{}
    const std::vector\PYGZlt{}double\PYGZgt{} stars = cluster\PYGZhy{}\PYGZgt{}get\PYGZus{}stars();
    for (int j=0; j\PYGZlt{}stars.size(); j++)
      std::cout \PYGZlt{}\PYGZlt{} \PYGZdq{}rank 1, star \PYGZdq{} \PYGZlt{}\PYGZlt{} j
                \PYGZlt{}\PYGZlt{} \PYGZdq{}: \PYGZdq{} \PYGZlt{}\PYGZlt{} stars[j] \PYGZlt{}\PYGZlt{} std::endl;
  \PYGZcb{}
\PYGZcb{}
\end{Verbatim}


\chapter{Contributors and Acknowledgements}
\label{acknowledgements::doc}\label{acknowledgements:contributors-and-acknowledgements}
The following people contributed to slug2:
\begin{itemize}
\item {} 
Mark Krumholz: primary author of slug2

\item {} 
Michele Fumagalli: primary author of the slug2 test suite, co-author of version 1 of slug

\item {} 
Robert da Silva: primary author of version 1 of slug and of sfr\_slug, wrote the first prototype version of slug2 and sfr\_slug

\item {} 
Jonathan Parra: contributed code that become part of the slug\_PDF module

\item {} 
Teddy Rendahl: wrote the first version of cloudy\_slug

\item {} 
Michelle Myers: contributed to the development of cluster\_slug

\item {} 
Greg Ashworth: contributed to the development of the variable PDF module

\item {} 
Evan Demers: wrote the first version of the yield module

\end{itemize}

In addition to these direct contributors, we gratefully acknowledge the following people who provided some of the data on which slug relies:
\begin{itemize}
\item {} 
The library of stellar evolutionary tracks and stellar atmospheres is taken from Claus Leitherer's \href{http://www.stsci.edu/science/starburst99/docs/default.htm}{starburst99} package.

\item {} 
Much of the library of photometric filters is taken from Charlie Conroy's \href{https://code.google.com/p/fsps/}{FSPS} package.

\item {} 
Daniela Calzetti provided the extinction curves

\end{itemize}


\chapter{Indices and tables}
\label{index:indices-and-tables}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{theindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{s}
\item {\texttt{slugpy}}, \pageref{slugpy:module-slugpy}
\item {\texttt{slugpy.bayesphot.bp}}, \pageref{bayesphot:module-slugpy.bayesphot.bp}
\item {\texttt{slugpy.cloudy}}, \pageref{cloudy:module-slugpy.cloudy}
\end{theindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}
